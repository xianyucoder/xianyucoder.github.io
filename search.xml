<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[解锁服务器新姿势]]></title>
    <url>%2F2020%2F02%2F28%2F%E8%A7%A3%E9%94%81%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%9A%84%E6%96%B0%E5%A7%BF%E5%8A%BF%2F</url>
    <content type="text"><![CDATA[记得之前给大家介绍过 如何搭建一个属于自己的 Telegram Bot 不少朋友都自己上手操作了一番，但是孤零零的一个应用，服务器好像又要落灰了。所以，如何解锁服务器的各种新姿势呢？ 我们先从上次的 Telegram Bot 来讲，上次教大家搭建了这个机器人，除了用来管理自己的频道外，其实还可以用它来监控我们想要购买的商品。【比如口罩】 这里只要搭配一个特殊的 Python 库就可以实现了。 目前搭建好的项目提供 X 宝及 X 猫的商品监控功能，使用上，用户可以便捷地发送 App 的分享链接给 Bot 直接进行添加，当商品上架时，用户会连续三次收到来自 Bot 的消息。 安装 Python 包： 12pip install requestspip install python_telegram_bot 接着参考之前文章中的搭建的 TG Bot ，获取 Token 代入之后的代码即可。 参考下面现成的项目代码就就可以完成配置了。 Github : https://github.com/xiejiangzhao/Ding-Bot 搭建好的项目效果参考下图。 除了上面的 TG Bot 监控商品的项目外，只要拥有一台服务器 + 想动手的心，你就可以不停解锁新的姿势。这里简单给大家列举几个： 搭建一人独享的 TG Bot，从此管理频道不费心。 【链接】 搭建一个可以解锁 VIP 音乐的服务，从此听歌不发愁。** 【链接】 搭建一个可以同步上传下载的 BT 服务，独享高速下载。 【链接】 服务器玩腻了？可以试试大佬们都在玩的 ModelArts ，只要鼠标点点点，就可以轻松实现爬虫工程师常用到的拼图验证码缺口识别 API。 再玩玩一站式的 AI 图像识别平台，轻松完成图像深度学习中的图片标注、图片分类预测和模型部署+生成 API。 一站式集成服务，方便快捷，最主要的是这些黑科技现在很便宜！！！ 现在华为开年采购季活动，购买一台服务器的成本最低 79元/年，除此之外，华为云还有数据库专场活动、域名建站专场活动、云安全专场活动。个人用户的各种打折不说，还可以领 8888 开年大礼包。 新用户 2H4G 的云服务器 1.3折，老用户最高可送2020元优惠券，云服务器79元用3年。 我酸了。 目前我手里的服务器，续费 6 年的有一台，趁着这次活动又搞了一台 3 年期的。 至少短时间内，写了很久的小玩意儿都可以愉快的跑起来了。]]></content>
      <categories>
        <category>服务器应用</category>
      </categories>
      <tags>
        <tag>服务器应用</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[怼就完事了，总结几种验证码的解决方案]]></title>
    <url>%2F2020%2F01%2F08%2F%E6%80%BB%E7%BB%93%E5%87%A0%E7%A7%8D%E9%AA%8C%E8%AF%81%E7%A0%81%E7%9A%84%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[截止到今天咸鱼以及写了很多期关于 Js 逆向的文章了，不过这么多的文章都有一个共通点，都是关于加密参数或者密码加密的解析，很多读者在后台私信希望能够出一些关于滑动验证或者人机验证的分析教程。 于是咸鱼总结了目前遇到过的的验证码类型以及总结出来的相关处理方式和大家聊一聊。 现在市面上的验证码的类型大致有下面几种类型。 图形验证码比较常见的英文数字组合成的图形验证码，常常辅以各类干扰线和扭曲图片中的内容达到提高混淆难度的目的，并且通过加长图片中的文字长度提升识别成本。 像这类验证码的处理方案有很多种，简单给大家概括一下。 难度中低的两类验证码，安装 tesserocr，通过 OCR 技术结合 Python 的 tesserocr 库可以就可以完成识别。如果验证码中带有简单干扰线同样可以使用灰度和二值化的方法提高代码的识别率。 常用示例代码： 1234567891011121314import tesserocrfrom PIL import Imageimage = Image.open('code2.jpg')image = image.convert('L')threshold = 127table = []for i in range(256): if i &lt; threshold: table.append(0) else: table.append(1)image = image.point(table, '1')result = tesserocr.image_to_text(image)print(result) 难度较高的多位英数+扭曲图形验证码包括上面总结的中低难度的图形验证码，可以通过 Tensorflow 训练的方式达到识别验证码的目的。 之前我有一个系列文章介绍了整个训练流程，大家可以点击参考。 https://mp.weixin.qq.com/s/-BfjGC6KZNe2PJ47H85LIQ https://mp.weixin.qq.com/s/qVZtKveH8h2BQn2OjsXZYQ https://mp.weixin.qq.com/s/AfefH4b5HqNtxTVOjZ-XvA 使用这个方式的朋友记得要先准备好足够用的验证码的样本，只要你的模型不是太差，通过足量的样本，不断调优是可以达到一个较为可观的是识别率的。 目前体验过最好的程序是冷月的四位英数识别成功率高达 99.99% ，不过据知情人透露整个训练的样本达到了 6000 W ，耗费的时间精力可想而知。 还有一类解决方法是使用打码服务，这个之后再说。 旋转验证码这类验证码是将验证码的图片旋转并且需要用户拖动下方滑块完成将图片摆正的操作才可以完成验证。 目前国内市面的服务商还没有一个好的解决方案，不过某家的验证码有一些小小的 bug，依靠劳苦大众的智慧在 GitHub 上我发现了一个很 Nice 的项目。 项目地址：https://github.com/scupte/xuanzhaunyanz 因为图库的容量问题，没有超大的图库作为后盾，将全部的原图抓取下来对比完全可以得到旋转的角度了。 部分对比代码： 1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*-import cv2import numpy as npimagepath = '9_1.png'img = cv2.imread(imagepath)gray = cv2.cvtColor ( img , cv2.COLOR_BGR2GRAY )ret, binary = cv2.threshold(gray,127,255,cv2.THRESH_BINARY) contours, hierarchy = cv2.findContours(binary,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE) #cv2.drawContours(img,contours,-1,(0,0,255),1) for cnt in contours: # 最小外界矩形的宽度和高度 width, height = cv2.minAreaRect(cnt)[1] if width* height &gt; 100: # 最小的外接矩形 rect = cv2.minAreaRect(cnt) box = cv2.boxPoints(rect) # 获取最小外接矩形的4个顶点 box = np.int0(box) print box if 0 not in box.ravel(): #绘制最小外界矩形 for i in range(4): cv2.line(img, tuple(box[i]), tuple(box[(i+1)%4]), 0) # 5 theta = cv2.minAreaRect(cnt)[2] if abs(theta) &lt;= 45: print('图片的旋转角度为%s.'%theta) # angle = thetaprint theta cv2.imshow("img", img) cv2.waitKey(0) 滑动验证码说到滑动验证码，一定一定要提某验，虽然说市面上关于滑动验证码的产品有很多，但是某验的地位就像10年前脑白金在保健品市场的地位一样，业界标杆啊。 不过它越牛逼，市场上用它做防护的网站也越多，像国家企业信用信息公示系统、B 站、狗东等等。 像某验的解决方案也有很多，不过大同小异。 selenium 模拟滑动使用 selenium 这个大家都听过，步骤大致是将缺口图和原图进行对比获取缺口的横坐标，并使用一定的计算完成轨迹模拟，之后再使用 selenium 滑动完成缺口的拼接。 这一类方法的优点是门槛低，原理简单，缺点是完成滑动耗时较长，成功率无法估计（同一轨迹计算使用多次后成功率迅速下降） 常见的轨迹生成代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npimport mathdef ease_out_expo(x): """ 曲线函数 :param x: :return: """ if x == 1: return 1 else: return 1 - pow(2, -10 * x)def get_tracks(distance, seconds): """ 轨迹生成函数 :param distance: 滑动总距离 :param seconds: 滑动总时间 :return: """ tracks = [0] # 存放轨迹的数组 offsets = [0] # 存放滑动总距离的记录数组 for t in np.arange(0.0, seconds, 0.1): # 产生一个数列如[0.0, 0.1, 0.2, 0.3] offset = round(ease_out_expo(t/seconds) * distance) # 根据时间t计算在曲线上的滑动距离 tracks.append(offset - offsets[-1]) # 本次计算的距离减去上一次移动的距离，得到本次的轨迹 offsets.append(offset) # 至本次滑动了的总距离 return offsets, tracksa, b = get_tracks(138, 3)print(a, b)def get_tracksb(distance): """ 根据物理的先加速再减速规律计算 :param distance: :return: """ distance += 20 # 加上20是为了滑动超过缺口再回滑 v = 0 # 初速度 t = 0.2 # 以0.2秒为一个计算周期 forward_tracks = [] # 轨迹记录数组 current = 0 # 初始移动距离 mid = distance * 3 / 5 # 减速阀值即五分之三的距离加速剩下距离减速 while current &lt; distance: # 总移动距离等于输入距离时结束 if current &lt; mid: # 加速状态 a = 2 # 加速度为+2 else: # 减速状态 a = -3 # 加速度-3 s = v * t + 0.5 * a * (t ** 2) # 计算0.2秒周期内的位移 v = v + a * t # 计算本次周期后的速度 current += s # 将之前移动的总距离，加上本次0.2秒周期内移动的距离 forward_tracks.append(round(s)) # 记录本次0.2秒周期内的移动距离为轨迹 back_tracks = [-3, -3, -2, -2, -2, -2, -2, -1, -1, -1] # 手动将开头加上的20，生成减去轨迹，即回滑轨迹 return &#123;'forward_tracks': forward_tracks, 'back_tracks': back_tracks&#125; Js 破解关键的参数这类方法的门槛就比较高了，通过断点调试 Js，逆向分析滑动后提交的参数的生成逻辑完成参数的生成，之后构造请求完成提交，当然这中间也是需要分析图片的缺口位置与模拟轨迹，不过没有使用到模拟所以速度快成功率高。 缺点是风险高，代码维护成本高，更新一个新版本就要重新分析而且逆向相关产品的代码是有一定的法律风险的，免费包吃住也不是开玩笑的，所以很多能够商业化的大佬们都闷声发大财不会到处张扬。 使用现有的服务上面两种方法各有各的优缺，很多人就想把这一块的工作量与风险分出去，这就要使用到第三方的服务商了。 不过目前国内市场上的服务商并没有这类服务，目前咸鱼在使用的是一家俄罗斯的服务商 - 2Captcha 这个服务商提供的验证码服务有很多种，其中包含了我们比较关心的 GeeTest 。 下面咸鱼给大家简单介绍下如何使用服务。（不要问为啥收费，人家服务商也要吃饭，况且这个价格实在很便宜了） 首先，注册一个账号，官网是 http://2captcha.com/zh 完成注册之后会跳转到控制台界面，这里最重要的是获取到属于你的 API Key 。 好，拿到这个 API Key 之后就可以上手使用服务完成滑动的破解了。 通过参考官方的 API 文档，我们只需要构建 Get 请求就可以了。 第一个 Get 请求的组成是这样的： 1234567https://2captcha.com/in.php?key= 上面获取的API KEY &amp;method=geetest&amp;gt= 极验参数&amp;challenge= 极验参数&amp;api_server=api-na.geetest.com(可选)&amp;pageurl= 滑动验证码所在的网页地址 参数列表： 参数名 参数介绍 key API KEY method 表示验证码类型 gt 极验参数1 challenge 极验参数2 api_server api-na.geetest.com(选填) pageurl 滑动验证码所在的网页地址 这里解释下关于 gt 与 challenge 这两个参数的获取。 第一个请求中这两个参数其中 gt这个参数是固定的，找一个使用某验的网站就可以获取。例如： challenge这个参数是有一个 Get 请求返回，你找到这个请求之后按照请求重新获取一次，如果是 XHR 的话也可以直接 reply XHR 。 提交完第一个请求之后，会返回类似下面的结果。 1OK|2122988149 or as JSON &#123;"status":1,"request":"2122988149"&#125; 这里面的一串数字就是会话 ID。 有了这个会话 ID 之后我们就可以构建下一个请求了，这中间需要等待一些时间。 1234https://2captcha.com/res.php?key=API KEY&amp;action=get&amp;id=2122988149 参数列表： 参数名 参数介绍 key API KEY action Get id 上一个请求返回的会话ID 这个请求返回的结果就是我们需要的加密参数了。 12345&#123; "challenge":"1a2b3456cd67890e12345fab678901c2de", "validate":"09fe8d7c6ba54f32e1dcb0a9fedc8765", "seccode":"12fe3d4c56789ba01f2e345d6789c012|jordan"&#125; 常见的几类验证码，已经全部介绍完了。 肯定有有人问像 google 家的 ReCaptcha 以及和他相似的 hCaptcha 的解决方案没有提到啊？ 像以上两类验证码，刚刚提到的服务商也同样有提供接口打码，至于其他的解决方案，目前咸鱼还没有接触过，毕竟这两类验证码，咸鱼手动点击都没办法做到一次通过，目前也只能依赖服务商了。]]></content>
      <categories>
        <category>验证码</category>
      </categories>
      <tags>
        <tag>验证码</tag>
        <tag>TensorFlow</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-裁判文书网加密分析20190902版]]></title>
    <url>%2F2019%2F09%2F17%2F%E6%AF%8F%E6%97%A5JS-%E8%A3%81%E5%88%A4%E6%96%87%E4%B9%A6%E7%BD%91-190902%E7%89%88%2F</url>
    <content type="text"><![CDATA[裁判文书网20190902版列表页 __RequestVerificationToken搜索关键词__RequestVerificationToken 【图1-1】 找到base.random(24)这个方法，这个复制出来就解决了。 12345678function get_random(size)&#123; var str = "", arr = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z']; for(var i=0; i&lt;size; i++)&#123; str += arr[Math.round(Math.random() * (arr.length-1))]; &#125; return str;&#125; Ciphertext通过XHR断点，查看堆栈信息【图2-1】 加密位置为【图2-2】 直接扣取ciphertext这个方法的代码即可，这里展示部分代码： 12345678910111213cipher=function()&#123; var date = new Date(); var timestamp = date.getTime().toString(); var salt =get_random(24); var year=date.getFullYear().toString(); var month = (date.getMonth()+1&lt;10 ? "0"+(date.getMonth()+1) : date.getMonth()).toString(); var day = (date.getDate()&lt;10 ? "0"+date.getDate() : date.getDate()).toString(); var iv =year+month+day; var enc = DES3.encrypt(timestamp,salt,iv).toString(); var str = salt+iv+enc; var ciphertext = strTobinary(str); return ciphertext;&#125; pageld这个参数在首页点击时自动带入，经过测试并不是必须项，可以通过分析首页的源码查看到调用的方法。 12345678910function get_uuid()&#123; var guid = ""; for (var i = 1; i &lt;= 32; i++) &#123; var n = Math.floor(Math.random() * 16.0).toString(16); guid += n; // if ((i == 8) || (i == 12) || (i == 16) || (i == 20)) guid += // "-"; &#125; return guid;&#125;]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>裁判文书网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反反爬-巧破 Cloudflare 5秒盾]]></title>
    <url>%2F2019%2F09%2F16%2F%E5%8F%8D%E5%8F%8D%E7%88%AC-%E5%B7%A7%E7%A0%B4%20Cloudflare%205%E7%A7%92%E7%9B%BE%2F</url>
    <content type="text"><![CDATA[巧破 Cloudflare 5秒盾相信下面这个界面大家都不会陌生。【图1-1】 当我们第一次访问使用 CloudFlare 加速的网站时，网站就会出现让我们等待 5 秒种的提示，当我们需要的通过爬虫爬取这类网站的时候，应该如何爬取呢？ 分析请求首先我们需要分析在这个等待的时间里浏览器做了哪些操作。 通过抓包，我们可以看到在等待的过程中，浏览器做了下面的三次请求【图1-2】- 【图1-4】： 【图1-2】请求 1 写入 cookie 字段 __cfduid 【图1-3】请求 2 带有疑似加密的请求参数请求并写入 cookie 字段 cf_clearance 【图1-4】请求 3 带上前面写入的cookie 请求网站首页，返回首页内容。 这整个过程需要的请求现在已经分析清楚了，接下来就是使用 Python 实现这个请求流程，不过是这样的话就配不上这个标题了。 先说说这个按照正常流程是怎么实现抓取绕过的： 使用浏览器模拟技术请求目标网站，例如：Selenium、 PhantomJS等 破解请求 2 的加密参数使用请求库模拟整个请求过程 这两个方法当然是可以抓取的，但是都不符合标题的巧字。 接下来给大家介绍一个专门为了绕过这个 CloudFlare 开发的 Python 库 cloudflare-scrape 用上它就可以无感爬取使用了 CloudFlare 的网站，使用这个库非常简单。 安装使用 pip install cfscrape 安装cloudflare-scrape，同时确认本地是否安装node.js开发环境，如果没有，需要安装配置nodejs开发环境。 使用实例处理 get 请求的 CloudFlare 12345678import cfscrape# 实例化一个create_scraper对象scraper = cfscrape.create_scraper()# 请求报错，可以加上时延# scraper = cfscrape.create_scraper(delay = 10)# 获取网页源代码web_data = scraper.get("https://wallhere.com/").contentprint(web_data) 处理 post 请求的 CloudFlare 123456# import cfscrape# 实例化一个create_scraper对象scraper = cfscrape.create_scraper()# 获取真实网页源代码web_data = scraper.post("http://example.com").contentprint(web_data) 使用cloudflare-scrape后整个请求过程如丝般顺滑。 总结今天的水文到这里就结束了，很高兴又水了一篇文章。 之后抽个时间讲讲请求 2 中的加密参数如何生成，这样就可以再水一篇，敬请期待~ EOF]]></content>
      <categories>
        <category>反反爬</category>
      </categories>
      <tags>
        <tag>反反爬</tag>
        <tag>Cloudflare</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-58同城密码加密分析]]></title>
    <url>%2F2019%2F09%2F16%2F%E6%AF%8F%E6%97%A5JS-58%E5%90%8C%E5%9F%8E%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86%E4%B8%8E%E6%8C%87%E7%BA%B9%E5%88%86%E6%9E%90-RSA%2F</url>
    <content type="text"><![CDATA[目标网站： 123aHR0cHM6Ly9wYXNzcG9ydC41OC5jb20vbG9naW4vP3BhdGg9aHR0cHMlM0EvL2Z6LjU4LmNvbS8mUEdUSUQ9MGQxMDAwMDAtMDAxMy0wMjk0LTFjZWItYjU3NTBiZDIwNmU5JkNsaWNrSUQ9Mg== 抓包分析与加密定位老规矩先抓包看看【图1-1】 今天主要分析的是划线的三个参数，先一个一个来研究一下。 先看看密码的加密，按照之前的思路我们测试了password的相关搜索项，但是搜索出来的结果都不尽如人意。【图1-2】 同时这个请求也不是XHR请求，所以xhr断点也用不上了。 搜索的结果中比较有相关性的是【图1-2】标注的部分，但是在文件的搜索项里搜索password是找不到加密位置的。 我简单总结了几种办法，虽然并不一定适用于其他网站的加密，算是提供一种新的思路。 第一、搜索相关的的提示语，就像在之前的我做的第一个 APP 逆向的项目中，我就是搜索相关的提示语找到了扣费逻辑的代码，这里也可以按照这样的思路。 在这个网站的加密里点击登陆，点击后按钮变为登录中...，搜索相关提示语再逐步打断点一直追下去是可以找到加密位置的，不过这是个下下策，耗时太长，很容易追丢了，为了这个不值当。（不推荐使用） 第二、参考开头的搜索关键字的方法，这里搜索的是encrypt，经常遇到加密的变量或方法是encrypt(XX)这样的形式。 第三、试试从网页中找找答案，之前做过一些简单的例子是将 js 的代码直接写到页面中自执行或者采用隐藏域传递值的方式，比如：像PublicKey这类比较有特色的值，如果能找到对我们的帮助是很大的。 按照上面总结的几种方法，我们再次搜索【图1-3】 又搜索到这个熟悉的文件，这次我们进去搜索一次，这次找到了好多类似加密的地方，为了省事我把所有相关的值都打上了断点，再次登录。【图1-4】 到这里就定位到加密的地方了。 继续追进去可以看到进入的是一个VM【图1-5】 到这里就没有什么难度了，密码的加密就是 RSA + eval。 如果你很幸运先找到了eval加密的地方，但是不知道怎么解决，可以参考之前一篇关于js混淆的文章。 —-&gt; 点我看文章 或者直接打开浏览器的开发者工具切换到console选项卡，把相关的eval代码粘贴进去，将代码开头的eval替换为console.log，就可以得到原来的代码了。【图1-6】 接下来看的是两个指纹相关的参数，这里就比较简单了，直接搜索fingerprint就可以找到相关加密文件，直接在里面打断点就可以了。【图1-7】 这里的注意点是想要重新生成指纹，记得要刷新页面才可以。 尾关于指纹的加密生成讲的比较简单，需要学习案例的朋友可以访问下面的网址。 https://lengyue.me/ 也可以看看冷月打来之前的关于某个网站指纹加密的视频教程。 链接 : https://pan.baidu.com/s/1Y8pcacZbHF1DXW0znzmZKQ 密码:vkmf 失效请留言。 EOF]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
        <tag>58同城</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-叮当快药 sign 加密分析]]></title>
    <url>%2F2019%2F09%2F16%2F%E6%AF%8F%E6%97%A5JS-%E5%8F%AE%E5%BD%93%E5%BF%AB%E8%8D%AF-MD5%2F</url>
    <content type="text"><![CDATA[今日份的网站 12aHR0cDovL3d3dy5kZGt5LmNvbS9jb21tb2RpdHkuaHRtbD9kZGt5Y2FjaGU9YTdiMTllODc5ZDJmMmYyNzlkMzU2ZjVhZmE2ZDVjZmY= 这个网站的加密比较简单，是那种新手做过一遍就对 JS 逆向充满信心的小练手。 分析请求先分析请求，看看需要的参数有没什么搞头【图1-1】 比较明显的是 sign 其他的参数好像没有什么特别的地方。 先定位 sign 的位置【图1-2】 直接搜索参数就看到结果了，点击搜索的结果文件在文件内再搜索一次，可以看到有 3 个结果。【图1-3】 到这里就看到 sign 的加密了。 接下来继续分析逻辑，这里的 sign 值是 y，y的值是通过MD5(f)获取的，而f又是t.get(&quot;method&quot;) + p + r的结果，经过一通分析，可以的得到下面的逻辑： 1234567var l = t.keys().sort(), p = ""; l.length;for (var g in l) &#123; var m = l[g]; p += m + t.get(m)&#125;sign = md5(t.get("method") + p + r) 接下来只需要把我们不知道的值通过断点的方式调试出来，这个加密我们就完成解密了。 所以先给不知道的值打上断点，不清楚就打上断点不要怕麻烦，像【图1-4】这样。 重新请求一下，就进入到我们打的断点里了，我们可以通过在 console 中打印变量的值来理解逻辑。【图1-5】 明白需要的变量的值之后，我们就可以开始扣取 JS 或者用 Python 复写加密的逻辑了。 因为这次的加密比较简单，我们试试用 Python 复写一遍加密。（主要是 Python 的 md5 用起来蛮舒服的） 这里的逻辑比较简单没啥好分析的，我就直接上代码了。 12345678910111213141516171819202122232425262728293031323334353637import timefrom hashlib import md5def get_sign(): timeStamp = time.time() localTime = time.localtime(timeStamp) strTime = time.strftime("%Y-%m-%d %H:%M:%S", localTime) l = ["method", "orderTypeId", "orgcode", "pageNo", "pageSize", "plat", "platform", "shopId", "t", "v", "versionName"] t = &#123; 'method': 'ddsy.product.query.orgcode.product.list.b2c', 'orderTypeId': '0', 'orgcode': '010502,010503,010504,010505,010506,010507', 'pageNo': '1', 'pageSize': '100', 'plat': 'H5', 'platform': 'H5', 'shopId': '-1', # 't': '2019-9-23 22:4:16', 't': '&#123;&#125;'.format(strTime), 'v': '1.0', 'versionName': '3.2.0' &#125; p = '' for i in range(0, 11): m = l[i] p += m + t.get(m) f = t['method'] + p + '6C57AB91A1308E26B797F4CD382AC79D' print(f) sign = md5value(f).upper() print(sign) return signdef md5value(s): a = md5(s.encode()).hexdigest() return a 到这里其实就没有什么难度了，直接带入 sign 请求就完事了。【图1-6】 EOF]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>sign</tag>
        <tag>叮当快药</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-新浪二手房与房天下加密分析]]></title>
    <url>%2F2019%2F09%2F07%2F%E6%AF%8F%E6%97%A5JS-%E6%88%BF%E5%A4%A9%E4%B8%8B%2B%E6%96%B0%E6%B5%AA%E4%BA%8C%E6%89%8B%E6%88%BF-RSA%2F</url>
    <content type="text"><![CDATA[X天下密码加密分析本次的受害者：1aHR0cHM6Ly9wYXNzcG9ydC5mYW5nLmNvbS8= 分析通过输入错误密码抓包查看加密字段。【图1-1】 直接通过检索pwd:定位加密位置【图1-2】。 根据检索结果给对应的位置打上断点【图1-3】，并把划线的代码复制出来备用。 重新发起请求，发现这些参数大概是下面这样的： 123// that.password.val() 输入的密码：11111111111// encryptedString 是加密方法encryptedString(key_to_encode, that.password.val()) 根据上面分析就少了一个 key_to_encode，直接检索可以找到下面的结果【图1-4】： 很明显的 RSA 加密，同时追进去就看到encryptedString的加密逻辑了，一起扣出来组装一下就得到加密逻辑了【图1-5】： XX二手房密码加密分析本次的受害者： aHR0cDovL2ouZXNmLmxlanUuY29tL3VjZW50ZXIvbG9naW4= 分析同样使用错误密码测试登陆【图2-1】 这里password和ckey两个参数疑似加密，我们先检索一遍。 ckey是在页面适用隐藏域传递的值【图2-2】 password的检索结果比较多，不过从文件名看第三个文件关联性比较强。【图2-3】 我们打开第三个文件，并格式化搜索一下【图2-4】。 有没有发现这个加密规律和上面 X天下 的加密非常像，而且还有RSA的加密标志，publickey 连下面的 encryptedString 加密方法都似曾相识【图2-5】？同时参考 X天下例子的【图1-3】 到这里就没有悬念了，直接用上一个例子的代码就可以实现加密，如果你想自己扣出来也是可以的，但是咸鱼感觉没有必要。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
        <tag>新浪二手房</tag>
        <tag>房天下</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-粉笔网密码加密]]></title>
    <url>%2F2019%2F09%2F06%2F%E6%AF%8F%E6%97%A5JS-%E7%B2%89%E7%AC%94%E7%BD%91-RSA%2F</url>
    <content type="text"><![CDATA[1aHR0cHM6Ly9mZW5iaS5jb20vcGFnZS9ob21l 参数加密的逻辑分析先抓包看看这次分析的参数【图1-1】 这个参数的值看着像Base64 ，我们不急着下结论，先搜索参数名看看。 有读者朋友纠结于搜索password = or password：这样需要搜索两遍是不是浪费时间了，不如直接搜索password来的方便快捷。 咸鱼之所以这样搜索是出于自己的习惯，定位加密位置的方法有很多大家有兴趣可以多尝试不要局限于搜索参数，例如：Js Hook，XHR 断点等等，怎么舒服怎么来就好。 经过搜索参数名password:在一个文件中定位到 3 处疑似加密的位置。【图1-2】 这里有两种方法判断加密位置： 给所有搜索到的结果打上断点，再次刷新看看进入到哪个断点当中 阅读上下文，分析大概的代码逻辑 我们用第一种试试，打上断点重新发起请求，可以看到成功断上了。【图1-3】 这里的this.password是测试的密码，我们需要分析的就是这个this.encrypt的逻辑是什么样的。 我们进入到this.encrypt这个函数中，发现有熟悉的 RSA 加密标志this.publicKey【图1-4】 在文件中再搜索一次publicKey，能够看到文件中已经声明了这个变量【图1-5】 继续进入到函数内部查看逻辑，发现进入的就是加密逻辑的文件了。【图1-6】 这一整个流程分析下来，可以发现前面的传参部分，在Python调用中我们完全可以跳过，只要给【图1-6】中的Js加密逻辑传入publicKey与明文密码就可以实现这个网站的密码加密逻辑了，既然这么简单，我们就动手试试。 参数加密的代码实现接上面的分析，我们直接复制了【图1-6】截图所示的全部代码。 接着在编辑器中加上一些我们自己的逻辑类似这样【图2-1】。 注：前939行都是复制的 Js 文件代码 试着运行一下，看看报错。 运行的报错提示，window is undefine【图2-2】，遇到这种情况我们可以试试在代码上加上window的声明。 1var window = &#123;&#125; 再次运行看看，这里再次提示{} is not a function，【图2-3】到这里新手朋友有点慌了，不知道怎么处理。 比较方便的处理方式是直接在(function(av) {前加上!，就是这么简单。 具体用法，这里套用一下百度： 使用括号包裹定义函数体，解析器将会以函数表达式的方式去调用定义函数。也就是说，任何能将函数变成一个函数表达式的作法，都可以使解析器正确的调用定义函数。而 ! 就是其中一个，而 + - || 都有这样的功能。 继续调试，这次提示变成了navigator is not defined【图2-4】 有了上次的经验，我们在代码里加入声明navigator。 1var navigator = &#123;&#125; 再次运行就得到加密后的结果咯~【图2-5】 总结这次的加密是比较简单的 RSA 加密，使用文件中包含的公钥对密码的值进行加密，且Js代码没有进过混淆，适合新手练手增加手感。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>RSA</tag>
        <tag>密码加密</tag>
        <tag>粉笔网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-极贷助手密码加密]]></title>
    <url>%2F2019%2F09%2F05%2F%E6%AF%8F%E6%97%A5JS-%E6%9E%81%E8%B4%B7%E5%8A%A9%E6%89%8B-aes%2F</url>
    <content type="text"><![CDATA[极X助手1aHR0cHM6Ly93d3cuamlkYWlob21lLmNvbS9vZmZpY2VyLyMvbG9naW4= 先抓包看看这次要分析的参数。【图1-1】 看到【图1-1】里是没有加密参数名的。所以为了快速定位到加密的位置，我们试试 XHR 断点。 切换到 控制台 - Source 选项卡 ，如【图1-2】位置填入下面的内容。 接着我们再发起一次请求，可以看到断点打上了。【图1-3】 我们点击左下角的{}格式化 JS 看看断点断上的位置有没有我们需要的内容。【图1-4】 我们可以看到这里函数 y传入一个参数t，t中包含我们需要的提交数据【图1-5】。但是在这个断点位置并不能找到参数加密的地方，所以我们需要点击右侧的堆栈往上看看。 我们通过查看堆栈信息，找到了这里【图1-6】。发现在这里出现了 params相关的的操作，所以这里的可能包含的就是我们需要的加密位置，我们根据提示进去看看这里的加密逻辑是什么样的。 可以看到这里把经过处理的密码和用户名传入后，再次做了另一次加密处理后得到了变量i。 1var r = t.randomKey(16), i = t.aesEncrypt(JSON.stringify(e), r); 我们分别把变量i需要的参数分别跟进去看看生成规则。【图1-8】是变量r的生成规则是一串随机的字符串。 接下来是加密变量i的加密方法【图1-9】。 接下来只需要把这些参数的方法复制到编辑器中进一步的补全就可以了。 Tip: 这里的参数e是我们传入的用户名和密码，在【图1-6】的 54-55 行中传入，且密码经过了一层MD5的加密。 这里很多朋友纠结于类似 i.enc.Utf8.parse 这里的i如何补全，其实这里的i 就是我们常用的CryptoJS，只要引入后，直接替换就可以了，不用扣取整个JS。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
        <tag>极贷助手</tag>
        <tag>AES</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-空中网密码加密]]></title>
    <url>%2F2019%2F09%2F04%2F%E6%AF%8F%E6%97%A5JS-%E7%A9%BA%E4%B8%AD%E7%BD%91%2F</url>
    <content type="text"><![CDATA[1aHR0cHM6Ly9wYXNzcG9ydC5rb25nemhvbmcuY29tL2xvZ2lu 参数位置分析先来看看加密的请求【图1-1】，之前在一篇加密总结的文章中提到过关于这个网站的加密，感兴趣的朋友可以回顾一下。 之前介绍过很多关于加密参数如何搜索的文章，就不啰嗦咯。 除了搜索加密参数之外，同样还要注意id 和 calssname等标志性的属性，能够帮助我们进一步定位加密位置。 通过密码框的id = password_txt 快速定位至网站的加密入口。【图1-2】 通过逐步调试进入虚拟引擎中加密码的位置打上断点。【图1-3】 上图中的encrypt这个方法才是我们需要的加密方法，所以继续最近去看看逻辑。 这里通过悬停在对应方法上没有提示加密的位置的，小伙伴表示无法进入下一步，我们可以选中方法后点击提示即可跳转【图1-4】 顺利跳转进加密方法的位置后，就可以直接抠出来补全了。【图1-5】 tip: 这里的第二个参数是在【图1-1】中上一条请求中返回的哦 补全加密JS先扣取【图1-5】中框选的方法到编辑器中，根据断点把需要的参数传入【图2-1】 这样在编辑器中的结果就像现在这样了【图2-2】 运行之后就是得到的加密参数喽。 总结这个网站的加密比较适合有一点点的新手作为练手项目，涉及多次的浏览器调试可以很好的帮助新手进一步的了解浏览器的调试技巧。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
        <tag>空中网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-咪咕视频密码加密与指纹分析]]></title>
    <url>%2F2019%2F09%2F03%2F%E6%AF%8F%E6%97%A5JS-%E5%92%AA%E5%92%95%E8%A7%86%E9%A2%91-RSA%2F</url>
    <content type="text"><![CDATA[正文先来看看今天的受害者： 1aHR0cDovL3d3dy5taWd1dmlkZW8uY29tL21ncy93ZWJzaXRlL3ByZC9pbmRleC5odG1s 一、分析密码加密这次分析的是他登陆的三个参数，先分析登陆逻辑，抓个包看看。 可以看到这里用的是弹出的窗口登陆【图1-1】，为了避免主页其他元素的干扰抓包，我们可以通过右键 查看框架源代码 的方式打开登陆框分析抓包【图1-2】。 把打开的源代码页面地址栏中的 view-source:删除即可打开以下页面【图1-3】： 接下来用错误的账号密码登陆一波，可以看到如下的包【图1-4】，不过通过上面的 publickey 隐隐猜到这里的加密参数可能和 RSA 有关，先来找找密码加密的位置： 照常搜索一下 enpassword ，预料之中是没有搜索到有用的结果【图1-5】，不过定位元素可以通过 name 定位 也可以通过 class 定位，所以同样试试 J_RsaPsd，果然找到像样的文件了【图1-6】。 在打开的文件里检索J_RsaPsd，可以看到有3个相关的结果(熟悉加密的已经可以看出这里是 RSA 加密)，我们通过把所有相关的结果都打上断点【图1-6】，并通过重新发起一次登录请求，来判断哪个J_RsaPsd是密码的加密逻辑。 通过重新发起请求，断点断在 333 行【图1-8】。 我们先复制整段代码： 123c.setPublic(a.result.modulus, a.result.publicExponent);var d = c.encrypt(b.val());b.siblings(".J_RsaPsd").val(d) 我们通过控制台看看这些参数分别是什么【图1-9】。 通过前文抓包和JS页面的代码，我们判断密码的加密是RSA加密，所以我们就需要找齐RSA加密的需要的几个要素，比如他的公钥 ，因为RSA是非对称加密本地使用公钥加密，服务器上使用私钥解密。再看看【图1-8】上的几个参数不知道你们有没想起开头【图1-4】我提及的那条抓包。我们切换到 控制台-Network 看看【图1-10】。 1&#123;"status":2000,"message":"","header":&#123;&#125;,"result":&#123;"publicExponent":"010001","modulus":"00833c4af965ff7a8409f8b5d5a83d87f2f19d7c1eb40dc59a98d2346cbb145046b2c6facc25b5cc363443f0f7ebd9524b7c1e1917bf7d849212339f6c1d3711b115ecb20f0c89fc2182a985ea28cbb4adf6a321ff7e715ba9b8d7261d1c140485df3b705247a70c28c9068caabbedbf9510dada6d13d99e57642b853a73406817"&#125;&#125; 是不是和我们在【图1-9】中打印出来的数值完全相同 tip : 这里的b.val()是我输入的错误密码。 这里我们找齐了加密需要的参数，其实我觉得没必要扣JS，但是我知道。。 不要我觉得，你要你觉得 所以偷懒用之前扣的代码测试一下： 1234567891011121314151617181920-----------此处省略500行-----------function bodyRSA()&#123; //setMaxDigits(130); var key = RSAUtils.getKeyPair("010001","","00833c4af965ff7a8409f8b5d5a83d87f2f19d7c1eb40dc59a98d2346cbb145046b2c6facc25b5cc363443f0f7ebd9524b7c1e1917bf7d849212339f6c1d3711b115ecb20f0c89fc2182a985ea28cbb4adf6a321ff7e715ba9b8d7261d1c140485df3b705247a70c28c9068caabbedbf9510dada6d13d99e57642b853a73406817"); return key&#125;function get_encrypt(password) &#123; key = bodyRSA(); var a = RSAUtils.encryptedString(key,password) //var b = RSAUtils.encryptedString(key,username) console.log(a) console.log('----------------------------------') //console.log(b) return a&#125;get_encrypt('11111111111') 二、分析FingerPrintDetail与fingerPrint根据第一部分的【图1-5】和图【1-6】可以很快定位加密的位置，这里就不说了【图2-1】。 追进去分析一下rsaFingerprint的逻辑，可以看到其实和密码加密用的是一样的RSA加密【图2-2】只是部分参数不同。 这里比较疑惑的是$.fingerprint.result和$.fingerprint.details是怎么来的。 通过检索$.fingerprint根据查找的结果找到他的位置【图2-3】 继续向上查找，最终找到这里，发现是根据浏览器的请求头等信息生成的一串哈希值【图2-4】。 这个值如果不修改请求头信息，那么这个值就是固定不变的，如果修改请求头信息，那么就需要重新生成这段hash，这就是这个的浏览器指纹信息。 因为我们这里只是抱着学习的态度学习，所以就不瞎搞，不过这个整段的代码也比较简单没事可以扣一扣。 tip: 如果想要分析这个指纹的算法，记得要刷新一下页面才可以进入断点。 三、总结结合之前的建议，文章的配图都标注了序号，希望能够起到一定梳理思路的作用。 这个网站的加密总体还是只用RSA的加密算法就可以搞定了，整体加密算法比较简单有经验的老哥直接通过加密库就可以实现，如果想练练手的也可以抠出来试试，只要头发多，肝就完事了。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
        <tag>咪咕视频</tag>
        <tag>指纹加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-楚楚街密码加密]]></title>
    <url>%2F2019%2F09%2F02%2F%E6%AF%8F%E6%97%A5JS-xx%E8%A1%97%E7%99%BB%E9%99%86%E5%AF%86%E7%A0%81%E5%8A%A0%E5%AF%86-md5%2Brsa%2F</url>
    <content type="text"><![CDATA[XX 街登陆密码加密1aHR0cDovL3NlbGxlci5jaHVjaHVqaWUuY29tL3NxZS5waHA/cz0vVXNlci9pbmRleA== 这个加密太简单了，五秒定位真的不是吹，所以直接来。 输入错误的账号密码，发起登陆请求，可以看到登录包里有个password的加密字段。所以我们依次搜索password:+password :+password=+password = 很快得到下面的结果【图1-1】，直接用python实现就完事了~ X博登陆密码与相关加密参数解析aHR0cHM6Ly93d3cud2VpYm8uY29tL2xvZ2luLnBocA== 用 Chrome 浏览器抓包相信大家都会，前面抓包我们跳过，直接到加密参数分析的地方。 Tip: 这次的加密不是异步的 不要在 XHR 选项卡傻乎乎的等了。 微博登陆框是嵌在页面中的加上微博的登陆页面图片以及要加载的东西很多，所以我们需要注意的包主要有两个： 登陆前返回相关加密秘钥的包【图2-1】 发起登陆请求的包【图2-2】 到这里就很清晰了，我们搞定 su 和 sp 就完事了。 先来看看 su 这个其实不要搜索，看着是不是很像我们开头的编码结果，由 a-zA-Z[0-9]+/= 这几种字符组成的编码 熟悉的就只有 Base64 , 我们直接找一个在线工具验证一下我们的猜想。【图2-3】 当然我们也可以试试搜索 参数名 su 【图2-4】，很明显搜索结果也验证了我们的猜想。 接下来看看 sp 在我们上面搜索 su 的地方我们找到了密码加密的地方，已经用红框框出主要的逻辑【图2-5】，接下来我们打上断点找找缺失的参数都是什么。 这里的参数都是比较简单的首先是 me.servertime 、me.rsaPubkey 和 me.nonce【图2-6】 这三个参数在上一个请求包都有同名的参数返回可以参考【图2-1】。 我们现在已经找齐了全部的参数，接下来需要切换到 webstorm 中调试出我们整体的加密算法。 直接粘贴 789 - 791 行的代码到编辑框中补全成下面这样。 1234567function get_sp() &#123; var f = new sinaSSOEncoder.RSAKey; f.setPublic(me.rsaPubkey, "10001"); b = f.encrypt([me.servertime, me.nonce].join("\t") + "\n" + b) return b&#125;get_sp() 虽然知道运行肯定报错，但是我们需要的就是报错的信息。【图2-7】 提示 sinaSSOEncoder is not defined ，我们回到浏览器调试窗口找 sinaSSOEncoder在哪儿定义的。可以直接在页面中搜索 var sinaSSOEncoder 结果就只有一个【图2-8】 接下来复制 sinaSSOEncoder的全部内容到编辑器中，第1048-1981行。继续运行并提示navigator is not defined，我们定义它为{}。接下来会提示 me 未定义【图2-9】。 但是我们调试过都知道这个 me.rsaPubkey 是在【图2-1】中返回的，所以我们把它替换掉。同理替换到我们已知的的其他参数。现在就只剩下 b 参数是未知的了。 我们向下运行，可以看到b参数就是我们输入的密码【图2-10】，我们继续替换再次运行，发现没有东西输出，也没有报错，我们在函数中加上一句 打印语句，看看返回的 b是什么结果，可以看到已经得到了加密后的结果了。【图2-11】 总结这次主要把如何扣一个简单的加密算法做了详解，这里包含了基础的JS知识，或许看不明白为什么是复制这些代码而不是其他的，这里我建议可以适当补充一些JS基础语法的知识。等到下次有类似的加密算法时，你可以自己做出判断复制扣取哪些，这样学的更快。 JS逆向学习的文章每一篇都是很简单的案例，希望看完可以自己动手扣一遍。 每天一题，头发掉的更快。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Apk打包流程梳理和加载流程学习]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%AE%89%E5%8D%93%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A0-apk%E6%89%93%E5%8C%85%E6%B5%81%E7%A8%8B%E6%A2%B3%E7%90%86%2F</url>
    <content type="text"><![CDATA[想要学习逆向必须先熟悉APP编译的流程。 安卓加载流程java 代码 – &gt; 加载布局文件 –&gt; 资源文件 资料参考链接： https://juejin.im/entry/58b78d1b61ff4b006cd47e5b 新版官网示意图 编译器将您的源代码转换成 DEX（Dalvik Executable) 文件（其中包括运行在 Android 设备上的字节码），将所有其他内容转换成已编译资源。 APK 打包器将 DEX 文件和已编译资源合并成单个 APK。不过，必须先签署 APK，才能将应用安装并部署到 Android 设备上。 APK 打包器使用调试或发布密钥库签署您的 APK： 如果您构建的是调试版本的应用（即专用于测试和分析的应用），打包器会使用调试密钥库签署您的应用。Android Studio 自动使用调试密钥库配置新项目。 如果您构建的是打算向外发布的发布版本应用，打包器会使用发布密钥库签署您的应用。要创建发布密钥库，请阅读在 Android Studio 中签署您的应用 在生成最终 APK 之前，打包器会使用 zipalign 工具对应用进行优化，减少其在设备上运行时的内存占用。 旧版官网示意图 通过aapt打包res资源文件，生成R.java、resources.arsc和res文件（二进制 &amp; 非二进制如res/raw和pic保持原样） 处理.aidl文件，生成对应的Java接口文件 通过Java Compiler编译R.java、Java接口文件、Java源文件，生成.class文件 通过dex命令，将.class文件和第三方库中的.class文件处理生成classes.dex 通过apkbuilder工具，将aapt生成的resources.arsc和res文件、assets文件和classes.dex一起打包生成apk 通过Jarsigner工具，对上面的apk进行debug或release签名 通过zipalign工具，将签名后的apk进行对齐处理。 安卓项目的文件结构与安卓apk的文件结构存在着一一对应的关系。 安卓应用开发的本质是：将源代码和各种资源文件编译整合成一个apk。 安卓逆向的本质是：想办法将apk转化为源代码和资源文件。 简单来说，apk就是一个带有签名的zip格式的压缩包，签名为了保护开发者的权益和标识apk。做为android逆向学习的第一步，了解apk的文件结构和生成过程是很有必要的。为了提升apk的安全性能，现在很多安卓应用程序的核心代码都采用NDK开发，所以生成的apk中会多出一个lib文件夹用于存放so文件。]]></content>
      <categories>
        <category>App逆向</category>
      </categories>
      <tags>
        <tag>打包流程</tag>
        <tag>加载流程</tag>
        <tag>APP编译</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[App逆向实基础学习]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%AE%89%E5%8D%93%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A01-%E9%80%86%E5%90%91%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[常见加密算法编码 escape unescape encodeURIComponent decodeURIComponent Base64 所有的数据都能被编码为只用65个字符就能表示的文本。标准的Base64每行为76个字符，每行末尾添加一个回车换行符(\r\n)。不论每行是否满76个字符，都要添加一个回车换行符。 65字符：A~Z a~z 0~9 + / =URL Base64算法中，为了安全，会把 + 替换成 - ，把 / 替换成 _= 有时候用 ~ 或 . 代替 Base64的应用密钥,密文,图片,数据简单加密或者预处理 Base64编码解码与btoa、atob Hex 二进制数据最常用的一种表示方式。 用0-9 a-f 16个字符表示。每个十六进制字符代表4bit。也就是2个十六进制字符代表一个字节。 在实际应用中，尤其在密钥初始化的时候，一定要分清楚自己传进去的密钥是哪种方式编码的，采用对应方式解析，才能得到正确的结果 单向散列函数(消息摘要算法) 不管明文多长，散列后的密文定长 明文不一样，散列后结果一定不一样 散列后的密文不可逆 一般用于校验数据完整性、签名 sign 由于密文不可逆，所以后台无法还原，也就是说他要验证，会在后台以跟前台一样的方式去重新签名一遍。也就是说他会把源数据和签名后的值一起提交到后台。所以我们要保证在签名时候的数据和提交上去的源数据一致,这种算法特喜欢在内部加入时间戳 常见算法 MD5 SHA1 SHA256 SHA512 HmacMD5 HmacSHA1 HmacSHA256 加密算法加密和解密的过程是可逆的 对称加密算法 加密/解密使用相同的密钥 DES 数据加密标准 3DES TripleDES DESede AES 高级加密标准 根据密钥长度不同又分为AES-128 AES-192 AES-256 其中AES-192 AES-256在Java中使用需获取无政策限制权限文件 CryptoJS提供ECB,CBC,CFB,OFB,CTR五种模式填充提供NoPadding ZeroPadding Pkcs7(Pkcs5) Iso10126 Iso97971 AnsiX923 非对称加密算法 RSA 使用公钥加密，使用私钥解密 公钥是公开的，私钥保密 加密处理安全，但是性能极差，单次加密长度有限制 pkcs1padding 明文最大字节数为密钥字节数-11密文与密钥等长 NoPadding 明文最大字节数为密钥字节数 密文与密钥等长 RSA既可用于数据交换，也可用于数据校验 数据校验通常结合消息摘要算法 MD5withRSA 等 两种加密算法常见结合套路 随机生成密钥 密钥用于AES/DES/3DES加密数据 RSA对密钥加密 提交加密后的密钥和加密后的数据给服务器 APK基本结构 lib:各种平台下使用的对应的so文件 META-INF文件夹:存放工程一些属性文件 CERT.RSA:公钥和加密算法描述 CERT.SF:加密文件，他是使用私钥对摘要明文加密后得到的密文信息,只有使用私钥配对的公钥才能解密该文件 MANIFEST.MF:程序清单文件，他包含包中所有文件的摘要明文 resource.arsc:资源加密(语言包)对res目录下的资源的一个索引文件,保存了原工程中strings.xml等文件内容 drawable:图片 layout:布局 menu:菜单 AndriodMainfest.xml:清单文件(图标、界面、权限、入口),安卓工程的基础配置属性文件。 classes.dex:java代码编译得到的Dalvik VM能直接执行的文件 assets:资源文件(图片、音频、数据库、网页、配置文件等) res:资源文件，需要编译 res目录与assets目录区别在哪？ res目录下的资源文件在编译时会自动生成索引文件(R.java),在java代码中用R.xxx.yyy来引用 asset目录下的资源文件不需要生成索引，在java代码中需要用AssetManager中访问。 一般来说,除了音频和视频资源(需要放在raw或asset下),用java开发的安卓工程使用到的资源文件都会放到res下；使用c++游戏引擎的资源文件均需要放在asset下。 JVM、DVM与ART JVM:Java虚拟机，运行的是.java文件编译后的.class文件 DVM:Android4.4及以前使用的都是Dalvik虚拟机，我们知道Apk在打包的过程中会先将java等源码通过javac编译成.class文件，但是我们的Dalvik虚拟机只会执行.dex文件，这个时候dx会将.class文件转换成Dalvik虚拟机执行的.dex文件。Dalvik虚拟机在启动的时候会先将.dex文件转换成快速运行的机器码，又因为65535这个问题，导致我们在应用冷启动的时候有一个合包的过程，最后导致的一个结果就是我们的app启动慢，这就是Dalvik虚拟机的JIT特性。 Dalvik是google专门为安卓操作系统设计的一个虚拟机，经过深度的优化,虽然安卓上的程序是使用java来开发的，但是Dalvik和标准的java虚拟机JVM还是两回事,Dalvik VM是基于寄存器的，而JVM是基于栈的；Dalvik有专属的文件执行格式dex,JVM则执行的是java字节码。 通过Dalvik的字节码我们不能直接看到原来的逻辑代码,这是需要借助如Apktool或dex2jar+jd-gui工具来帮助查看,但是注意的是最终我们修改APK需要操作的是.smali文件，而不是导出来的java文件重新编译。 ART:ART虚拟机是在Android5.0才开始使用的Android虚拟机，ART虚拟机必须要兼容Dalvik虚拟机的特性，但是ART有一个很好的特性AOT（ahead of time），这个特性就是我们在安装APK的时候就将dex直接处理成可直接供ART虚拟机使用的机器码，ART虚拟机将.dex文件转换成可直接运行的.oat文件，ART虚拟机天生支持多dex，所以也不会有一个合包的过程，所以ART虚拟机会很大的提升APP冷启动速度。Xposed hook的是Java代码，所以Xposed不支持5.0以及以上系统。 安卓分区Android通常有以下分区（用df 来查看分区情况） System分区: 就是我们刷ROM的分区 Data分区: 分区就是我们装APK的分区 Catch分区: 是缓存分区 SDCard分区: 就是挂载的SD卡。 data分区常见目录：app、system、data、local、misc data/data目录存放的是所有APK程序数据的目录，每个APK对就一个自己的Data目录，就是在data/data/目录下，会产生一个跟 Package一样的目录。比如有一个APK,它的包名叫com.test.hello则,在data/data/目录下会有一个 com.test.hello的目录,这个APK只能操作此目录,不能操作其它APK的目录 data/app目录用户安装的APK放在这里。我们如果把APK放入这个文件夹下面的话，就算安装好了。这就叫静默安装。不用管APK文件里面的lib目录下的库文件，系统会自动帮我们放入调用库 data/misc目录保存WIFI帐号，VPN设置信息等。比如保存了一个WIFI连接帐号，则此目录下的WIFI目录下面wpa_supplicant.conf可以查看到 system分区常用目录：app、lib、xbin、bin、media、framework system/app目录存放系统自带的APK。将APK放入到System/app目录下，也是静默安装 system/lib目录存放APK程序用到的库文件 system/bin目录和system/xbin目录存放的是shell命令 system/framework目录启用Android系统所用到框架，如一些jar文件 sd卡目录：/sdcard /mnt/sdcard]]></content>
      <categories>
        <category>App逆向</category>
      </categories>
      <tags>
        <tag>基础</tag>
        <tag>app逆向</tag>
        <tag>编码</tag>
        <tag>加密</tag>
        <tag>apk基本结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[App签名]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%AE%89%E5%8D%93%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A01.2-apk%E7%AD%BE%E5%90%8D%2F</url>
    <content type="text"><![CDATA[什么是签名？如果这个问题不是放在Android开发中来问，如果是放在一个普通的版块，我想大家都知道签名的含义。可往往就是将一些生活中常用的术语放在计算机这种专业领域，大家就开始迷惑了。计算机所做的事情，或者说编程语言所做的事情，不正是在尽可能地模拟现实吗？所以，计算机中所说的签名和生活中所说的签名在本质上是一样的，它所起到的作用也是一致的！ 为什么要给Android应用程序签名？如果只能用一句简单的话语来回答这个问题的话，我会说：“这是Android系统所要求的”。Android系统要求每一个Android应用程序必须要经过数字签名才能够安装到系统中，也就是说如果一个Android应用程序没有经过数字签名，是没有办法安装到系统中的！Android通过数字签名来标识应用程序的作者和在应用程序之间建立信任关系，不是用来决定最终用户可以安装哪些应用程序。这个数字签名由应用程序的作者完成，并不需要权威的数字证书签名机构认证，它只是用来让应用程序包自我认证的。 为什么我开发的Android应用程序没有做什么签名也能在模拟器和手机上运行？你没有给Android应用程序签名并不代表Android应用程序没有被签名。为了方便我们开发调试程序，ADT会自动的使用debug密钥为应用程序签名。debug密钥？它在哪？debug密钥是一个名为debug.keystore的文件，它的位置：系统盘符:/Documents and Settings/liufeng/.android/debug.keystore liufeng对应于你自己的windows操作系统用户名，怎么样，是不是已经找到它了。这也就意味着，如果我们想拥有自己的签名，而不是让ADT帮我们签名的话，我们也要有一个属于自己的密钥文件（*.keystore）。 如果你以前的程序是采用默认签名的方式（即debug签名），一旦换了新的签名应用将不能覆盖安装，必须将原先的程序卸载掉，才能安装上。因为程序覆盖安装主要检查两点：1）两个程序的入口Activity是否相同。两个程序如果包名不一样，即使其它所有代码完全一样，也不会被视为同一个程序的不同版本；2）两个程序所采用的签名是否相同。如果两个程序所采用的签名不同，即使包名相同，也不会被视为同一个程序的不同版本，不能覆盖安装。另外，可能有人可能会认为反正debug签名的应用程序也能安装使用，那也没有必要自己签名了嘛。千万不要这样想，debug签名的应用程序有这样两个限制，或者说风险：1）debug签名的应用程序不能在Android Market上架销售，它会强制你使用自己的签名； 2）debug.keystore在不同的机器上所生成的可能都不一样，就意味着如果你换了机器进行apk版本升级，那么将会出现上面那种程序不能覆盖安装的问题。不要小视这个问题，如果你开发的程序只有你自己使用，当然无所谓，卸载再安装就可以了。但要是你的软件有很多使用客户，这就是大问题了，就相当于软件不具备升级功能！]]></content>
      <categories>
        <category>App逆向</category>
      </categories>
      <tags>
        <tag>app逆向</tag>
        <tag>签名</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[App逆向之smali学习]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%AE%89%E5%8D%93%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A02-smali%E8%AF%AD%E6%B3%95%2F</url>
    <content type="text"><![CDATA[smali文件什么是smali文件？Smali，Baksmali分别是指安卓系统里的Java虚拟机（Dalvik）所使用的一种。dex格式文件的汇编器，反汇编器。其语法是一种宽松式的Jasmin/dedexer语法，而且它实现了.dex格式所有功能（注解，调试信息，线路信息等） 当我们对APK文件进行反编译后，便会生成此类的文件，小编在此对smali文件进行简要的介绍。其中在Davlik字节码中，寄存器都是32位的，能够支持任何类型，64位类型（Long/Double）用2个寄存器表示；Dalvik字节码有两种类型：原始类型；引用类型（包括对象和数组） 原始类型1234567891011B---byteC---charD---doubleF---floatI---intJ---longS---shortV---voidZ---boolean[XXX---arrayLxxx/yyy---object 这里解析下最后两项，数组的表示方式是: 在基本类型前加上前中括号“[”，例如int数组和float数组分别表示为：[I、[F；对象的表示则以L作为开头，格式是LpackageName/objectName;（注意必须有个分号跟在最后），例如String对象在smali中为：Ljava/lang/String;，其中java/lang对应java.lang包，String就是定义在该包中的一个对象。 或许有人问，既然类是用LpackageName/objectName;来表示，那类里面的内部类又如何在smali中引用呢？答案是：LpackageName/objectName$subObjectName;。也就是在内部类前加“$”符号，关于“$”符号更多的规则将在后面谈到。 方法的定义 方法的定义一般为： 1Func-Name (Para-Type1Para-Type2Para-Type3...)Return-Type 注意参数与参数之间没有任何分隔符12345671. hello ()V --&gt; void hello()2. hello (III)Z --&gt; boolean hello(int, int, int)3. hello (Z[I[ILjava/lang/String;J)Ljava/lang/String;--&gt;String hello (boolean, int[], int[], String, long) Smali基本语法1234567891011121314.field private isFlag:z 定义变量.method 方法.parameter 方法参数.prologue 方法开始.line 123 此方法位于第123行invoke-super 调用父函数const/high16 v0, 0x7fo3 把0x7fo3赋值给v0invoke-direct 调用函数return-void 函数返回void.end method 函数结束new-instance 创建实例iput-object 对象赋值iget-object 调用对象invoke-static 调用静态函数 smali条件分支1234567891011121314条件跳转分支：&quot;if-eq vA, vB, :cond_**&quot; 如果vA等于vB则跳转到:cond_**&quot;if-ne vA, vB, :cond_**&quot; 如果vA不等于vB则跳转到:cond_**&quot;if-lt vA, vB, :cond_**&quot; 如果vA小于vB则跳转到:cond_**&quot;if-ge vA, vB, :cond_**&quot; 如果vA大于等于vB则跳转到:cond_**&quot;if-gt vA, vB, :cond_**&quot; 如果vA大于vB则跳转到:cond_**&quot;if-le vA, vB, :cond_**&quot; 如果vA小于等于vB则跳转到:cond_**&quot;if-eqz vA, :cond_**&quot; 如果vA等于0则跳转到:cond_**&quot;if-nez vA, :cond_**&quot; 如果vA不等于0则跳转到:cond_**&quot;if-ltz vA, :cond_**&quot; 如果vA小于0则跳转到:cond_**&quot;if-gez vA, :cond_**&quot; 如果vA大于等于0则跳转到:cond_**&quot;if-gtz vA, :cond_**&quot; 如果vA大于0则跳转到:cond_**&quot;if-lez vA, :cond_**&quot; 如果vA小于等于0则跳转到:cond_** 深入smaliSmali中的包信息12345678.class public Lcom/aaaaa; .super Lcom/bbbbb;.source &quot;ccccc.java&quot;它是com.aaaaa这个package下的一个类（第1行）继承自com.bbbbb这个类（第2行）这是一个由ccccc.java编译得到的smali文件（第3行） Smali中的声明 一般来说在Smali文件中是这个样子的： 12345# annotations.annotation systemLdalvik/annotation/MemberClasses; value = &#123; Lcom/aaa$qqq;, Lcom/aaa$www; &#125; .end annotation 这个声明是内部类的声明：aaa这个类它有两个成员内部类——qqq和www，内部类将在后面小节中会有提及。 Smali的成员变量 成员变量格式是:.field public/private [static] [final] varName:&lt;类型&gt; 对于不同的成员变量也有不同的指令:一般来说，获取的指令有：iget、sget、iget-boolean、sget-boolean、iget-object、sget-object等。 操作的指令有：iput、sput、iput-boolean、sput-boolean、iput-object、sput-object等。没有“-object”后缀的表示操作的成员变量对象是基本数据类型，带“-object”表示操作的成员变量是对象类型，特别地，boolean类型则使用带“-boolean”的指令操作。 实例: 1sget-object v0, Lcom/aaa;-&gt;ID:Ljava/lang/String; sget-object就是用来获取变量值并保存到紧接着的参数的寄存器中，本例中，它获取ID这个String类型的成员变量并放到v0这个寄存器中。 注意：前面需要该变量所属的类的类型，后面需要加一个冒号和该成员变量的类型，中间是“-&gt;”表示所属关系。 1iget-object v0, p0, Lcom/aaa;-&gt;view:Lcom/aaa/view; 可以看到iget-object指令比sget-object多了一个参数，就是该变量所在类的实例，在这里就是p0即“this”。 获取array的话我们用aget和aget-object，指令使用和上述一致 put指令的使用和get指令是统一的如下：123const/4 v3, 0x0sput-object v3, Lcom/aaa;-&gt;timer:Lcom/aaa/timer; 相当于：this.timer= null; 123.local v0, args:Landroid/os/Message; const/4 v1, 0x12 iput v1, v0, Landroid/os/Message;-&gt;what:I 相当于：args.what = 18;（args是Message的实例） 寄存器知识寄存器是什么意思呢？ 在smali里的所有操作都必须经过寄存器来进行：本地寄存器用v开头数字结尾的符号来表示，如v0、v1、v2、…参数寄存器则使用p开头数字结尾的符号来表示，如p0、p1、p2、…特别注意的是，p0不一定是函数中的第一个参数，在非static函数中，p0代指“this”，p1表示函数的第一个参数，p2代表函数中的第二个参数…而在static函数中p0才对应第一个参数（因为Java的static方法中没有this方法) 12const/4 v0, 0x1iput-boolean v0, p0, Lcom/aaa;-&gt;IsRegistered:Z 我们来分析一下上面的两句smali代码，首先它使用了v0本地寄存器，并把值0x1存到v0中，然后第二句用iput-boolean这个指令把v0中的值存放到com.aaa.IsRegistered这个成员变量中。即相当于：this.IsRegistered= true;（上面说过，在非static函数中p0代表的是“this”，在这里就是com.aaa实例）。 ####]]></content>
      <categories>
        <category>App逆向</category>
      </categories>
      <tags>
        <tag>app逆向</tag>
        <tag>smali学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[App逆向实战和AndroidKiller简单使用]]></title>
    <url>%2F2019%2F08%2F26%2F%E5%AE%89%E5%8D%93%E9%80%86%E5%90%91%E5%AD%A6%E4%B9%A05-AndroidKiller%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%E5%92%8C%E5%AE%9E%E6%88%98%2F</url>
    <content type="text"><![CDATA[先聊聊为什么学习安卓逆向大数据的普及，好像没有一手数据收集能力就要落伍了似得，不少无关开发的岗位都加上需要初级数据采集的能力要求，越来越多的 title 加上 数据 的字眼提升逼格。 渐渐初级爬虫工程师的饭越来越不好吃，中高级的爬虫的招聘需求也添加上 JS 逆向+ APP 逆向的要求，所以有一手破签逆向的技术也越来越吃香。 APP逆向其实已经属于安全方面的内容，而我们爬虫工程师要做的其实远没有逆向工程师那么多和精，只要拿到请求中加密参数的加密方法，再用 Python 模拟加密即可。 为了面包，学习吧。 学习资源推荐关于逆向的前置知识，可以参考之前发过的几篇基础文章，同时咸鱼也推荐下面这些学习资源，希望能帮助你快速入门（伸手党福利？？？）： 吾爱破解入门教程集合贴：http://t.cn/Ai8hXHzs 推荐关注的博客： 尼古拉斯·赵四：https://blog.csdn.net/jiangwei0910410003 鬼哥：https://blog.csdn.net/guiguzi1110?viewmode=contents 天天记小本子上的lilac：https://blog.csdn.net/qq_38851536 推荐逆向书籍： 《Android软件安全与逆向分析》 《Android应用安全防护和逆向分析》 《JAVA加密与解密的艺术》第二版 推荐关注的公众号： 小周码字 编程美丽 妄为写代码 其实还有好多好多的资源，请善用搜索。 以上仅为我的个人推荐，大佬请无视~ 接下来是咸鱼动手跟着做的第一个项目，某个小游戏的付费破解，了解AndriodKiller逆向工具的使用。 AndriodKiller的简单使用和实战在使用这个逆向工具，需要先检查本地的 JAVA 开发环境是否安装。 命令行下输入： 1java -version -- 输出JAVA jdk 版本 如果出现无版本信息输出或其他报错请参照相关博客或后台回复 【逆向环境搭建】获取相关搭建视频。 接下来是 AndriodKiller 的安装了，在吾爱的网站上提供了相关的 绿色免安装版本，找到软件可以直接点击使用。 【良心】 打开界面后是下面这样的： 打开后第一件事是在 AndriodKiller 的配置中配置 jdk 的安装路径之后既可以通过拖入需要反编译的受害者，即可打开进行分析： 我们这次要完成一款小游戏的付费破解，我们先安装未破解的APP看看能不能通过未破解的APP找到思路。 通过一番把玩，发现进入支付流程后取消支付会弹出相应提示语，顺着这个思路我们在反编译后的文件中检索一下看能不能找到对应的代码： 我们点击进去看看，发现了我们搜索的字符串： 反编译的代码不太方便查看，我们可以通过查看还原为java代码理解相关的逻辑： 可以看到，还原后的代码中支付失败，支付取消在同一个方法中，支付成功的逻辑为单独一个方法。 所以我们可以试试通过直接把支付成功的代码逻辑直接替换到支付失败的那个方法中来破解支付 替换过后查看字符串，发现整个类文件中就只剩下支付成功的方法了。 保存文件之后我们编译一下试试是否成功吧。 编译后，控制台输入如下代表编译成功。 打开 APP - 商店 随便购买一个道具，但是不完成支付，点击取消支付，可以看到左上角的道具数量增加了。说明我们的操作是成功的。（为避免破解不成功或误操作导致的扣费，请使用未插卡的机器测试） OK，到这里我们就完成了第一个安卓APP的逆向破解，当然这个是非常简单的逆向项目。 Tip AndriodKiller 并不是万能的，有些项目是没办法反编译成功的，所以需要搭配其他的逆向工具使用 AndriodKiller 在第一次导入项目时，会一直卡在反编译成功，xxxx但是没有打开项目，需要收到重启AndriodKiller ，在工程中打开刚刚的项目。 AndriodKiller 转化为 java 代码时在 java 代码中是无法编辑的，需要返回到 smali 中修改，所以 smali语法相当关键，需要好好研究不要对 软件自带的 转换功能过多依赖，且在 smali 中修改后要记得 ctrl + s保存，之后再编译。 在编译过程中出现报错，报错日志大致如下： 12345678910111213141516171819202122232425262728293031当前 Apktool 使用版本：apktool_2.3.0正在编译 APK，请稍等...&gt;I: Using Apktool 2.3.0&gt;I: Smaling smali folder into classes.dex...&gt;I: Building resources...&gt;S: WARNING: Could not write to (C:\Users\i\AppData\Local\apktool\framework), using C:\Users\i\AppData\Local\Temp\ instead...&gt;S: Please be aware this is a volatile directory and frameworks could go missing, please utilize --frame-path if the default storage directory is unavailable&gt;W: E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res\values-v23\styles.xml:8: error: Error retrieving parent for item: No resource found that matches the given name '@android:style/WindowTitle'.&gt;W: &gt;W: E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res\values-v23\styles.xml:9: error: Error retrieving parent for item: No resource found that matches the given name '@android:style/WindowTitleBackground'.&gt;W: &gt;W: E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res\values-v24\styles.xml:7: error: Error retrieving parent for item: No resource found that matches the given name '@android:style/Animation.DropDownUp'.&gt;W: &gt;W: E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res\values-v24\styles.xml:8: error: Error retrieving parent for item: No resource found that matches the given name '@android:style/Animation.DropDownDown'.&gt;W: &gt;Exception in thread "main" brut.androlib.AndrolibException: brut.androlib.AndrolibException: brut.common.BrutException: could not exec (exit code = 1): [C:\Users\i\AppData\Local\Temp\brut_util_Jar_6839346043621878032.tmp, p, --min-sdk-version, 16, --target-sdk-version, 26, --version-code, 1, --version-name, 1.0, --no-version-vectors, -F, C:\Users\i\AppData\Local\Temp\APKTOOL8354570300607898895.tmp, -0, resources.arsc, -0, arsc, -I, C:\Users\i\AppData\Local\Temp\1.apk, -S, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res, -M, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\AndroidManifest.xml]&gt; at brut.androlib.Androlib.buildResourcesFull(Androlib.java:485)&gt; at brut.androlib.Androlib.buildResources(Androlib.java:419)&gt; at brut.androlib.Androlib.build(Androlib.java:318)&gt; at brut.androlib.Androlib.build(Androlib.java:270)&gt; at brut.apktool.Main.cmdBuild(Main.java:224)&gt; at brut.apktool.Main.main(Main.java:75)&gt;Caused by: brut.androlib.AndrolibException: brut.common.BrutException: could not exec (exit code = 1): [C:\Users\i\AppData\Local\Temp\brut_util_Jar_6839346043621878032.tmp, p, --min-sdk-version, 16, --target-sdk-version, 26, --version-code, 1, --version-name, 1.0, --no-version-vectors, -F, C:\Users\i\AppData\Local\Temp\APKTOOL8354570300607898895.tmp, -0, resources.arsc, -0, arsc, -I, C:\Users\i\AppData\Local\Temp\1.apk, -S, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res, -M, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\AndroidManifest.xml]&gt; at brut.androlib.res.AndrolibResources.aaptPackage(AndrolibResources.java:454)&gt; at brut.androlib.Androlib.buildResourcesFull(Androlib.java:471)&gt; ... 5 more&gt;Caused by: brut.common.BrutException: could not exec (exit code = 1): [C:\Users\i\AppData\Local\Temp\brut_util_Jar_6839346043621878032.tmp, p, --min-sdk-version, 16, --target-sdk-version, 26, --version-code, 1, --version-name, 1.0, --no-version-vectors, -F, C:\Users\i\AppData\Local\Temp\APKTOOL8354570300607898895.tmp, -0, resources.arsc, -0, arsc, -I, C:\Users\i\AppData\Local\Temp\1.apk, -S, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\res, -M, E:\AndroidKiller_v1.3.1\projects\idm下载\Project\AndroidManifest.xml]&gt; at brut.util.OS.exec(OS.java:95)&gt; at brut.androlib.res.AndrolibResources.aaptPackage(AndrolibResources.java:448)&gt; ... 6 moreAPK 编译失败，无法继续下一步签名! 我的解决方案是：替换 AndriodKiller 目录下 bin\apktool\apktool 中的 ShakaApktool.jar 为更新的版本即可成功编译，签名。]]></content>
      <categories>
        <category>App逆向</category>
      </categories>
      <tags>
        <tag>app逆向实战</tag>
        <tag>逆向资源</tag>
        <tag>AndroidKiller</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-如何从互联网中高效摄取信息？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%95%8F%E5%B7%A5%E4%BD%9C-%E5%A6%82%E4%BD%95%E4%BB%8E%E4%BA%92%E8%81%94%E7%BD%91%E4%B8%AD%E6%9C%89%E9%AB%98%E6%95%88%E6%91%84%E5%8F%96%E4%BF%A1%E6%81%AF%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[如何从互联网中有高效摄取信息？信息源检查清单 信息源的目的（想学什么？） 信息源的质量 信息源的半衰期 = 信息的有效时间 信息源的稀缺程度 = 价值高低 信息的半衰期越久，稀缺程度越高，价值越大 对于信息源掌握主动权 （区分 push 和 pull ) = 人人喜闻乐见的，终会趋于庸俗。 警惕回声效应，应该从多角度获取信息 处理信息的能力 速读 通过关键词快速判读信息的价值 溯源 知道自己读过什么 = 稍后读 保留信息来源，随时进行回顾 吸纳 记录下来后应该对信息进行自我归纳，才能吸收成为自己的东西]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>信息摄取</tag>
        <tag>互联网</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-如何做好任务管理？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E4%BB%BB%E5%8A%A1%E7%AE%A1%E7%90%86%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[任务管理就是人生规划 长期任务（人生规划） 健康 教育 理财 中期任务（职业规划） 实现自我价值 短期任务（周计划，日计划） 双任务系统 今天必须做完的事（短期或中期任务） 对自己很重要的事情（长期任务） 理解任务管理的体系（LTF体系） list tag filter 高级任务管理注重的点在与 filter 的使用 OmniFocus &amp; Todoist 团队协作：Teambition 大量数据的任务管理：在线表格工具]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>任务管理</tag>
        <tag>任务管理体系</tag>
        <tag>LTF体系</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-如何做好文件管理？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[文件管理文件的「生命周期」 = 新建 + 修改 + 移动 +删除 桌面文件整理 给新建文件好好取个名字 时间 + 类型 + 文件内容 + 版本号 做好桌面文件的关联和归档 合理安排桌面的文件夹 下载目录整理 推荐工具：Everything，Droplt，Total Commander]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>文件管理</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-如何做好笔记整理和思维导图？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E5%A6%82%E4%BD%95%E5%81%9A%E5%A5%BD%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[笔记整理了解笔记整理的基本原则 记重点 过于详细的记录反而拉低效率，只记关键字 先收集 把笔记变成收集箱，关键字要记成 可回溯 记下来的笔记以后要能找到 不同类型笔记工具的优缺 电子笔记整理更方便，可编辑，已分享 纸笔更自然 选择适合自己的笔记APP记完笔记推荐使用费曼学习法 思维导图为什么使用思维导图信息接受速度太快，导致「消化不良」 思维导图的逻辑简单理解 节约时间和精力 应用范围很广泛 头脑风暴 书摘 决策 商务提案 目标设定 思维导图的通用技巧最重要就是敢记，思维导图就是为了帮助我们整理思路，所以不要怕记错记杂 内容：多记，速记 形式：注意结构，清晰简明 只记关键词，避免整句记录 自创常用字标签，提高记录速度 如何选择思维导图 iThoughtsX Xmind MindNode（多文件导入导出）]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>笔记整理</tag>
        <tag>思维导图</tag>
        <tag>通用技巧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-如何做好信息收集？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E5%A6%82%E4%BD%95%E9%AB%98%E6%95%88%E6%95%B0%E6%8D%AE%E6%94%B6%E9%9B%86%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[高效数据收集 数据收集 = 表单数据 问卷星 金数据 麦客 腾讯问卷 Airtable 数据处理 Excel 比较适合这个环节 数据应用]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>数据收集</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-知识记忆与知识内化]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E7%9F%A5%E8%AF%86%E8%AE%B0%E5%BF%86%E4%B8%8E%E7%9F%A5%E8%AF%86%E5%86%85%E5%8C%96%2F</url>
    <content type="text"><![CDATA[知识记忆与知识内化人们对于记忆的三个误区 最好的记忆方法就是不断重复 存进电脑，就不用记在脑子里了 信息过载的时代，「多」并不是优势 买书如山倒，读书如抽丝 互联网的信息 != 硬盘里的文件 != 你掌握的知识 从海量信息中获得有价值的部分，并转化为自己内在的知识储备，才是要紧事 聪明人靠创意不靠记忆 不少所谓创意型的人才，只是所谓的搬运工 第一步，把信息记下来 知识内化的三个理论和九个方法理论1：必要难度理论我们应该让自己学习记忆的过程始终保持必要的难度，这种难度虽然在最初会减慢你的学习速度，但是长期来看对学习效果更加有利。 记忆的两种维度 存储强度 提取强度 记忆过程越容易，忘记的越快，反之则越难忘记 四个提升记忆内化效果的方法方法1：建立复述意识 你缺的是复述的习惯和意识 学习完你需要告诉自己，你看完这个你总得记住点什么 康奈尔笔记法 方法2：间隔学习法 间隔穿插着学习 方法3：费曼学习法 选择一个概念 把它讲给小孩子听（非专业人士）听 发现理解不到位的地方，返回原始材料继续学习 回顾和简化信息 方法4：为了记忆的输出 输出，是为了更好地强化自己的记忆 理论2：遗忘曲线理论 在日常学习工作的过程中学习和记忆的东西都是不断追加的，所以使用遗忘曲线学习记忆成本太高。 降低遗忘曲线使用成本的工具 Anki = ankisrs.net 理论3：记忆宫殿方法5：提升既视感 既视感，将抽象的知识、信息图像化 方法6：情绪记忆（调动感官，记忆过程中接受刺激的感官越多，在回忆的时候越清晰） 长期记忆系统 外显性记忆 陈述性记忆 语义记忆 情景记忆 内隐性记忆 程序性记忆 情绪记忆 方法7：用联想解除记忆封印 将数字的音，型，意，进行预编码 将图片两两组合为故事 一边回忆故事，一边逆向匹配数字 方法8：在记忆宫殿里讲故事]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>知识记忆</tag>
        <tag>知识内化</tag>
        <tag>费曼学习法</tag>
        <tag>必要难度理论</tag>
        <tag>间隔学习法</tag>
        <tag>遗忘曲线</tag>
        <tag>记忆宫殿</tag>
        <tag>情绪记忆</tag>
        <tag>知识储备</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-遭遇裁员怎么办？]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E8%A2%AB%E7%A6%BB%E8%81%8C%E4%B9%8B%E5%90%8E%E5%BA%94%E8%AF%A5%E5%81%9A%E4%BB%80%E4%B9%88_%2F</url>
    <content type="text"><![CDATA[现在市场上被离职的方式形形色色,并不都是法律条款规定里的模式,所以有必要了解基本的法律规定,这里咸鱼分享一下在极客时间「白话法律42讲」里学习到的一些知识要点,希望在维权路上帮到你。 先引用周老师在专栏里的两个例子: 口头裁员有效吗？我国《劳动合同法》明确规定，劳动合同的签订、变更、解除都需要用书面的形式，公司应用书面通知 小蔡，而不能口头裁员。结合小蔡的描述，公司只是口头提出，但实际上双方没有解除劳动关系，劳动 合同也继续生效。 劳动关系存续期间，能否找新的工作，签新的劳动合同？虽然劳动关系继续，但是既然公司有裁员的意向，作为员工，提前找好新工作是无可厚非的，法律也不 禁止。但在上一份劳动合同解除之前，尽量不要签新的劳动合同，否则会让自已陷入被动。 比如说，原公司可能倒打一耙，以你“在劳动关系存续期间，签署新的劳动合同，严重影响工作”为由， 提出解除劳动合同，你反而成了过错方，得不到补偿。再或者，公司变卦，不放你走了，对下家公司你 就成了违约方。 如果公司不提出解除劳动合同，则需要继续工作； 如果公司依然是口头解除，不给书面文件，则可以取证（证据包括比较多，比如说工作证、工资 条、劳动合同、辞退相关证据、工资发放情况、录音录像、证人证言、社保等），依法提出解除劳 动合同，并要求经济补偿。 接下来再详细聊聊技术人应该知道的具体的事项。 什么是劳动合同解除?简单来说就是劳动合同规定的时间还没到,双方任意一方提出解除合同。所以这里就要分情况分析了: 协商解除 员工单方面解除 公司单方面解除 前两种情况这里就不细讲,一般不会有太多的坑,甚至在公司过错的前提下，员工还享有法定的解释权。(通常指公司未按时发放工资,未按时缴纳社保等) 这里需要注意的是第三点: 公司单方面解除劳动合同,在法律上只有三种情况下公司才可以单方面解除合同: 员工存在过错(违反公司制度、违法犯罪、简历弄虚作假等) 员工没有过错,但是在因为一些因素不能正常工作(患病、受伤(非工伤)、无法胜任工作、外部条件发生变化) 特殊情况下的经济性裁员 而这里的第三点才是我们关注的重点。 经济性裁员经济性裁员这里是指员工没有过错,公司单方面解除劳动合同。那么特殊情况下的经济性裁员又是什么东西呢？ 公司破产 公司经营困难 企业转产，技术革新 劳动合同订立时客观情况发生重大变化，导致合同不能继续履行的 裁员补偿了解完上面比较枯燥的概念问题,就到了关键的地方了,被离职什么情况下有补偿?被离职的补偿怎么算? 什么情况下有补偿? 由公司提出和员工协商一致解除劳动合同 由员工提出,理由是公司存在过错(通常指公司未按时发放工资,未按时缴纳社保等) 公司提前三十天解除合同,员工无过错 被离职的补偿怎么算? 每满一年支付一个月的工资 六个月以上没满一年，按一年计算,补偿一个月工资 不满半年，支付半个月工资 经济赔偿金怎么算?公司违反法律规定(公司是不能毫无理由的解除与劳动者的劳动合同的，否则就会要求给经济赔偿金)解除劳动合同。如: 员工在孕期，哺乳期,产期,在规定医疗期等特殊情况 公司没有合法理由,页面有按照法定的流程解除合同 公司要向员工支付经济赔偿金,经济赔偿金是上面计算出来金额乘二 其余情况的赔偿参考: https://www.66law.cn/laws/146137.aspx n + 1 是什么?经常在脉脉上看到有 n + 1之类的说法,这里的n就是上面的经济补偿金,1这里指的是公司没有提前三十天通知你就需要支付额外的1个月工资。 n一般按照前12个月的平均工资算,1是按照上个月工资算,且n最高为12,1最高不高于公司所在地年度职工平均工资的三倍,若高于则按三倍计算。 总结一下其实还有很多关于技术人应该懂得法律知识,建议大家不妨订阅学习一下,可以避开不少雷区。 之前咸鱼也写过一篇关于爬虫是否违规的一篇普法文,感兴趣的朋友可以看看: 看到这希望对于被离职,能有自己的一套应对策略,不会成为无头苍蝇,再之,没有遇到被离职的技术人也更应该注意提升自己。]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>裁员</tag>
        <tag>维权</tag>
        <tag>离职</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[聪明工作-邮件礼仪]]></title>
    <url>%2F2019%2F08%2F26%2F%E8%81%AA%E6%98%8E%E5%B7%A5%E4%BD%9C-%E9%82%AE%E4%BB%B6%E7%A4%BC%E4%BB%AA%2F</url>
    <content type="text"><![CDATA[邮件礼仪 邮箱的名称 邮箱的注册名，如「wuyanzu@163.com」 不太正式的邮箱注册名 纯数字，qq邮箱 包含其他数字，如生日：wyz19931023@163.com 较为稳妥的邮箱注册名 姓名拼音 拼音（姓）+英文名，Jack.Wu@163.com 对外展示的名字 邮箱收件时上方发件人的名字 拒绝使用QQ有一大原因是因为之前QQ邮箱之前默认使用QQ昵称当做发件人名称，有些过于非主流了 推荐使用 中文姓名 英文名+姓 如果经常频繁沟通，建议使用，公司名+职位+姓名 收件人、CC和BCC的区别 收件人：直接相关的人，大多数时候，你希望他们做出回应（听众） CC（抄送）：让对方了解下，不需要对方回应（旁听） BCC（密送）：被填入BCC一栏的收件人，将对其他收件人隐藏。（类似网易邮箱的群发单显） 谁要抄送，谁不需要抄送，请先搞清楚公司架构及事务的相关人员（问你的直接上级） 如何写好邮件？ 邮件正文 忌长篇大论，提倡ABC法则 A - Action（行动）：一句话总结希望收件人做的事情 B - Background（背景）：解释事件背景和细节 C - Close（结束语）：一两句结束邮件的礼貌用语 可以放上一些相关联系人，比如有哪件事情需要了解详情的时候应该联系谁+联系方式 间接等于 5W 法则 A = who，when，what，where B = why 邮件标题 内容：精简 Action 的内容 格式：在标题前面加上邮件类型 - 通知，请求 写邮件的顺序 附件：很容易在最后被忘记 正文：方便在正文中提及附件 标题：方便总结正文内容来起标题 收件人：写好内容，更容易判断收件人，CC等关系 关于附件的好习惯 千万别发一份邮件，正文只有几个字 建议在正文简要说明附件中的关键信息以及注意文件的格式 养成好的附件命名习惯 文件修改的日期 版本号 序列号等 自动回复如何设置 自动回复的错误用法 你的来信我已经收到。。。 自动回复的正确用法 休假或外出的时候才会用 阐述你正在休假 如果很急的话，可以联系谁（姓名 + 联系方式） 如果不是很急的话，你大概什么时间可以回复 邮件管理 遵循FAST法则管理 Filter 过滤 把符合规则的邮件自动归类到某个分类的文件夹下（如过滤器，过滤规则，来信分类等功能） Archive 归档 将已经阅读或已经处理的邮件转移到「归档」的文件夹下 Snooze 延后 不能马上处理或马上回复的邮件 适用于简单，稍后才需要完成的任务 Transfer 流转 大型任务清单，任务比较复杂，可能需要进一步细化和重新分配 这里的流转存在两种意思 一是在你的任务处理层面上的流转，比如流转细分到你的 todolist 或其他任务管理软件上 二是这个任务需要多人合作，比如会议结束后把邮件中汇总的任务细化分配给各个负责人 如何进一步整理邮件？ 文件夹 标签（标签的三个维度）- 以过滤器自动打标签 来源标签：以发件人所在的公司部门，或是外部合作方、客户的名字命名 类型标签：以邮件属性命名，如任务、通知、用户反馈、新闻订阅等 项目标签：以邮件所属项目名字命名 高级的邮件检索功能 *：匹配 AND：包含 -：排除 has:document - 找出带word文档的邮件 has:spreadsheet - 找出带有exceld的邮件 has:presentation - 找出带有ppt的邮件 参考 https://support.google.com/mail/answer/7190]]></content>
      <categories>
        <category>聪明工作</category>
      </categories>
      <tags>
        <tag>邮件礼仪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-基础-RSA加密与xxtea编码实例]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A002-rsa%E4%B8%8Exxtea%2F</url>
    <content type="text"><![CDATA[这次要研究的网站采用的是rsa加密以及xxtea。 这里先复习下上一篇讲过的调试流程基础版: 如果网页有跳转，必须勾选 preservelog 防止丢包 看一下有没有框架 右键查看框架源代码(弹出式登陆界面) 登陆尽量使用错误密码 防止跳转 查看关键登陆包 分析哪些参数是加密的 使用别的浏览器分析哪些参数是固定的值 初步猜测加密方法 搜索 直接搜索参数 pwd= pwd = pwd: pwd : 密码框地方右键 检查 查看 id name type 找到加密的地方(重点) 调试 找出所有的加密代码 从最后一步开始写起，缺啥找啥 如果找的是函数的话 search 要带上 function xxx 如果看到加密的地方有个类，并且之后是用 prototype 把方法加在原生对象上的话，要把所有加在原生对象上的方法都找出来 函数找多了没关系，只要不报错不会影响结果，但是不能找少了 以上就是上篇文章调试的核心流程,接下来我们继续用实战案例练习一下。 RSA先看下要解析的加密参数,记住流程的前几点: 接下来搜索参数,分析可能存在加密代码的js文件: 尝试了上面流程里面列举的可能的参数名,并没有发现有用的信息,所以试了下直接搜索password关键字,找到一个可能和登陆相关的文件,搜索发现疑似的加密代码: 先测试打个断点试试: 成功进入断点接下来,就是缺啥找啥补齐就可以了,强烈建议大家下载一个webstorm,方便调试。 我们先把代码里面的值填写一下,运行看看报错是什么: 根据报错我们找找这里的RSAKey在哪里,不知道如何查找可以将鼠标悬停在对应位置即可根据提示找到对应的代码。 反复操作即可,缺啥找啥直至js文件无报错即可。 小结咸鱼找了一些简单网站,查看了对应的RSA加密的方法,总结了以下套路: 一般的rsa加密通常会先声明一个rsa对象 本地使用公钥加密即public key 通常有Encrypt关键字 加密后字符长度为128位或256位结合以上套路可以帮助我们快速判断加密方式如何,便于我们理清解密思路。 XXTEA “微型加密算法（TEA）及其相关变种（XTEA，Block TEA，XXTEA）都是分组加密算法，它们很容易被描述，实现也很简单（典型的几行代码）。 XXTEA是其最新的变种，于1998年提出。目前还没有人找到对其进行攻击的方法，是对前面一些变种的改进。XXTEA 算法很安全，而且非常快速，非常适合应用于 Web 开发中。 以上引用自https://my.oschina.net/mickelfeng/blog/109388 介绍完 XXTEA 我们先看看加密案例网站是什么样子的: 可以看到传输的参数都是加密后的密文,像这类加密我们可以参考提交表单中的id,class等,查看密码框的id后,全局搜索pass: 找到疑似代码的位置,不确定可以打上断点测试一下,通过断点发确认加密位置: 接下来就是缺啥补啥的过程,经过解密发现这个网站加密过程是一个xxtea.base64的加密。 对比上一篇文章的base64的加密其实区别不大,但是在解密过程中需要思路的转变。 以上就是咸鱼对js解密的案例练习,希望对你有所帮助。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>RSA</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-QQ空间密码加密]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A003-QQ%E7%A9%BA%E9%97%B4%E8%A7%A3%E5%AF%86%E6%80%9D%E8%B7%AF%2F</url>
    <content type="text"><![CDATA[QQ空间的解密思路本篇文章只是提供解密思路,不提供代码。 找了一圈发现越是大厂加密越是简单,安全防护之类的完全不靠js加密,扫码登陆才是关键。比如微信公众号平台的登陆加密就是简单的MD5,但人家有扫码登陆,此类情况比比皆是,所以爬虫越来越难做了。 老规矩先找加密位置,相比于上一次的XXTEA加密,这次的加密参数是p,这个关键字太过常见,我们不以这个作为检索条件,否则干扰项太多。先检索其他加密的参数看看能不能发现什么。 按照上图断点的位置,发现了类似加密的位置,像此类带关键字的很容易就可以发现目标代码,我们先打个断点测试一下。 成功进入断点,说明判断没错。接下来我们这要把关键加密的代码抠出来补全即可。 理清参数扣代码不仅仅是复制粘贴,需要先理清楚加密思路,像这里的$.Encryption.getEncryption(n, pt.plogin.salt, i.verifycode, pt.plogin.armSafeEdit.isSafe)这种一个加密函数的还是比较清晰,我们先要把参数补齐,再找关键的加密函数。 经过检索可以发现,这里的n就是我们传入的密码。 pt.plogin.salt是传入的用户名加密后的结果。 i.verifycode获取的是验证码的值。 pt.plogin.armSafeEdit.isSafe找了半天,打上断点发现是个空值。 现在所有的参数已经找齐了,我们直接进入关键加密的函数就可以了,打上断点把鼠标放在关键函数的位置在出现的蓝色链接上点击即可跳转。 把上面找的代码和关键加密的方法组装就可以得到加密后的参数了。 总结以上加上前面的两篇就是js基础解密的部分了,基础的方法用三篇概述只能说个大概,之后会继续写基础部分的文章就是没有这三篇这么详细,没有动手自己走一遍流程永远只能停留在基础。 总的来说这三篇文章核心就是是下面这个流程。 基本流程的复述 如果网页有跳转，必须勾选 preservelog 防止丢包 看一下有没有框架 右键查看框架源代码(弹出式登陆界面) 登陆尽量使用错误密码 防止跳转 查看关键登陆包 分析哪些参数是加密的 使用别的浏览器分析哪些参数是固定的值 初步猜测加密方法 搜索 直接搜索参数 pwd= pwd = pwd: pwd : 密码框地方右键 检查 查看 id name type 找到加密的地方(重点) 调试 找出所有的加密代码 从最后一步开始写起，缺啥找啥 如果找的是函数的话 search 要带上 function xxx 如果看到加密的地方有个类，并且之后是用 prototype 把方法加在原生对象上的话，要把所有加在原生对象上的方法都找出来 函数找多了没关系，只要不报错不会影响结果，但是不能找少了 基础的解密其实很简单了,只要自己动手去练习基本不会有太大问题。 流程捋清楚之后就是关于JS的学习,可以去B站上面看看。 https://www.bilibili.com/video/av34087791 最后就是心态问题,耐心在js调试上很关键,一直报错心态很容易崩,一烦躁反而调不出来了。 以上。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>RSA</tag>
        <tag>基础</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-漫画柜加密分析]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A005-%E6%BC%AB%E7%94%BB%E6%9F%9C%2F</url>
    <content type="text"><![CDATA[这次文章是补之前文章提到的eval加密 – 漫画柜 之前文章简单介绍了常见的JS混淆 查看请求打开控制台,随意点击一本漫画,进入到正文页面,查看请求: 可以很容易看到这里有个md5的参数值是加密的。 定位加密位置这里有两种定位方法:第一种面向老手,先翻一遍请求,网页源码可以迅速定位。 第二种就是按照之前的文章提到的分析流程,我们分析一遍,虽然麻烦点会走弯路但是适合新手: 没有看过的朋友可以点击下面的文章链接回顾一下: 先搜索一下关键字,这里加密的参数是md5,所以试试下面这几种搜索关键词:12345md5:md5 :md5=md5 =md5 搜索结果如下: 通过关键字搜索没有得到想要的结果,按照之前文章提到的流程到这里就卡住了。 不过我们看到上面请求里还有另一参数cid,既然都是请求参数,那么md5这个参数可能是和他一起提交的,我们可以试试搜索cid关键词试试。 搜索cid这个参数结果如下: 果然有点东西,我们点进第一个文件,搜索下有多少和cid这个参数相关搜索项,检索之后有18项相关,通过分析代码,很快我们定位到下图的代码,打上断点重新加载看看是否能进入我们的断点位置: 果然成功进入断点了,但是好像并没有我们想要得md5参数,这个时候好像又卡住了。 不过很快我们通过查看左侧堆栈信息找到了md5参数的位置: 这里vm的代码又是通过下面这段代码生成的: 这开头,不就是熟悉的eval加密吗。 到这里就定位到解密的地方了,但是这段eval代码又是在哪呢？ 发现左侧有.html的标识,搜索是搜不到了,我们就看看这个页面的源代码吧,发现右键是进入下一页漫画,所以通过控制台的Doc选项卡看看,通过格式化代码我们找到了上面的eval代码。 1234567891011121314151617181920212223window["\x65\x76\x61\x6c"](function(p, a, c, k, e, d) &#123; e = function(c) &#123; return (c &lt; a ? "" : e(parseInt(c / a))) + ((c = c % a) &gt; 35 ? String.fromCharCode(c + 29) : c.toString(36)) &#125; ; if (!''.replace(/^/, String)) &#123; while (c--) d[e(c)] = k[c] || e(c); k = [function(e) &#123; return d[e] &#125; ]; e = function() &#123; return '\\w+' &#125; ; c = 1; &#125; ;while (c--) if (k[c]) p = p.replace(new RegExp('\\b' + e(c) + '\\b','g'), k[c]); return p; &#125;('I.H(&#123;"G":4,"J":"M","L":"4.2","K":B,"A":"z","C":["F.2.3","E.2.3","D.2.3","N.2.3","X.2.3","W.2.3","V.2.3","Y.2.3","11.2.3","10.2.3","Z.2.3","Q.2.3","P.2.3","O.2.3","R.2.3","U.2.3","T.2.3","S.2.3","d.2.3","c.2.3","b.2.3","e.2.3","h.2.3","g.2.3","f.2.3","a.2.3","5.2.3","6.2.3","8.2.3","7.2.3","9.2.3","i.2.3","u.2.3","t.2.3","s.2.3","v.2.3","y.2.3","x.2.3","w.2.3","r.2.3","m.2.3","k.2.3","j.2.3","n.2.3","q.2.3","p.2.3","o.2.3","12.2.3","1E.2.3","1D.2.3","1C.2.3","1F.2.3","1I.2.3","1H.2.3","1G.2.3","1x.2.3","1w.2.3","1v.2.3","1y.2.3","1B.2.3","1A.2.3","1z.2.3","1V.2.3","1S.2.3","1T.2.3","1R.2.3","1W.2.3","1U.2.3","1L.2.3","1M.2.3","1J.2.3","1K.2.3","1P.2.3","1Q.2.3","1N.2.3","1O.2.3","1c.2.3","1b.2.3","1a.2.3","1d.2.3","1g.2.3","1f.2.3","1e.2.3","15.2.3","14.2.3","13.2.3","16.2.3"],"19":18,"17":1q,"1p":"/1o/l/1r/1u/","1t":1,"1s":"","1j":1i,"1h":0,"1k":&#123;"1n":"1m"&#125;&#125;).1l();', 62, 121, 'D7BWAcHNgdwUwEbmARgJwHYAcwAMAmANgIz3wwJwLVwGZcytq966UyBWAws3A93OgINBTdGRQF8ZACwEOZevhll8BWi0m1puGWpkbdkvSF18ZAg7pW7iM0rq4ye1xyxpyWc2grr1aNtp+LFy0LrRMtGgsFBEsxLSkgDTegoDuysAAxgB2AIYAtnCohBgyCgBmAJYANnAAznjCdA2SBA18gsAIFQAmwBV5kAAiOQAuOcAAygCyABKduQWZPZ3gFRnAgOr+gJGqgATyDaw2KPQoh2pHeCiSKDongr4oFChUKMQPF1wvDcS4DrhcuC4zH8GhRcM8+JcGjRJA0mLhoroKDIqFguFgXFg5Ki8Fh6JiccQsKQallgCMAE4AV0KlSyFVqAAs4L0fkwMAiMBRsHgMMROTyaEwcWpcTjJFgdHCsOxwOS4AA3ACSvReJR4WTgAA8RsrgLUqsBZXBFXSRsAspSAII5S0AYRm3QAYgANDJYABagwmAHkYOSKuTupbgHlugpwLUZaMGcAicAqgyAF6dKoAewyAGsAPoZda1MYjSn1Dp/CgcKgcYgcBwcLiVvDlv4IwiSQg6b4ths0YgNvgcSw0Lh4ZG6BH9v46DhyWsN+hThtqDiGUEYASrn7tpiEZs0Cg8uQYXyHn4uDBqDDL+glPCELiEFyEeiEGzPgG+YoAqhtgGGe8AjBAA=='['\x73\x70\x6c\x69\x63']('\x7c'), 0, &#123;&#125;)) 解密过程我们来捋一捋整个过程,首先网站加载页面,执行了这段eval,解密了参数里的一堆密文,之后根据参数请求具体内容,那我们逆向只要拿到页面的代码,用execjs执行这段代码不就能拿到md5值直接请求了吗。 但是把这段代码直接复制到eval解密里好像并没有用,我感觉应该和末尾的加密参数有关,经过测试这段参数虽然长得和Base64很像但并不是base64加密,我又卡住了,所以我求助了大佬。 经过 @ 悦来客栈的老板 的提点我尝试了下果然是这段是这段代码有问题: 经过解密运行的结果就是我们在vm中看到的结果了: 到这里就简单了,请求网页的代码,使用正则替换代码里的密文,使用execjs执行这段代码就可以得到md5值,再使用这个md5值就可以请求了。 结束总结这次的解密文章写的比较啰嗦,虽然整个加密比较简单,但是自己在这整个过程也踩了不少坑,走了不少弯路。 JS逆向是细致活,需要大胆假设,小心求证,耐心调试,同时在逆向过程中卡住了需要求助的时候也不要不好意思。把自己思考的结果、遇到的问题描述清楚附上小小的红包和大佬聊聊,会有意想不到的惊喜。 共勉~]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>漫画柜</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-淘大象加密分析]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A006-%E6%B7%98%E5%A4%A7%E8%B1%A1%2F</url>
    <content type="text"><![CDATA[今天继续和大家研究JS逆向，不少小伙伴在JS逆向的时候遇到过无限debugger的反爬，今天就拿一个网站练练手感受下无限debugger。 分析请求先打开这次的目标网站–淘大象(https://taodaxiang.com/credit2) 打开「开发者工具」就自动进入debug，我们先格式化看看能不能通过重写函数解决。 显然不是很好解决，所以我们试试通过「条件断点」的方法绕过这个反爬。 我们在 3393 与 3395 行右键添加「Add Conditional breakpoint」并在出现的对话框中输入「false」，这个时候无限debbuger的反爬就被我们绕过了。 虽然这里很快就绕过了反爬但是「条件断点」有一定的局限性,这里推荐一篇文章以供学习，就不再赘述。 https://segmentfault.com/a/1190000012359015 解决完反爬措施，我们刷新页面查看页面请求,很快就定位到请求接口、加密参数等信息。 请求接口: 加密参数 sign : 定位加密位置通过抓包请求我们定位到了接口和加密参数,我们试着通过搜索sign关键字定位一下加密位置。 通过搜索我们找到三个包含关键字的文件,通过检索文件,很快定位到了sign值加密的位置: 分析加密很明显这个js文件经过一定程度的混淆,现在有两种方法解决: 一是直接扣代码，绕过debugger之后扣这个代码不要太简单。 二是通读加密,直接改写成python加密。 这个加密比较简单,整体是md5加密这点通过分析请求可以猜到,多次请求加密都不变,接下来就是分析是哪些字段加密成md5值。 这里比较明显的是js中的两段代码: _0x2c114b[&#39;XDFEp&#39;]这个方法是将字符串拼接 _0x2c114b[&#39;zANjZ&#39;]这个方法是取字符串的MD5值 经过断点调试，发现最后字符串是由「查询的关键字」+ 「固定的key」+ 「查询的关键字」+ 「type参数值」拼接成的。 比对两种方式,通过python方法改写更加方便,主要代码如下: 1234from hashlib import md5def md5value(s): a = md5(s.encode()).hexdigest() return a 到这里这次的分析的网站 – 淘大象 就被我们攻克了，这次的网站加密不是很难主要难点是分析时的无限debugger，其实其他同类型的网站还有很多,这里我推荐大家阅读下面的文章，文章提到了其他解决debugger的方法希望对你有帮助。 https://mp.weixin.qq.com/s/BwH1JxY5jhFomZLwlQIvSA]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>淘大象</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-历代JS混淆学习]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A007-%E5%8E%86%E4%BB%A3%E6%B7%B7%E6%B7%86%2F</url>
    <content type="text"><![CDATA[eval把一段字符串当做js代码去执行 1eval(function()&#123;alert(100);return 200&#125;)() 例子: 漫画柜,空中网 变量名混淆 把变量名、函数名、参数名等，替换成没有语义，看着又很像的名字。 1_0x21dd83、_0x21dd84、_0x21dd85 用十六进制文本去表示一个字符串 1\x56\x49\x12\x23 利用JS能识别的编码来做混淆JS是Unicode编码，本身就能识别这种编码。类似的一些变量名，函数名都可以用这个表示，并且调用。 1234567891011类似:\u6210\u529f表示中文字符(成功)。类似:\u0053\u0074\u0072\u0069\u006e\u0067.\u0066\u0072\u006f\u006d\u0043\u0068\u0061\u0072\u0043\u006f\u0064\u0065就代表String.fromCharCode类似:('')['\x63\x6f\x6e\x73\x74\x72\x75\x63\x74\x6f\x72']['\x66\x72\x6f\x6d\x43\x68\x61\x72\x43\x6f\x64\x65'];效果等同于String.fromCharCode 把一大堆方法名、字符串等存到数组中，这个数组可以是上千个成员。然后调用的时候，取数组成员去用 123var arr = ["Date","getTime"];var time = new window[arr[0]]()[arr[1]]();console.log(time); 字符串加密后发送到前端，然后前端调用对应的函数去解密，得到明文 1234567var arr = ['xxxx']// 定义的解密函数function dec(str)&#123; return 'push'&#125;test[dec(arr[0])](200); 乱序混淆(这名字我自己起的)将顺序执行的代码混淆成乱序执行,并加以混淆 以下两段代码的执行结果是相同的:123456789101112131415161718192021222324252627282930313233343536373839// 正常形态function test(a)&#123; var b = a; b += 1; b += 2; b += 3; b += 4; return a + b&#125;// 乱序形态//（这里比较简单,在很多加密网站上case 后面往往不是数字或字符串,而是类似 YFp[15][45][4]这样的对象，相当恶心）function test1(a)&#123; var arr = [1,2,3,4,5,6] for(var i = 0, i &lt; arr.lenght, i++)&#123; switch (arr[i]) &#123; case 4: b += 3; break; case 2: b += 1; break; case 1: var b = a; break; case 3: b += 2; break; case 6: return a + b case 5: b += 4; break; &#125; &#125;&#125;// 结果都是30 但是test1看着费劲console.log(test1(10));console.log(test(10)); 压缩代码把多行代码压缩成一行 1234567891011121314function test(a)&#123; var b = a; var c = b + 1; var d = b + 2; var e = b + 3; var f = b + 4; return e + f&#125;// 压缩一下function test1(a)&#123; var b,c,d,e,f return f = (e = (d = ( c = (b = a,b + 1),b + 2),b + 3),b + 4),e + f&#125;]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>混淆</tag>
        <tag>eval</tag>
        <tag>变量名混淆</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-贝贝网加密分析]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A009-%E8%B4%9D%E8%B4%9D%E7%BD%91%E7%99%BB%E9%99%86%E5%8F%82%E6%95%B0%2F</url>
    <content type="text"><![CDATA[又来练手了，需求在这: 想想有点激动: 分析请求用浏览器抓一波登陆包: 可以看到有 _abr_和 beibeitoken 两个加密参数。 我们再找找发现beibeitoken是上一个请求返回的: 好像难度一下减少了一半。 定位加密位置还是老套路,搜就完事了: 通过关键字就找到一个相关文件,先进去看看。 先打上断点,重新登录一下,成功进入断点说明位置十有八九是找到了,F11进去看看里面的加密逻辑是什么样的。 看上去逻辑还蛮简单的,就是先这样再那样然后拼接在一起返回回来,完事儿~ 分析加密我们一个个跟进去研究下: u,d,a,e,s这几个加密都可以通过上图的方法追踪到对应的加密。 这里我们主要研究f与m的加密逻辑。 先看看f的加密逻辑:1var f = d.length ? (0,l["default"])(d.join("&amp;")) : "" 这里跟进去看看l[&quot;default&quot;]是什么: 这里的e是初始化加密算法,我们再跟进去看看: 看到这里我已经不怎么想扣代码了,这么多乱七八糟的什么东西,要是真抠出来估计头发要掉一半。 而且这个代码看着就不像自写的算法,估计也是套的通用的那几种算法,所以我开始翻之前扣过的代码，果然让我找到了: 上面的代码是不是长得有点像,为了验证这个想法我把js里关于加密的代码扣下来look look 不得不说好的编辑器可以让你事半功倍看到图中划线的地方这个参数的加密方法就一目了然了。 接下来继续分析m的加密方法 1m = (0,c["default"])(g, "ytU7vwqIx2UXQNsi"); 有了上面的铺垫,我们继续追进去分析一波: 接下里就是编辑器表演的时候了 好了，到这里就破案了。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>贝贝网</tag>
        <tag>密码加密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-犀牛数据加密分析]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A011-%E7%8A%80%E7%89%9B%E6%95%B0%E6%8D%AE%2F</url>
    <content type="text"><![CDATA[咸鱼又来练手了,这次来研究下在搜索参数搜不到的情况下怎么办？ 有点经验的朋友肯定知道这次要用的就是 XHR BreakPoint。 关于XHR BreakPoint在很多文档中都有提及，咸鱼就不啰嗦了,还不是很了解的可以看下面的文档。 https://developers.google.com/web/tools/chrome-devtools/ 【大宝贝】 在分析请求之前,咸鱼分享一个调试小技巧。 今天在交流群划水的时候有几个群友比较纠结于如何判断数据是不是由js生成或异步加载的。 我常用的方法有两种: 第一种,右键查看「网页源代码」,之后在打开的网页源码的界面搜索我们想要得数据是否在其中就可以判断了。 第二种,关闭网页的js加载功能,查看网页我们需要的数据是否能够顺利加载或者查看数据是否完整,步骤也非常简单。 第一步: 点击地址栏前端的图示位置,选择网站设置。 第二步: 在权限位置将JavaScript选择为禁止,返回刷新网页,查看数据是否顺利加载。 以上两个小技巧可以大大方便我们的调试,就不用猜来猜去了。 分析请求用上面的方法我们判断,数据是通过js加载的,并且请求的参数和返回的参数都是经过加密的。 到这里就需要 XHR 断点帮助我们定位加密了。 定位加密位置我们先添加一个XHR断点。 重新刷新页面。 可以看到断点的位置上已经有了我们需要的参数加密。 加密分析参数加密我们先找到参数还没有加密的地方。往上找很快就看到参数还没有加密的地方。 可以看到上图1,2两个断点的位置还只是对数据进行序列化等操作,到了第3,4断点的位置参数就被加密了。所以我们可以重新刷新一下看看数据是怎么被加密的。 到这个位置就可以追进去分析了,接下来就比较简单了。 可以看到这里就是参数的主要加密逻辑: 还有就是关于 sig 参数,逻辑也被分析出来了。 接下来就是分析返回的参数解密的位置了。 数据解密数据解密的位置很快也被定位到了,我们看到 l 就是返回的密文. 我现在断点端的位置也就是解密的位置。按照参数加密的方法追进去就可以看到解密的方法了. 把解密和解密的方法扣出来就是我们需要的js代码了。 本文完。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密文解密</tag>
        <tag>犀牛数据</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-工信系统加速乐]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A012-gsxt_gg%2F</url>
    <content type="text"><![CDATA[本周练手加一 上次分享了两个练手喂饭教程，朋友好奇喂饭是啥意思？就是把知识当成饭，喂到嘴里。 【表情】 同时咸鱼整理了一下过往所有关于JS逆向文章的代码，希望附上代码能让你学( 记 )的( 得 )更( star )快： https://github.com/xianyucoder/Crack-JS 【nice】 接下来继续练手，上次的项目二有读者反映讲的不是很明白，这次找了同类型的网站再来一次。 分析请求打开 开发者模式 抓包 很快就可以看到下面这个请求返回的是我们需要的数据： 我们先根据这里的请求，写一份代码测试一下哪些是我们必须的参数： 类似这样： 1234567891011121314151617import requestsurl = "http://www.gsxt.gov.cn/affiche-query-area-info-paperall.html"querystring = &#123;"noticeType":"11","areaid":"100000","noticeTitle":"","regOrg":""&#125;payload = "draw=1&amp;start=0&amp;length=10"headers = &#123; 'Cookie': "__jsluid_h=aa1c7a0fc461a8ea63747a4b02956b38; __jsl_clearance=1565006542.706|0|D0rZ6Fm9vgqQ2QyKbNgIlZRkMsw%3D; SECTOKEN=6968716378603526401; UM_distinctid=16c61a8e0d03f5-01bbcf42e0196a-37647c02-13c680-16c61a8e0d17c1; CNZZDATA1261033118=2135658768-1565002015-http%253A%252F%252Fwww.gsxt.gov.cn%252F%7C1565002015; JSESSIONID=40B833BC895E78C1A2E6C2D74C1DE6F8-n2:2; tlb_cookie=S172.16.12.42", 'Origin': "http://www.gsxt.gov.cn", 'Accept-Encoding': "gzip, deflate", 'Accept-Language': "zh-CN,zh;q=0.9", 'User-Agent': "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/75.0.3770.142 Safari/537.36", &#125;response = requests.post(url, data=payload, headers=headers, params=querystring)print(response.text) 通过不断删除 headers 中的参数并观察返回值，来判断哪些是必须携带的参数，哪些是无关紧要的参数。 测试过程比较繁琐就不一一截图演示，同时建议放慢测试请求速度，否则会触发网站的反爬措施。 经过测试可以发现 UA 以及 Cookie 中的 __jsluid_h __jsl_clearance SECTOKEN or JSESSIONID 都是不可缺少的参数。 分析参数的生成现在就需要一一找齐参数是怎样生成的： 这里的 UA 就不需要分析了，直接复制浏览器的即可，这里需要注意的是这里的 UA 与 Cookie 还有 IP 是对应的，所以出现更换 UA 的情况那么 Cookie 就需要重新生成，这样的设置对于批量爬取的兄die是个难度不小的点，我们抱着学习的态度就不用考虑太多了。 【有点厉害】 接下来分析 Cookie 的生成，先看看 __jsluid_h 通过用浏览器抓包 可以看到 __jsluid_h 是通过请求 Set 进来的，我们照着来一遍就行。 接下来看看 SECTOKEN or JSESSIONID 和上面的参数套路一样，区别在于这个请求的 Cookie 中必须带上 __jsl_clearance 接下来需要关注的就只有 Cookie 值 __jsl_clearance 在这几个请求包里反复筛选都没有发现这个值是怎么写入的（怕漏掉的可以用 Fiddler 抓），那么就只可能是 js 生成的了。 通过不带 Cookie 的方式请求一次，可以看到网页返回了和上次文章中加密二类似的 js 代码 ，按照上次文章的套路，我们复制到 Console 中测试一下。 测试流程： 先把 eval 改成 console.log 让结果打印出来。 把测试结果复制出来格式化，可以看到格式化后的代码分为两个部分，主要的是上面的 Cookie 生成代码。 所以单独把上半部分代码复制出来调试即可，需要注意的是通过 Python 调用的 Node 环境不支持 document 这类浏览器属性所以调试的时候需要将它去除或在 Node 下执行不报错即可。 通过调试我们生成了下面这个字符串。 运用至 Python 中，即可请求到需要的数据了。 丢个效果图，溜了。 后台回复「js逆向」获取完整代码。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>工信系统</tag>
        <tag>加速乐</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-财联社加密分析]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A013-%E8%B4%A2%E8%81%94%E7%A4%BE%2F</url>
    <content type="text"><![CDATA[本周练手加一。 这次写一写这周险些翻车的一次逆向吧,整体加密逻辑并不难,是自己太过想当然,要是没有群友的提醒就栽了。 【群除我佬】 分析请求老规矩先分析一波请求。 这次要分析的是首页的「电报」部分,刷新首页的发现抓不到「电报」这部分的请求。无奈点击加载更多发起请求(其实并没有区别) 选中部分就是要分析的请求了,想要快速定位可以使用上一篇用到的 XHR 断点,没看过的看下面: 除了此之外,还有一个比较坑的地方,这个网站是定时刷新的，所以打在sign逻辑上面的断点不用刷新就自己断上了,再配合上这个网站的叮咚声。 和无限debugger有异曲同工之妙。 【白眼】 定位加密位置打上 XHR 断点很快就看到 sign 的位置了 先打上断点刷新试试。 重新点击加载更多之后果然断上了,现在就要一步一步追进去分析。 分析加密先追进去看到返回了三个参数: 之后把所有的参数传入并拼接: 接下来才是重头戏: 看到这种e.xxx样式的加密我就想到之前逆向遇到的CryptoJS,对比一下果然差不多,果然一对比和CryptoJS.sha1一毛一样。 接下来就是打脸现场了,我以为到这里就结束了,没想到: 本来还想秀一下,结果翻车了。 经过群里大佬的提醒发现其实后面还有一层加密: 也怪我自经验主义作祟,希望大家不要像我这样的翻车。 ==&gt; 群里隐藏的大佬是真的多。 本文完。 我是只会说老哥牛逼的咸鱼。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密文解密</tag>
        <tag>财联社</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-66ip加速乐]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A014-66ip%2F</url>
    <content type="text"><![CDATA[练手系列来喽~ 这周在读者群顺手搞了个小活动,在群里摸鱼的时候有水友说练手系列能不能来点简单的写详细点,所以我这次准备了两个比较简单的练手加密,简单到我一度怀疑自己在水文。 加密一 分析请求先打开受害网站 -&gt; 打开控制台 -&gt; 切换至 XHR 完美,刷新就看到了。 这里要解决的只有两个: 返回的密文 请求中的token 接下来定位加解密位置 定位加密位置与分析加密按照之前的套路，我们第一步是要搜索加密参数名 token 打开search ,搜索参数名就找到我们要的 token 了。 按照搜索的结果文件名,猜也猜到是第二个结果,点击打开就可以看到token 的生成代码了。 1var token = md5(String(page) + String(num) + String(timestamp)); 这段代码我们可以用 js 实现也可以用 python。 偷懒一点用 python 实现 token加密算法: 按照请求里看到的参数生成 token 试试 现在有了 token 参数之后接下来看看这个解密怎么搞: 右键 查看源代码 发现了这个: 图中ip 列表的位置是没有数据的,只有 id ip_list ,这是个唯一值,所以值得再搜索一下: 追进去之后就可以看到下面的代码了: 打上断点,执行两步可以看到大 decode_str 后就开始出现我们需要的数据了,所以这里的decode_str 就是我们要的解密方法: 我们复制到编辑器里调试运行一下: 可以看到提示Base64 未定义, 我们追进去把 Base64 的算法赋值出来: 再次运行,根据提示缺失 window 对象, 根据之前的方式是缺啥补啥,但我们先接下来分析一下这里调用的方法: 这里调用的是 String.fromCharCode 方法,我们替换下再次运行: 可以看到这里就解出来了。 加密一，完。 加密二 分析请求这次要分析的网站不同上一个的是,这次网站使用的是 cookie 加密, cookie的有效期过了之后就会返回一段 js 。 使用有效的 cookie 请求我们需要的数据就在返回的网页中,所以我们只要解决这段 js 就好了。 分析加密把返回的 js 复制到编辑器里格式化,可以看到代码分为两个部分: 参数定义部分: 函数执行部分: 直接执行没得出结果,为了方便知道这段代码发生了什么,我们复制到 浏览器 console 里看看: 为了方便观察,我们把第二段的 eval 修改为 console.log 可以看到这段代码写入了 cookie: 继续分析,我们把这段cookie 相关的代码复制出来执行一下: 可以看到这里成功生成了一段 cookie 值 和我们在网页上看到的一样！ 到这里加密就分析结束了。 加密二，完。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>加速乐</tag>
        <tag>66ip</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-关于jsfuck的处理]]></title>
    <url>%2F2019%2F07%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A015-jsfuck%2F</url>
    <content type="text"><![CDATA[文章开始我本想和大家用 JSFuck 打个招呼，结果发现是我天真了，简单的一句 hello world 转换后有近 5 W 字符。 所以大家将就看看下面的图： 【表情】 JSFuck是什么？下面这个网站是 JSFuck 的官网： http://www.jsfuck.com/ 我们可以使用它对我们原有的 JS 代码进行编码风格上的转换。 JSFuck 是使用 [、]、(、)、! 和 + 六种字符来表示原有的字符的，就像下面这样的对应关系： 123456789101112131415false =&gt; ![]true =&gt; !![]undefined =&gt; [][[]]NaN =&gt; +[![]]0 =&gt; +[]1 =&gt; +!+[]2 =&gt; !+[]+!+[]10 =&gt; [+!+[]]+[+[]]Array =&gt; []Number =&gt; +[]String =&gt; []+[]Boolean =&gt; ![]Function =&gt; []["filter"]eval =&gt; []["filter"]["constructor"]( CODE )()window =&gt; []["filter"]["constructor"]("return this")() 当然还有更多的对应关系没有列出来，但是这并不重要，我们只需要知道 JSFuck 是什么就可以了，接下来我们实战走一波。 如何在实战中处理 JSFuck?JSFuck 的处理方法有以下几种： 123document.write(xxx)alert(xxx)console.log(xxx) 这里的 xxx 指的就是 JSFuck 的内容，解决方法很简单，我们拿个例子测试一下。 这里本来想拿某文书网的 JSFuck 实战的，但是自从某文书网下了瑞数之后，整个网站频繁瘫痪。 猝。 所以这里用另一个例子测试一下，因为处理的方法是通用的： 复制 JSFuck ，打开 开发者模式，切换至 Console 选项卡，输入下面的内容并执行： 1console.log(xxx) 执行结果如下： 这里看到提示Cannot read property &#39;attr&#39; of null 我们不管他，我们注意到右手边的vm87:1,点进去看看： 这里就是浏览器为我们还原的代码了，用其他的方法也是同样的效果。 欢迎大家提供更多 JSFuck 的案例，本文测试的 JS 已经上传至 Github , 后台回复「js逆向」即可获取地址。 本文完。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>密文解密</tag>
        <tag>jsfuck</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-基础整理 ( 二 )]]></title>
    <url>%2F2019%2F06%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A001-chrome%E8%B0%83%E8%AF%95%E5%B7%A5%E5%85%B7%E5%B8%B8%E7%94%A8%E5%8A%9F%E8%83%BD%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[chrome调试工具常用功能整理(待补充)windows: ctrl + shift + imac: cmd + opt + i ElementsDom 选中 元素 切换至 Event… Tab可以查看这个元素绑定的事件 在 Element 选项卡中可以修改html 断点 在 Sources 面板 js 文件行号处设置断点, 这里除了常规断点外, 还有个条件断点(右键 conditional breakpoint), 在设置的条件为 true 时才会断电, 在循环中需要断点时比较有用 在调用堆栈这里可以切换到堆栈中的任何地方重新执行(右键restart frame), 如果想查看断点前的信息时比较有用 断点后的变量保存到全局 选中变量, 右键 Evalute in console 在 console 中选中输出的内容, 右键 store as global variable dom mutation 断点(推荐) dom mutation event 是 DOM3 添加的新的事件, 一般是 dom 结构改变时触发. devtools 可以对 DOMSubtreeModified DOMAttrModified 和 DOMNodeRemoved 时断点. 对上面 元素上事件断点(mouseover) 后不容易找到业务代码, 使用 mutation 断点, 断点后配合 call stack 就可以找到业务代码了 全局事件断点 devtools 还可以对事件发生时断点, 比如 click 发生时断点, 这个跟 元素上事件断点 不同, 不会限定在元素上, 只要是事件发生, 并且有 handler 就断点; 还可以对 resize, ajax, setTimeout/setInterval 断点. xhr 断点 xhr：XMLHttpRequest 使用步骤 打开 chrome 浏览器控制台 选择 source 面板 点击 XHR Breakpoints 右侧的“+”，添加断点规则 实例文章： 条件断点 实例文章 : 【无限debugger】 几个常用的断点快捷键: F8: 继续执行 F10: step over, 单步执行, 不进入函数 F11: step into, 单步执行, 进入函数 shift + F11: step out, 跳出函数 ctrl + o: 打开文件 ctrl + shit + o: 跳到函数定义位置 ctrl + shift + f: 所有脚本中搜索 Console元素选择 $(selector) 即使当前页面没有加载jQuery，你也依然可以使用$和$$函数来选取元素，实际上，这两个函数只是对document.querySelector()和document.querySelectorAll()的简单封装，$用于选取单个元素，$$则用于选取多个 $_ 使用 $_ 来引用最近的一个表达式 使用Reres替换线上文件 待补充 使用Charles 替换线上JS文件 先把电脑上的代理软件关掉，再打开 Charles，在菜单栏选择 Proxy 打开 macos Proxy。 打开 Chrome 上的开发者工具，选择 Network 把禁止缓存勾上(Disable cache)。 刷新页面，在 Charles 上选中需要替换的 JS 代码，右键选择 Map Local…，然后在点击 Choose 按键，选择需要替换的 JS 代码。 使用Fiddler修改线上JS文件 打开 Fiddler ，设置只抓取浏览器流浪 使用 SwitchyOmega 设置 浏览器代理模式 为 Fiddler代理 使用 Fiddler 截断请求，找到需要修改的 JS 文件修改后 点击 RUN 实例网站 : 某文书网首页 JS 未完待续。。。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>Chrome调试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-基础-DES加密与Base64编码实例]]></title>
    <url>%2F2019%2F06%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A001-des%E4%B8%8Ebase64%2F</url>
    <content type="text"><![CDATA[讲在前面本篇旨在简单总结js与Python加密的一些例子,文中演示的案例也是面向新手,如果有大佬很碰巧看到这篇文章,欢迎加我交流,咸鱼愿意付费学习。 常用的加密有哪些？ 对称加密（加密解密密钥相同）：DES、DES3、AES 非对称加密（分公钥私钥）：RSA 信息摘要算法/签名算法：MD5、HMAC、SHA 学习资源推荐冷月大佬的博客:https://lengyue.me/ 突破前端反调试–阻止页面不断debugger:https://segmentfault.com/a/1190000012359015 岚光的JavaScript反调试和混淆:https://0x0d.im/archives/javascript-anti-debug-and-obfuscator.html 常用调试流程以下是可以参考的调试流程(面向新手): 如果网页有跳转，必须勾选 preservelog 防止丢包 看一下有没有框架 右键查看框架源代码(弹出式登陆界面) 登陆尽量使用错误密码 防止跳转 查看关键登陆包 分析哪些参数是加密的 使用别的浏览器分析哪些参数是固定的值 初步猜测加密方法 搜索 直接搜索参数 pwd= pwd = pwd: pwd : 密码框地方右键 检查 查看 id name type 找到加密的地方(重点) 调试 找出所有的加密代码 从最后一步开始写起，缺啥找啥 如果找的是函数的话 search 要带上 function xxx 如果看到加密的地方有个类，并且之后是用 prototype 把方法加在原生对象上的话，要把所有加在原生对象上的方法都找出来 函数找多了没关系，只要不报错不会影响结果，但是不能找少了 DES加密js调试实战案例 为了避免被删文,所以这里就不说是哪个网站了,有需要可私信 我们先参照上面的调试流程抓包,搜索加密参数’password’,经过测试通过’password=’这关键字找到了加密所在的js。 经过测试(步骤7)在文件内搜索’password:’找到加密位置,并通过断点确认了猜测。 通过点击提示的路径,我们找到以下代码: 找到这里就可以把需要的js代码扣出来,然后一步步调试了,通过补齐缺失的代码,得到以下js加密代码并运行: 12 输出运行结果’+g64648uhmWlN9eoU3Tszw==’,与我们抓包的结果相同。 Base64编码js调试实战案例 同样参照上面的调试流程,搜索加密参数’encodePassword’,经过测试通过’encodePassword =’这关键字找到了加密所在的js,并通过断点验证 找到关键加密位置后,查看加密的js文件,直接复制出来即可 base64编码比较简单,直接复制补上我们的密码,输出的就是编码后的字段了 Python实现加密方法合集这里给大家推荐一个大佬写的代码(我就不献丑了),记得给大佬来个starGitHub地址: 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264https://github.com/dhfjcuff/R-A-M-D-D3-S-M-H/blob/master/RSA-AES-MD5-DES-DES3-MD5-SHA-HMAC.py# -*- coding：utf-8 -*-import base64import rsafrom Crypto.Cipher import AESfrom Crypto.PublicKey import RSAfrom pyDes import des, CBC, PAD_PKCS5from Crypto.Cipher import DES3import hashlibimport hmacclass USE_AES: """ AES 除了MODE_SIV模式key长度为：32, 48, or 64, 其余key长度为16, 24 or 32 详细见AES内部文档 CBC模式传入iv参数 本例使用常用的ECB模式 """ def __init__(self, key): if len(key) &gt; 32: key = key[:32] self.key = self.to_16(key) def to_16(self, key): """ 转为16倍数的bytes数据 :param key: :return: """ key = bytes(key, encoding="utf8") while len(key) % 16 != 0: key += b'\0' return key # 返回bytes def aes(self): return AES.new(self.key, AES.MODE_ECB) # 初始化加密器 def encrypt(self, text): aes = self.aes() return str(base64.encodebytes(aes.encrypt(self.to_16(text))), encoding='utf8').replace('\n', '') # 加密 def decodebytes(self, text): aes = self.aes() return str(aes.decrypt(base64.decodebytes(bytes( text, encoding='utf8'))).rstrip(b'\0').decode("utf8")) # 解密class USE_RSA: """ 生成密钥可保存.pem格式文件 1024位的证书，加密时最大支持117个字节，解密时为128； 2048位的证书，加密时最大支持245个字节，解密时为256。 加密大文件时需要先用AES或者DES加密，再用RSA加密密钥，详细见文档 文档:https://stuvel.eu/files/python-rsa-doc/usage.html#generating-keys """ def __init__(self, number=1024): """ :param number: 公钥、私钥 """ self.pubkey, self.privkey = rsa.newkeys(number) def rsaEncrypt(self, text): """ :param test: str :return: bytes """ content = text.encode('utf-8') crypto = rsa.encrypt(content, self.pubkey) return crypto def rsaDecrypt(self, text): """ :param text:bytes :return: str """ content = rsa.decrypt(text, self.privkey) con = content.decode('utf-8') return con def savePem(self, path_name, text): """ :param path_name: 保存路径 :param text: str :return:bytes """ if "PEM" in path_name.upper(): path_name = path_name[:-4] with open('&#123;&#125;.pem'.format(path_name), 'bw') as f: f.write(text.save_pkcs1()) def readPem(self, path_name, key_type): """ :param path_name: 密钥文件 :param key_type:类型 :return: """ if 'pubkey' in key_type: self.pubkey = rsa.PublicKey.load_pkcs1(path_name) else: self.privkey = rsa.PublicKey.load_pkcs1(path_name) return True def sign(self, message, priv_key=None, hash_method='SHA-1'): """ 生成明文的哈希签名以便还原后对照 :param message: str :param priv_key: :param hash_method: 哈希的模式 :return: """ if None == priv_key: priv_key = self.privkey return rsa.sign(message.encode(), priv_key, hash_method) def checkSign(self, mess, result, pubkey=None): """ 验证签名：传入解密后明文、签名、公钥，验证成功返回哈希方法，失败则报错 :param mess: str :param result: bytes :param pubkey: :return: str """ if None == pubkey: pubkey = self.privkey try: result = rsa.verify(mess, result, pubkey) return result except: return Falseclass USE_DES: """ des(key,[mode], [IV], [pad], [pad mode]) key:必须正好8字节 mode（模式）：ECB、CBC iv:CBC模式中必须提供长8字节 pad:填充字符 padmode:加密填充模式PAD_NORMAL or PAD_PKCS5 """ def __init__(self, key, iv): if not isinstance(key, bytes): key = bytes(key, encoding="utf8") if not isinstance(iv, bytes): iv = bytes(iv, encoding="utf8") self.key = key self.iv = iv def encrypt(self, text): """ DES 加密 :param text: 原始字符串 :return: 加密后字符串，bytes """ if not isinstance(text, bytes): text = bytes(text, "utf-8") secret_key = self.key iv = self.iv k = des(secret_key, CBC, iv, pad=None, padmode=PAD_PKCS5) en = k.encrypt(text, padmode=PAD_PKCS5) return en def descrypt(self, text): """ DES 解密 :param text: 加密后的字符串，bytes :return: 解密后的字符串 """ secret_key = self.key iv = self.iv k = des(secret_key, CBC, iv, pad=None, padmode=PAD_PKCS5) de = k.decrypt(text, padmode=PAD_PKCS5) return de.decode()class USE_DES3: """ new(key, mode, *args, **kwargs) key:必须8bytes倍数介于16-24 mode： iv:初始化向量适用于MODE_CBC、MODE_CFB、MODE_OFB、MODE_OPENPGP，4种模式 ``MODE_CBC``, ``MODE_CFB``, and ``MODE_OFB``长度为8bytes ```MODE_OPENPGP```加密时8bytes解密时10bytes 未提供默认随机生成 nonce：仅在 ``MODE_EAX`` and ``MODE_CTR``模式中使用 ``MODE_EAX``建议16bytes ``MODE_CTR``建议[0, 7]长度 未提供则随机生成 segment_size：分段大小，仅在 ``MODE_CFB``模式中使用，长度为8倍数，未指定则默认为8 mac_len： 适用``MODE_EAX``模式，身份验证标记的长度（字节），它不能超过8（默认值） initial_value：适用```MODE_CTR```，计数器的初始值计数器块。默认为**0**。 """ def __init__(self, key): self.key = key self.mode = DES3.MODE_ECB def encrypt(self, text): """ 传入明文 :param text:bytes类型，长度是KEY的倍数 :return: """ if not isinstance(text, bytes): text = bytes(text, 'utf-8') x = len(text) % 8 text = text+b'\0'*x cryptor = DES3.new(self.key, self.mode) ciphertext = cryptor.encrypt(text) return ciphertext def decrypt(self, text): cryptor = DES3.new(self.key, self.mode) plain_text = cryptor.decrypt(text) st = str(plain_text.decode("utf-8")).rstrip('\0') return stdef USE_MD5(test): if not isinstance(test, bytes): test = bytes(test, 'utf-8') m = hashlib.md5() m.update(test) return m.hexdigest()def USE_HMAC(key, text): if not isinstance(key, bytes): key = bytes(key, 'utf-8') if not isinstance(text, bytes): text = bytes(text, 'utf-8') h = hmac.new(key, text, digestmod='MD5') return h.hexdigest()def USE_SHA(text): if not isinstance(text, bytes): text = bytes(text, 'utf-8') sha = hashlib.sha1(text) encrypts = sha.hexdigest() return encryptsif __name__ == '__main__': aes_test = USE_AES("assssssssdfasasasasa") a = aes_test.encrypt("测试") b = aes_test.decodebytes(a) rsa_test = USE_RSA() a = rsa_test.rsaEncrypt("测试加密") b = rsa_test.rsaDecrypt(a) des_test = USE_DES(b"12345678", b"12345678") a = des_test.encrypt("测试加密") b = des_test.descrypt(a) des3_test = USE_DES3(b"123456789qazxswe") a = des3_test.encrypt("测试加密") b = des3_test.decrypt(a) md5_test = USE_MD5("测试签名") hmac_test = USE_HMAC("123456", "测试") sha_test = USE_SHA("测试加密") 小结本文简单介绍了关于js调试加密字段的流程,并且分别调试了关于DES与Base64加密的两个案例。 文中举例的案例属于菜鸟级别,但是千万因为这样就小瞧js逆向这门学问,因为上面写的案例其实连JS逆向入门的门槛都没跨过。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>基础</tag>
        <tag>DES</tag>
        <tag>BASE64</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-企名片加密分析]]></title>
    <url>%2F2019%2F06%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A004-%E4%B8%80%E6%AC%A1%E7%BE%A4%E8%81%8A%E5%BC%95%E5%8F%91%E7%9A%84%E7%9A%84js%E7%A0%B4%E8%A7%A3%E6%95%99%E5%AD%A6%2F</url>
    <content type="text"><![CDATA[事情起因 这是aio老陈的交流群里一位群友的提问,交流群里的大佬巨多,我想这个问题应该很快就会得到解决。 果不其然,很快有老哥给出了破解代码,默默点个赞,说句牛逼。 这里先贴上 Loco大佬的知乎专栏 , 专栏内容主要是爬虫反爬,逆向的文章,更多人关注说不定大佬能奋起更新。 https://zhuanlan.zhihu.com/webspider 还有给出代码的老哥i写的教程,发表在aio老陈的公众号: https://mp.weixin.qq.com/s/UP3EiMIuUV5jSQZIP7FJ8A 经过我看到起因里有老哥给出解答后,也抱着试一试的态度自己去试试,这次破解的主要内容在aio老陈的公众号写的足够详细了,所以本文就说说我遇到的几个卡住的点,对这次的解密做个笔记,便于之后复习回顾。 查看加密请求打开开发者模式,切换到XHR选项卡,发现这里有两个请求: 搜索加密参数快捷键打开全局搜索,搜索encrypt_data发现只找到一个文件,在文件内搜索encrypt_data有六处关联的地方: 这里我遇到第一个坑: 通过检索我很快就找到了关联的地方,但是我用错了方法,没有认真分析代码一直在callback里找解密的地方把问题复杂化了。 其实解密的地方就在下面截图断点的地方: 找到关键解密的地方通过两次F11的调试,我找到了解密的地方: 这里我遇到的第二个坑,这里坑的主要原因是我对调试工具的不熟悉,我一直使用的是F10的调试功能,正好 Loco大佬在群里讲解的时候说到了这一方面的资料: https://developers.google.com/web/tools/chrome-devtools/?hl=zh-cn 补全代码找到加密代码之后就可以按照前面缺啥补啥的套路了。 这里来个代码的开头,抛转引玉。 运行代码这里是第三个坑: 抠出全部的js代码后有一个坑的地方,就是在js运行正常的代码在python中调用的时候出现报错,这个报错的解决方案如下: 正常我们抠出的js代码如下:（关键部分） 实际上在python中调用的时候要这样写:(代码源自开头给出代码的老哥那) 123function my_decrypt(t) &#123; return new Buffer(s("5e5062e82f15fe4ca9d24bc5", my_decode(t), 0, 0, "012345677890123", 1)).toString("base64")&#125; 之所以这么改的原因是: “因为直接返回object给Python会报错，所以这里将JSON.parse移除了，返回parse前的json字符串,同时为了防止这串字符串内有特殊编码的字符，这里将它转成base64再return” – Loco大佬 以上就是这次源自群聊的js破解历程,希望本文中提及的文章、教程对你有所帮助。 EOF]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>企名片</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-梦幻西游装备阁加密分析]]></title>
    <url>%2F2019%2F06%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A010-%E6%A2%A6%E5%B9%BB%E8%A5%BF%E6%B8%B8%2F</url>
    <content type="text"><![CDATA[来咯来咯~这次真的来喽! 咸鱼练手第二弹。 最近风声略紧,咸鱼在这声明: 本公众号所有文章仅供技术交流未经许可严禁转载,且本公众号发布的内容不得用于非法用途,如有侵权，请后台联系咸鱼删除文章。 【律师函警告】 分析请求老规矩先抓包,找了半天没发现有异步请求,不过很快在Html页面里找到了下面的东西: 以我多年的经验来看,这个应该是把页面上的属性栏加密,页面展示的时候再按照某种解密的逻辑还原并展示的,就属这种最麻烦。 定位加密说他麻烦,果然定位加密定位了20多分钟还没找到,感觉超出了我的能力范围。 为了能够愉快的摸鱼不再烦恼,所以我决定今天就先到这。 本文完。 不过还好在带薪摸(la)鱼(shi)的过程中,我突然有了想法,想到之前一直忽略的点。 我先捋一捋思路: 这段加密的信息在页面展示一定有一个位置标识解密之后展示的位置,在页面密文上面展示的位置正好有一个id(全局唯一),如果猜的没错的话那事情就简单多了。 【奸笑】 于是,搜就完事儿了,结果发现没找到想象中的js文件。 遂,猝。 【吐血】 不过在这个页面里找两个equip_desc_panel,跟进去看看,发现下面这段代码: 1$("equip_desc_panel").innerHTML = parse_style_info(get_equip_desc("equip_desc_value")); 下意识再搜了一次: 果然我找到了这个看着就很像解密的东西,打上断点,刷新一下,成功进入断点。 这个定位加密的过程虽然曲折且还有点味道,但是事实告诉我们: 带薪摸(la)鱼(shi),有助于灵感迸发。 分析加密既然成功进入断点,先追进去看看: 这段经过混淆的代码就是我们要分析的加密了。 这段混淆初看有点头大,不过混淆的方式之前一篇文章也有提过,感兴趣的朋友可以翻翻看。 了解了混淆的方式之后,先平复下心情,解这种看着很像的东西最怕烦躁。 【淡定】 先从头逐步执行,并观察传入的值,顺带把代码复制一份并把代码里_0x1b3f48[&#39;\x63\x68\x61\x72\x43\x6f\x64\x65\x41\x74&#39;]这样的方法名翻译成正常的js代码便于理解,了解完大概解密流程,找到我们最终需要的值是这样生成的。 之后就是老套路缺啥找啥的阶段,不过扣代码的过程有两个坑点: 在扣代码的时候,明明整体逻辑已经扣好了,但是不返回任何值,通过打印调试发现这段代码里_0x1b3f48返回的是null值,以至于下面的for循环没有结果,所以需要注意。 还有就是在浏览器中base64编码转换使用的是_0x1c0cdf = _0xcbc80b[&#39;atob&#39;](_0x1c0cdf),但是在nodejs调试的时候使用的是Buffer.from(_0x1c0cdf,&quot;base64&quot;).toString() 经过一番不懈的努(tu)力(tou),成功得到结果。]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>梦幻西游</tag>
        <tag>密文解密</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JS逆向-基础流程梳理 ( 一 )]]></title>
    <url>%2F2019%2F05%2F16%2Fjs%E5%8A%A0%E5%AF%86%E5%AD%A6%E4%B9%A001-%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[基础流程 如果网页有跳转，必须勾选 preservelog 防止丢包 看一下有没有框架 右键查看框架源代码 登陆的话尽量使用错误密码 防止跳转 查看关键登陆包 分析哪些参数是加密的 使用别的浏览器分析哪些参数是固定的值 初步猜测加密方法 搜索(md5,aes,des,tripedes,rsa,encrypt,setpubkey,setpublickey) 直接搜索参数 pwd= pwd = pwd: pwd : 密码框地方右键 检查 查看 id name type 原生js方法获取 doucumnet.getElementById[“”].value doucumnet.getElementByName[“”][0].value jQuery获取 alert($(“#id”).val()); alert($(“input[type=’password’]”).val()); 找到加密的地方 调试10. 从最后一步开始写起，缺啥找啥 如果找的是函数的话 search 要带上 function xxx 如果看到加密的地方有个类，并且之后是用 prototype 把方法加在原生对象上的话，要把所有加在原生对象上的方法都找出来 函数找多了没关系，只要不报错不会影响结果，但是不能找少了 直接保存整页JS浏览器调试 RSA加密RSA中比较熟悉的东西是: public key 本地用公钥加密，服务器上用私钥解密 Encrypt 这个关键字 长度是128位或者256路 MD5加密 长度 32 位]]></content>
      <categories>
        <category>js逆向</category>
      </categories>
      <tags>
        <tag>js逆向</tag>
        <tag>基础流程</tag>
        <tag>RSA</tag>
        <tag>MD5</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[App爬虫环境搭建]]></title>
    <url>%2F2019%2F04%2F07%2Fapp_spider%2F</url>
    <content type="text"><![CDATA[本文来讲讲APP抓包环境搭建。App抓包应该是每个爬虫工程师都避不开的话题,在之前我也写过关于自动参与「抽奖助手」抽奖的文章,当时使用的抓包工具是Charles,有需要的朋友可以翻下之前的文章。 App抓包原理 客户端向服务器发起HTTPS请求 抓包工具拦截客户端的请求，伪装成客户端向服务器进行请求 服务器向客户端（实际上是抓包工具）返回服务器的CA证书 抓包工具拦截服务器的响应，获取服务器证书公钥，然后自己制作一张证书，将服务器证书替换后发送给客户端。（这一步，抓包工具拿到了服务器证书的公钥） 客户端接收到服务器（实际上是抓包工具）的证书后，生成一个对称密钥，用抓包工具的公钥加密，发送给“服务器”（抓包工具） 抓包工具拦截客户端的响应，用自己的私钥解密对称密钥，然后用服务器证书公钥加密，发送给服务器。（这一步，抓包工具拿到了对称密钥） 服务器用自己的私钥解密对称密钥，向客户端（抓包工具）发送响应 抓包工具拦截服务器的响应，替换成自己的证书后发送给客户端 爬虫的本质就是「骗」过服务器,各种反反爬手段就是增强信任的过程,不停的让服务器相信你是自己人如果你被反爬了,就是你骗术不精,被发现了。 App数据好抓吗？App数据有易有难,目前我还是停留在简单的部分,这里简单说下我理解的(如果说错求放过): 简单: app的数据比web端数据更容易抓取，基本都是http、https协议，返回的数据格式也相对规整,大多是json格式 困难: 1.需要反编译的知识,需要分析加密算法 2.需要脱壳+反编译 3.需要破解各式各类的签名,证书。。 所以一个爬虫工程师慢慢就需要掌握以下技能: java编程基础 android编程基础 app逆向 app脱壳 破解加密 … 从入门到全栈 常用抓包工具fiddler mitmproxy Charles fiddler安装和使用下载:https://telerik-fiddler.s3.amazonaws.com/fiddler/FiddlerSetup.exe 安装:一路Next 主要界面介绍: 会话列表界面: 监控面板在点击会话列表界面的某个请求后会出现下面两个界面: 请求面板: 响应面板: 本地CA证书安装 点击 Tools - Options - HTTPS - 勾选勾选 Capture HTTPS CONNECTs，勾选 Decrypt HTTPS trafic，会弹出安装证书的提示。一路点是或确定安装即可。 重启fiddler，点击右侧Actions，能看一个下拉菜单，点击 Export Root Certificate to Desktop，此时证书会生成到桌面上，名为 FiddlerRoot.cer，点OK保存 点击安装 连接手机抓包需要的配置先设置PC端fiddler: 之后手机需要访问 本地主机IP + 设置的端口，安装证书，证书安装成功后即可抓包。 以上就是关于Fiddler的安装和简单使用,进阶使用还有很多骚操作, mitmproxy安装和使用mitmproxy在linux版本下和windows版本下略有不同。 linux下借助pip,可以一键安装:pip install mitmproxy windows 下需要安装Microsoft Visual C++ V14.0以上,之后再使用pip install mitmproxy安装 mitmproxy有三大组件: mitmproxy - linux下的抓包组件 mitmdump - python交互 mitmweb - windows下的可视化界面工具 在windows下仅支持后两种组件的使用。 证书配置在安装目录下可以看到以下这些文件: 文件名 文件简介 mitmproxy-ca.pem PEM格式的证书私钥 mitmproxy-ca-cert.pem PEM格式证书，适用于大多数非Windows平台 mitmproxy-ca-cert.p12 PKCS12格式的证书，适用于Windows平台 mitmproxy-ca-cert.cer 与mitmproxy-ca-cert.pem相同，只是改变了后缀，适用于部分Android平台 mitmproxy-dhparam.pem PEM格式的秘钥文件，用于增强SSL安全性 window安装证书双击mitmproxy-ca.p12,一路确定直至结束。期间会弹出警告点击“确认”即可。 Mac安装证书Mac下双击mitmproxy-ca-cert.pem即可弹出钥匙串管理页面，然后找到mitmproxy证书，打开其设置选项，选择“始终信任”即可 Android/iPhone安装证书方法一: 将mitmproxy-ca-cert.pem发送到手机上点击安装就可以了,苹果手机点击安装描述文件即可。 方法二: 在linux下启动 mitmproxy,命令为mitmproxy -p 8889,同时将手机代理设置为linux的IP地址与端口后访问mitm.it安装证书。 简单使用mitmproxy过滤功能的使用举例: 123456输入z,清除屏幕上全部的包输入f,进入编辑模式,可在最下面编辑条件,ESC或Enter退出编辑!(~c 200) #显示所有返回不是200的请求!(~c 200) &amp; ~d baidu.com #显示域名包含baidu.com,返回不是200的请求~m post &amp; ~u baidu #显示请求的链接里面包含baidu的post请求~d baidu.com (http://baidu.com) 过滤所有域名包含baidu.com (http://baidu.com)的包 mitmproxy断点功能的使用举例:123456789输入i,进入编辑模式,可在最下面编辑条件,ESC或Enter退出编辑断点的条件和过滤是一样的,符合条件的链接会被拦截~d baidu.com &amp; ~m get域名包含baidu.com的get 请求会被拦截按Enter进入详情页,在详情页输入e进入模式,可以修改各项数据完成后,回到请求显示列表,输入a,将请求放行.1.请求重放2.选中需要重放的请求,输入r可以重放请求,也可以先编辑后再重放3.输入Q可退出程序 mitmproxy经常配合appium使用: 首先我们需要写一个抓包的脚本,类似下面这个:12345678import jsondef response(flow): if 'aweme/v1/user/follower/list/' in flow.request.url: for user in json.loads(flow.response.text)['followers']: info = &#123;&#125; nfo['share_id'] = user['uid'] info['_id'] = user['short_id'] save_task(info) 注意:这里的方法名必须使用response 编写完抓包的脚本后,使用 mitmdump -p [port] -s [脚本文件] 启动,配合appium自动化脚本即可实现app自动化抓包。 Charles的使用之前写过了相关的实战,可以直接看下面的文章。 以上是我面试后的一点总结,希望对你有所帮助~]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy要点复习与常用框架复习]]></title>
    <url>%2F2019%2F04%2F06%2Fscrapy-intrduct%2F</url>
    <content type="text"><![CDATA[Scrapy工作流程与注意点 从spiders抓取请求requests经过爬虫中间件发送给引擎 引擎将请求发送给调度器，调度器对请求进行调度 调度器将调度排序好的请求发送给引擎(调度器维护的是一个LIFO的队列，广度优先) 引擎将请求经过下载中间件发送给下载器 下载器将请求返回的response经过下载中间件发送给引擎 引擎将下载器返回的response返回给爬虫中间件 爬虫中间件将解析的字段与request返回给引擎 引擎将解析的字段发送给项目管道，将新的request发送给调度器 注意点 第四点是我们能应对反爬的最后一步，换代理，加cookie，换ua等等 第五点最好是能在这里校验数据，不是正确的数据不传给spider Scrapy的基本方法基本流程: 命令行123scrapy startproject projectnamescrapy genspider spidername hostnamescrapy crawl spidername 这里注意爬虫名称不要和项目名称相同 最简化的scrapy爬虫的执行流程 scrapy的name是用来标识是哪个爬虫的 不重写start_requests(),默认scrapy调用的是start_requests()获取start_url中的url 在执行第二步就默认回调parse()默认解析器 item主要函数 process_item(self,item,spider)必须实现的函数，用来处理item,常用的就是处理items,需要这个item就返回，不需要就Dropitem open_spider(self,spider)不是必须实现的函数,当爬虫运行的时候调用 close_spider(self,spider)不是必须实现的函数，当爬虫结束的时候调用 from_crawler(cls,crawler)不是必须实现的函数，当爬虫运行时调用，比open_spider()还早，常用于初始化爬虫的一些属性。 注意 yield管道里的时候就是yield item yield调度器里就是yield requests 中间件中间件最重要的作用就是拦截 下载中间件三大函数 process_request(request,spider)处理拦截 process_response(request,response,spider)处理拦截 process_exception(request,exception,spider)常用来处理process_request的异常 process_request 返回None,默认返回这个值,表示当前这层的下载中间件过了,去往下一层的下载中间件 返回request,返回这个值就是把当前请求重新丢回调度器队列，相当于重试 返回response,直接伪造一个返回值,把这个值丢到爬虫中解析 raises IgnoreRequest,抛弃当前的请求 process_response 返回response,默认返回到了下载中间件当中。 返回request,返回到队列当中，重新请求 返回raises IgnoreRequest,忽略了之前的请求也要忽略它对应的response process_exception 返回None,就是不处理传给下一个中间件 返回response,错误被纠正了,进入正常的流程 返回request，返回给队列重新请求 CookieMiddleware 单spider多cookie session 12for i,url in enumerate(url): yield scrapy.Request("xxxx.com",meta=&#123;'cookiejar':i&#125;,callback=self.parse_page) 注意需要注意的是 cookiejar meta key 不是“黏性的”，需要再之后的request请求中接着传递 123def parse_page(self,response): # do some processing return scrapy.Request("xxxx.com",meta=&#123;'cookiejar':response.meta['cookiejar']&#125;,callback=self.parse_other_page) 注意事项 中间件后面的数字越小越远离下载器，数字越大越靠近下载器 数字越小的request越先通过，数字越大的response越先通过 这里比较重要的是retry模块还有CookieMiddleware模块 爬虫中间件爬虫中间件的函数 from_crawler process_spider_input process_spider_output process_spider_exception process_start_request from_crawler(必须的)会被manager调用 process_start_request和process_spider_output很像，只能返回request scrapy middleware的实例DepthMiddlewarescrapy的数据收集收集404的页面demo12345678handle_httpstatus_list = [404]def __init__(self): self.fail_urls = []def parse(self, response): if response.status == 404: self.fail_urls.append(response.url) self.crawler.stats.inc_value("failed_url") Request和ResponseReuqests对象基础参数: url 请求的网址 meta 用来在页面之间传递的参数 callback 请求回来的response处理函数 header 设置页面的请求头 cookie 请求页面cookie 高级参数: encoding 请求的转换编码 priority 链接优先级 dont_filter 强制不过滤 errback 和callback一对，当请求成功时调用callback，请求失败调用errback 方法: copy() 复制一个一模一样的对象，常用是deepcopy() replace() 对对象参数进行替换 meta中常用的key: Response对象基础参数: url 请求的url body 请求回来的html meta 用来在页面之间传递数据 headers 页面的headers数据 cookies 设置页面的cookie Request 发出这个response对应的request对象 方法: copy() 与request相同 replace() 与request相同 urljoin() 将页面的相对路劲传入，返回绝对路径 follow() 传入一个相对路径直接返回一个request对象 scrapy的暂停和重启 暂停实例scrapy crawl spider lagou -s JOBDIR=job_info/001 spidername = lagouworkfilename = job_info且每次请求都要求不一样的 启动实例再次运行暂停的命令 自定义扩展参考scrapy源码中的extensions这个包，可以查看corestats这个文件的源码 去重与入库url去重 用造好的轮子 scrapy-deltafetch(前两个原理相同) scrapy-crawl-once scrapy-redis scrapy-redis-bloomfilters 自己造轮子 自己写的init_add_request方法实现轻量级的去重 scrapy_redis 运行爬虫:scrapy crawl 爬虫 redis-cli lpush myspider:start_urls xxx.comredis-cli sadd myspider:start_urls xxx.com scrapy-rabbitmq-linkSCHEDULR = “scrapy_rabbitmq_link.scheduler.SaaS”RABBITMQ_CONNECTION_PARAMETERS=’amqp://guest:guest@localhost:5672/‘SCHEDULR_REQUEUE_ON_STATUS=[500]DOWNLOADER_MIDDLEWARES = { ‘scrapy_rabbitmq_link.middleware.RabbitMQMiddleware’:999} celeryfrom celery import Celeryapp = Celery(‘hello’,broker=’amqp://guest@localhost//‘)@app.taskdef hello(): return ‘hello worlds’ 编写代码中的小tip 错误回调函数 1234from traceback import format_excdef error_back(self,e): _ = e self.logger.error(format_exc()) 通用爬虫-CrawlSpiderclass scrapy.spiders.CrawlSpider 爬取一般网站常用的spider 定义一些规则来提供跟进link的方便机制 链接提取器(link extractors) 导入scrapy.linkextractors import LinkExtractor 每个link extractor 有唯一的公共方法是extract_links,他接收一个Response对象，返回scrapy.link.Link对象 常用参数class scrapy.linkextractors.lxmlhtml.LxmlLinkExtractor() allow() -正则 deny() - 正则 allow_domains() - 域名 deny_domains() - 域名]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy-splash与splash]]></title>
    <url>%2F2019%2F04%2F06%2Fscrapy-splash_and_splash%2F</url>
    <content type="text"><![CDATA[splash介绍splash是一个针对js的渲染服务，它内置了一个浏览器和http接口，基于python3和twisted引擎，所以是可以异步处理任务。 安装https://splash.readthedocs.io/en/stable/install.html 只有mac和linux支持，也可以通过docker安装。 使用docker pull scrapinghub/splash 使用docker run -p 8050:8050 -p 5023 scrapinghub/splash运行镜像 使用基础使用使用浏览器访问http://localhost:8050 scrapy-splashscrapy集成seleniumselenium在scrapy中无界面的解决方案(linux) 安装环境 pip install pyvirtualdisplay 安装报错解决 apt-get install xvfbpip install xvfbwrapper 使用实例12345from pyvirtualdisplay import Displaydisplay = Display(visible=0, size=(800,600))display.start()browser = webdiver.Chrome()browser.get() selenium gridslinter]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器]]></title>
    <url>%2F2019%2F04%2F05%2Fpython-and-docker%2F</url>
    <content type="text"><![CDATA[有不少的朋友读了之前的一系列的Docker文章一脸懵逼，这和Python有什么关系？ Docker可以用来减轻我们搭建环境的繁琐步骤，我们完全可以把类似selenium等环境用Docker容器部署好，在代码里直接使用远端的selenium,简单方便。 使用Docker容器中的Python环境进行开发环境准备:腾讯云CentOS 7 + Docker 修改Docker配置简易版本: 编辑Docker相关配置文件:1vi /etc/docker/daemon.json 插入下面的配置: 注意这里是标准的json格式,格式出错Docker重启不了123&#123; "hosts": ["tcp://0.0.0.0:2375","unix:///var/run/docker.sock"]&#125; 重载Docker配置:systemctl daemon-reload 重启Docker:systemctl restart docker 复杂版本:这版本用于使用上面的配置修改之后依旧无法连接的情况,咸鱼第一次配置就出现了这个情况，咸鱼在某课网的手记板块找到了答案。 这里贴一下原文地址:1参考链接：https://www.imooc.com/article/details/id/28426 具体操作如下:编辑下面的文件:1vi /lib/systemd/system/docker.service 将文件对应配置项修改后保存: 1234将ExecStart=/usr/bin/dockerd改为ExecStart=/usr/bin/dockerd -H unix:///var/run/docker.sock -H tcp://0.0.0.0:2375 配置Pycharm首先打开Pycharm中Docker的显示项: 修改Docker配置: 在下图对应的地方填入你之前编辑的Docker配置: 这里注意格式: tcp://host:ports 配置好后提示连接成功即可。 配置远程Docker的镜像作为Pycharm的解释器点开配置,点击新增配置: 在新增项中选择Docker，这里会自动加载你连接的Docker服务中包含Python的全部镜像: 配置代码的自动上传你以为到上面就结束了吗？解释器是配置好了，但是你的代码还在本地，所以需要配置代码自动上传到云服务器。 首先找到对应的选项,之前没有配置过的话这里选项是灰色的，需要先配置, 点击 configuration : 点击加号，填入对应的配置，填完可以测试一下是否可用: 如果不能使用，建议登录控制台，配置相关的安全组配置。 切换到隔壁的mapping选项卡,按照下图配置相关的路径和要上传到服务器上的路径: 以上都做完之后，返回本小节的第一张图，有自动上传选项将它勾起，之后当你的文件有更改就会自动上传至服务器指定的路径下。 到这里，你以为结束了那就太天真了。 解决代码自动上传后运行报错当你上传代码后，运行后报错“无法找到对应的文件夹/文件”,这是因为这个时候我们的代码仅仅上传到了服务器上，但是我们的Python解释器是运行在容器中，而我们容器查找代码是查找的数据卷，所以这个时候就需要我们在配置中做一个简单的地址映射。 先看下上面一大段所涉及的原理图: 接下来我们开始配置,全局的地址映射: 在Docke组件中编辑配置，添加path mapping效果：当我们配置云服务路径时会自动将我们本地路径映射过去 设置Python默认的mapping为对应的路径: 这里注意一点:这里的container path 指的是容器中路径,结合上面的原理图，全部的映射过程是 本地路径 -&gt; 云服务器路径 -&gt; 容器路径 以上就是本次文章的全部内容了，如果对这其中的原理有不明白的咸鱼建议可以重新温习下之前关于数据卷的部分，如果还是不明白可以直接无视原理，按图索骥直接配好了事，当然欢迎大家留言交流~ ~]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dockerfile总结]]></title>
    <url>%2F2019%2F04%2F04%2FDockerfile%2F</url>
    <content type="text"><![CDATA[什么是Dockerfile?Dockerfile其实就是根据特定的语法格式撰写出来的一个普通的文本文件，可以使用docker build命令依次执行在Dockerfile中定义的一系列命令，最终生成一个新的镜像 如果你想要定制属于自己的镜像Dockerfile是你必不可少的工具。 docker build的使用想要构建定制化的镜像，可以使用docker build运行Dockerfile构建镜像 命令格式: docker build [参数] PATH | URL其中: PATH 是Dockerfile所在路径 URL 是Dockerfile所在的URL 常用参数:12-t 为镜像设置名称和tag(不适用-t参数，创建出来的镜像名称以及版本为none)-f 指定Dockerfile的路径(这是可以使用其他名称命名Dockerfile) 关于Dockerfile构建过程的浅析在之前关于Docker镜像与容器的文章中，咸鱼对镜像的文件系统做了一个简单的解释，不熟悉的朋友可以再温习一下。 我们先看下可以直接使用的Dockerfile结构是什么样的: 现在简单讲讲咸鱼对Dockerfile构建过程的理解： Dockerfile的执行顺序是自上而下进行的，当第一次使用Dockerfile构建镜像时Dockerfile中的全部命令都会执行一边最后构建为一个完整的镜像。 在构建的过程中，每执行一条命令Docker都会执行一次commit命令，接下来的每一条命令都是在前面所有命令集成的镜像基础上再次执行的。 这样的执行有什么好处呢，假设当你执行过一次的镜像构建后想要在Dockerfile中间部分添加一行Dockerfile的命令，那么在执行新的Dockerfile命令时，Docker只会执行新增命令(包括新增命令)之后的构建命令，大大减少了性能的损耗。 Dockerfile常用命令解析接下来就是关于Dockerfile命令的介绍更多实例可以参考官方的Dockerfile实例:https://github.com/docker-library/docs 关于Dockerfile命令的介绍也可以参考:https://docs.docker.com/engine/reference/builder/#usage RUN构建镜像过程中需要执行的命令，可以执行多条 同时RUN命令后可以使用两种形式输入要执行的命令 exec与shell 当使用exec方式输入执行的命令时，命令格式是json格式的，命令在当前进程执行 当使用shell方式输入执行的命令时，命令在子进程中执行(推荐使用shell方式) CMD与ENTRYPOINTCMD是添加启动容器是需要执行的命令，多条命令只有最后一条生效，可以在启动容器事被覆盖和修改。CMD命令格式有三种，exec、shell、默认提供给ENTRYPOINT的命令 命令格式为shell形式，命令是以子进程的形式执行 命令格式为json形式，命令在当前进程执行(推荐使用) 命令格式为json形式，且所有json内容为参数形式，则默认提供给ENTRYPOINT ENTRYPOINT的使用CMD与相同，但这个一定会被执行，且不会被覆盖和修改 LABEL与MAINTAINERLABEL用于为镜像添加对应的数据 添加的数据格式为:LABEL key=value …. Key=value MAINTAINER:用于表示镜像的作者(即将被遗弃使用) ENV与ARGENV用于设置执行命令时的环境变量，并且在构建完成后，仍然生效 ARG用于设置只在构建过程中使用的环境变量，构建完成后消失 ADD与COPYADD用于将本地文件或目录拷贝到镜像的文件系统中，且能解压特定格式文件，且能将URL作为要拷贝的文件(会先将URL的文件先下载下来再拷贝) COPY将本地文件或目录拷贝到镜像的文件系统中(推荐使用COPY+RUN)，因为ADD命令对于需要解压的文件支持的不是非常智能 VOLUME添加数据卷 命令格式可以为数组格式,也可以直接接上路径，路径为容器或镜像中的路径常用命令行格式为: VOLUME [“/XXX”] VOLUME /XXX USER与WORKDIRUSER指定以哪个用户的名义执行RUN，CMD和ENTRPOINT等命令 WORKDIR设置工作目录]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker总结]]></title>
    <url>%2F2019%2F04%2F01%2FDocker-end%2F</url>
    <content type="text"><![CDATA[Docker总结篇转眼关于Docker基础的相关文章就已经写完了，看完这一系列文章，希望你能有所收获。以下为本系列的全部文章: Docker系列文章除了文章的输出外，咸鱼在学习的过程中做了一份Docker的思维导图，如果有需要的朋友可以回复「Docker导图」获取PDF版。 以下为思维导图的图片版，以上相关资料，如有谬误，烦请指正~ 关于Docker的相关应用，之后咸鱼会继续输出相关文章。 除此之外，咸鱼接下来的文章不会仅仅局限在爬虫方向，例如web、数据可视化等方向都会有相关的文章输出，如果读到这里，你有更好的建议不妨在底部留言区说出你对咸鱼的建议，咸鱼不胜感激。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Compose浅析]]></title>
    <url>%2F2019%2F03%2F29%2FDocker-Compose%2F</url>
    <content type="text"><![CDATA[什么是Docker Compose Docker Compose是一个能一次性定义和管理多个Docker容器的工具。 Compose中定义和启动的每一个容器都相当于一个服务(service) Compose中能定义和启动多个服务，且它们之间通常具有协同关系 我们通常使用YAML文件来配置我们应用程序的服务，并且使用单个命令（docker-compose up），就可以创建并启动配置文件中配置的所有服务 如何安装Docker ComposeWindows与Mac:Docker for Mac与Docker for Windows自带docker-compose Linux系统:1234567sudo curl -L https://github.com/docker/compose/releases/download/[版本号]/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-composesudo chmod +x /usr/local/bin/docker-compose安装完成后使用 docker-compose version 查看版本可以访问 https://github.com/docker/compose/releases 查看历史版本 Docker Compose文件配置与注意事项Docker Compose file的配置项有非常多需要注意的点，如果一一赘述就相当于搬运官方文档了，这里标识几个经常使用的配置项，以供大家参考: 1234567version：指定Docker Compose File版本号services：定义多个服务并配置启动参数volumes：声明或创建在多个服务中共同使用的数据卷对象networks：定义在多个服务中共同使用的网络对象configs：声明将在本服务中要使用的一些配置文件secrets：声明将在本服务中要使用的一些秘钥、密码文件x-***：自定义配置。主要用于复用相同的配置。 官方文档直通车：https://docs.docker.com/compose/compose-file/#service-configuration-reference Docker Compose File 注意事项Docker Compose File 的格式要求非常严格，一定需要注意的有: 配置项的缩进使用空格 注意配置项冒号后要添加空格 编写完Docker Compose File后可以使用docker-compose config 检查文件是否出错 Docker Compose 项目实例演示Kafka运行环境搭建这个项目的Docker File在昨天的文章中已经给大家展示过了，这里重新贴一次，让大家先有个大概印象。 使用Docker Compose 部署小型flask web 项目 首先准备好你的flask项目源码，并上传至云服务器上 将项目需要使用的类库导出 requirements.txt 备用 编写运行对应项目的Dockerfile,并构建出一个新的项目镜像(这步也可以不做，但是这里为了方便理清思路建议按步骤一步一步来) 按照项目需要将要启动的容器写成对应的服务 使用 docker-compose up -d运行服务 Docker Compose 与 Dockerfile 总结关于Dockerfile与Docker Compose的两篇文章关于理论的描述比较少，大多都是实操的内容，如果你看到这里希望你能结合之前的文章动手实践一下，加深印象才能了解到关于Dockerfile和Dcoker compose 相关的难点。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 仓库管理]]></title>
    <url>%2F2019%2F03%2F24%2FDocker-cang-ku%2F</url>
    <content type="text"><![CDATA[什么是Docker仓库 Docker仓库就是存放docker镜像并有docker pull方法下载的云环境 docker pull 用法可以参考docker系列文章的第一篇: Docker仓库分为公有仓库和私有仓库。 公有仓库指Docker Hub(官方)等开放给用户使用、允许用户管理镜像。 私有仓库指由用户自行搭建的存放镜像的云环境。 如何搭建无认证私有仓库主要步骤如下: 第一步：在需要搭建仓库的服务器上安装docker。 第二步：在服务器上，从docker hub下载registry仓库 docker pull registry 第三步：在服务器上，启动仓库 12345docker run -d -ti --restart always\ --name my-registry\ -p 8000:5000\ -v /my-registry/registry:/var/lib/registry\ registry 注意：registry内部对外开放端口是5000。默认情况下，会镜像存放于容器内的/var/lib/registry(官网Dockerfile中查看)目录下，这样如果容器被删除，则存放于容器中的镜像也会丢失。 注意:本地利用curl 服务器IP:8000/v2_catalog查看当前仓库中的存放的镜像列表。（注意打开8000端口访问） 向私有仓库上传、下载镜像 第一步：利用docker tag重命名需要上传的镜像 docker tag IMAGE 服务器IP:端口/IMAGE_NAME 第二步：利用docker push上传刚刚重命名的镜像 docker push 服务器IP:端口/IMAGE_NAME 注意：必须重命名为服务器IP:端口/IMAGE_NAME 如果push出现了类似https的错误那么需要往配置文件/etc/docker/daemon.json里添加：”insecure-registries”:[“服务器IP:端口”] 然后重启docker。 搭建带认证的私有仓库在服务器上: 第一步：删除先前创建的无认证的仓库容器 docker rm -f my-registry 第二步：创建存放认证用户名和密码的文件： mkdir /my-registry/auth -p 第三步：创建密码验证文件。注意将将USERNAME和PASSWORD替换为设置的用户名和密码 1docker run --entrypoint htpasswd registry -Bbn USERNAME PASSWORD &gt; /my-registry/auth/htpasswd 第四步：重新启动仓库镜像 1234567docker run -d -p 8000:5000 --restart=always --name docker-registry \-v /my-registry/registry:/var/lib/registry \-v /my-registry/auth:/auth \-e "REGISTRY_AUTH=htpasswd" \-e "REGISTRY_AUTH_HTPASSWD_REALM=Registry Realm" \-e "REGISTRY_AUTH_HTPASSWD_PATH=/auth/htpasswd" \registry 12345678910111213#### 带认证的私有仓库，如何上传、下载镜像在本地机器上:- 第一步：首先登录到服务器 docker login -u username -p password 服务器IP:8000- 第二步：然后执行pull或者push命令 ,参考无认证仓库的上传/下载- 第三步：操作完毕后，可以退出登录 docker logout 服务器IP:8000这是如果想查看仓库中已有的镜像，那么需要进行http验证才可以。可以直接借助浏览器访问服务器IP:8000/v2/_catalog就可以访问了]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker 的安装与介绍]]></title>
    <url>%2F2019%2F03%2F23%2FDocker-de-an-zhuang%2F</url>
    <content type="text"><![CDATA[Docker 可以让你将所有应用软件以及它的以来打包成软件开发的标准化单元。 Docker 容器将软件以及它运行安装所需的一切文件（代码、运行时、系统工具、系统库）打包到一起，这就保证了不管是在什么样的运行环境，总是能以相同的方式运行。 Docker的版本介绍Docker有Docker-CE和Docker-EE两种。 Docker-CE指Docker社区版，由社区维护和提供技术支持，为免费版本，适合个人开发人员和小团队使用。 Docker-EE指Docker企业版，为收费版本，由售后团队和技术团队提供技术支持，专为企业开发和IT团队而设计。相比Docker-EE，增加一些额外功能，更重要的是提供了更安全的保障。 此外，Docker的发布版本分为Stable版和Edge版，区别在于前者是按季度发布的稳定版(发布慢)，后者是按月发布的边缘版(发布快)。 通常情况下，Docker-CE足以满足我们的需求。 Docker的安装这里主要介绍Docker在Windows/MacOS以及Centos/Ubuntu上的安装 Docker在centos/ubuntu上如何安装docker的安装推荐使用阿里云的源，速度快。 官方的安装教程：https://docs.docker.com/install/linux/docker-ce/centos/#install-using-the-repository 阿里源安装教程：https://help.aliyun.com/document_detail/60742.html 以下为搬运： Ubuntu:12345678910# step 1: 安装必要的一些系统工具sudo apt-get updatesudo apt-get -y install apt-transport-https ca-certificates curl software-properties-common# step 2: 安装GPG证书curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -# Step 3: 写入软件源信息sudo add-apt-repository "deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable"# Step 4: 更新并安装Docker-CEsudo apt-get -y updatesudo apt-get -y install docker-ce Centos:123456789# step 1: 安装必要的一些系统工具sudo yum install -y yum-utils device-mapper-persistent-data lvm2# Step 2: 添加软件源信息sudo yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo# Step 3: 更新并安装Docker-CEsudo yum makecache fastsudo yum -y install docker-ce# Step 4: 开启Docker服务sudo service docker start Docker在Windows/MacOS下的安装先看下官方对安装环境的要求： Docker for windows 在Windows上运行Docker,系统要求，Windows10 x64 支持Hyper-V Windows10以下的版本需要使用Docker Toolbox，Docker Toolbox适用于Windows7/8.1。 下面是官方的提供的下载地址：1http://get.daocloud.io/#install-docker-for-mac-windows 下载之后，傻瓜式安装即可。 如何验证是否安装成功？在命令行下输入docker version输出一堆版本信息即代表安装成功。 如下： Docker加速器配置Docker加速器可以提升在国内获取Docker官方镜像的速度，否则后面下载镜像的过程会很慢，甚至无法下载镜像。 配置阿里云加速器登录你的阿里云控制台，找到容器镜像服务 -&gt; 镜像加速器 -&gt; 复制加速器地址。 找不到的点这个： 1https://cr.console.aliyun.com/cn-hangzhou/mirrors Centos/Ubuntu 通过修改daemon配置文件/etc/docker/daemon.json来使用加速器。12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'&#123; "registry-mirrors": ["https://thd78540.mirror.aliyuncs.com"]&#125;EOFsudo systemctl daemon-reloadsudo systemctl restart docker 安装了Docker Toolbox的Windows/MacOS 创建一台安装有Docker环境的Linux虚拟机，指定机器名称为default，同时配置Docker加速器地址。 1docker-machine create --engine-registry-mirror=https://thd78540.mirror.aliyuncs.com -d virtualbox default 查看机器的环境配置，并配置到本地，并通过Docker客户端访问Docker服务。 123docker-machine env defaulteval "$(docker-machine env default)"docker info Docker for Windows 在系统右下角托盘图标内右键菜单选择 Settings，打开配置窗口后左侧导航菜单选择 Docker Daemon。编辑窗口内的JSON串，填写下方加速器地址：123&#123; "registry-mirrors": ["https://thd78540.mirror.aliyuncs.com"]&#125; 编辑完成后点击 Apply 保存按钮，等待Docker重启并应用配置的镜像加速器。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker数据卷管理]]></title>
    <url>%2F2019%2F03%2F23%2FDocker-shujvjuan%2F</url>
    <content type="text"><![CDATA[为什么使用数据卷方便宿主机直接访问容器中的文件，容器中的文件没有持久化，当容器删除后，文件数据也会随之消失，且没有使用数据卷的容器，其他容器也无法直接访问相互的文件。 数据卷的特点 数据卷存在于宿主机的文件系统中，独立于容器，和容器的生命周期是分离的。 数据卷可以目录也可以是文件，容器可以利用数据卷与宿主机进行数据共享，实现了容器间的数据共享和交换。 容器启动初始化时，如果容器使用的镜像包含了数据，这些数据会拷贝到数据卷中。 容器对数据卷的修改是实时进行的。 数据卷的变化不会影响镜像的更新。数据卷是独立于联合文件系统，镜像是基于联合文件系统。镜像与数据卷之间不会有相互影响。Docker数据卷的三种挂载方式bind mounts:将宿主机上的一个文件或目录被挂载到容器上 volumes:由Docker创建和管理。使用docker volume命令管理 tmpfs mounts:tmpfs是一种基于内存的临时文件系统，tmpfs mounts数据不会存储到磁盘上 bind mounts 方式挂载数据卷命令参数: docker run/create -v 具体用法: -v 宿主机文件或文件夹路径:容器中的文件或文件夹路径 –mount type=bind,src=宿主机文件或文件夹路径,dst=容器中的文件或文件夹路径 注意:使用方法二创建时，src后的文件夹或文件必须提前创建 volumes 方式挂载数据卷命令参数: docker run/create -v 具体用法: -v VOLUME-NAME:容器中的文件或文件夹 –mount type=volume,src=VOLUME-NAME,dst=容器中的文件或文件夹路径 volume对象管理命令:123456docker volume 命令管理volume数据卷对象docker volume create 创建数据卷对象docker volume inspect 查看数据卷详细信息docker volume ls 查看已创建的数据卷对象docker volume prune 删除未被使用的数据卷对象docker volume rm 删除一个或多个数据卷对象 tmpfs mount 方式挂载数据卷命令参数: docker run/create -v具体用法: –mount type=tmfps,dst=PATH 如何共享其他容器的数据卷？命令参数: docker run/create –volumes-from [容器] 命令格式:docker run/create –volumes-from CONTAINER 容器数据卷使用注意事项Docker的数据卷更多会是使用volumes方式来进行使用。 使用时需注意： 如果挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中。 如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉。 第一个规则可以帮助我们初始化数据卷中的内容。第二个规则可以保证挂载数据卷后的数据总是你期望的结果。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker镜像]]></title>
    <url>%2F2019%2F03%2F21%2FDocker-image%2F</url>
    <content type="text"><![CDATA[docker镜像是什么？镜像是一个Docker的可执行文件，其中包括运行应用程序所需的所有代码内容、依赖库、环境变量和配置文件等。 通过镜像可以创建一个或多个容器,用另一种便于理解的说法：docker中的镜像相当于我们日常生活中接触到的操作系统，容器则是基于操作系统上的程序。(以上为咸鱼一家之言，如若理解有误，可以略过) docker镜像管理我们先看一张镜像管理示意图： 下面咸鱼会对镜像管理的部分命令简要学习： 镜像搜索(docker search)在搭建项目环境的时候我们可以通过检索docker官方为我们提供的镜像云仓库，查看是否已经有别人构建好的镜像，可以拉取下来直接使用。 命令格式：docker search [参数] 搜索项常用参数：12-f 根据提供的格式筛选结果--limit int 展示最大的结果数，默认为25 镜像下载(docker pull)根据项目需求挑选心仪的镜像，我们就可以把它下载下来。 命令格式：docker pull [参数] 镜像名称[:版本号] 注意：这里版本号没有指定，默认下载最新版本的镜像，例如 Ubuntu 最新的镜像版本为18.04，你想下载16.04的版本就可以使用 docker pull ubuntu:16.04下载指定版本 镜像查看(docker images / doker image ls)上一步我们下载了心仪的镜像，如何确定镜像下载成功了呢？我们可以使用docker images / doker image ls查看本地镜像 命令格式：12docker images [参数] [仓库名[版本号]]docker image ls [参数] [仓库名[版本号]] 常用参数：12-a 展示所有的镜像-q 只展示镜像ID 镜像删除(docker rmi / docker image rm)命令格式：1docker rm [参数] 镜像名[镜像名..] -- 可以同时删除一个或多个本地镜像 常用参数：1-f 强制删除 镜像保存备份(docket save)docker之所以被人喜爱，一大原因就是因为它的可移植性，我们可以很方便的把我们构建好的镜像打包放到任何已经安装了docker环境的机器上运行。 命令格式： docker save [参数] 镜像名[镜像名..] | 镜像ID[镜像ID..] -- 可以打包一个或多个镜像保存成本地tar文件 常用参数：1- o 指定写入的文件名和路径，默认为STDOUT 注意：导出时建议指定镜像的名称最好不要使用镜像ID，否则备份导入时镜像名称与版本号会显示none 镜像备份导入(docker load)命令格式：docker load [参数] 常用参数：1- i 指定要导入的文件默认为STDIN 镜像重命名(docker rename)如果很不幸，你在镜像导出时选择的是镜像ID导出，那么你可以使用rename重命名那些信息显示不全的镜像 命令格式：docker rename [源镜像] [新镜像] 镜像历史信息(docker history)这个命令在之后使用Dockerfile创建docker镜像时会经常用到，我们可以使用这个命令查看镜像在之前的更改操作。 命令格式：docker history [参数] 镜像 镜像详细信息(docker image inspect)命令格式：12docker image inspect [参数] 镜像 [镜像...]docker inspect [参数] 镜像 [镜像...] 命令参数：1-f 利用特定Go语言的format格式输出结果 注意：我们不带参数的使用docker inspect 会打印长串的信息(标准的json格式)，所以推荐使用 -f 参数查看指定的信息 -f 参数的简单使用举例： 总结在学习docker的过程中咸鱼也做了一些笔记，以便之后复习，当然最好的学习还是要自己动手。 咸鱼在这里总结的是常用的基础命令，用于python开发的环境构建是完全够用的，如若需要更多详细内容建议参考官方文档。1https://docs.docker.com/ 在拉取镜像的过程中不知道有哪些镜像TAG可以拉取的朋友，咸鱼建议可以参考官方的github仓库，这里列举了最新最全的镜像TAG方便开发者查阅。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器]]></title>
    <url>%2F2019%2F03%2F19%2FDocker-rong-qi%2F</url>
    <content type="text"><![CDATA[什么是容器？容器（Container）：容器是一种轻量级、可移植、并将应用程序进行的打包的技术，使应用程序可以在几乎任何地方以相同的方式运行，Docker将镜像文件运行起来后，产生的对象就是容器。容器相当于是镜像运行起来的一个实例且容器具备一定的生命周期。 Docker容器和虚拟机的区别相同点： 容器和虚拟机一样，都会对物理硬件资源进行共享使用。 容器和虚拟机的生命周期比较相似（创建、运行、暂停、关闭等等）。 容器中或虚拟机中都可以安装各种应用，如redis、mysql、nginx等。也就是说，在容器中的操作，如同在一个虚拟机(操作系统)中操作一样。 同虚拟机一样，容器创建后，会存储在宿主机上：linux上位于/var/lib/docker/containers下 不同点： 虚拟机的创建、启动和关闭都是基于一个完整的操作系统。一个虚拟机就是一个完整的操作系统。而容器直接运行在宿主机的内核上，其本质上以一系列进程的结合。 容器是轻量级的，虚拟机是重量级的。首先容器不需要额外的资源来管理(不需要Hypervisor、Guest OS)，虚拟机额外更多的性能消耗；其次创建、启动或关闭容器，如同创建、启动或者关闭进程那么轻松，而创建、启动、关闭一个操作系统就没那么方便了。也因此，意味着在给定的硬件上能运行更多数量的容器，甚至可以直接把Docker运行在虚拟机上。 容器的生命周期管理先来看一张容器生命周期示意图： 下面咸鱼会对容器管理的部分命令简要学习： 容器创建(docker create)命令格式：docker create [参数] 镜像名称[容器执行命令][执行命令时需要提供的参数] 常用参数：123-t 分配一个虚拟终端-i 提供一个模拟输入，不提供则无法输入默认命令--name 为创建好的容器提供一个容器名，不提供的话随机分配一个 容器启动(docker start)启动一个或多个容器。 命令格式：docker start [参数] 容器[容器..] 常用参数：12-a 将当前的输入/输出连接到容器-i 将当前的输入连接到容器上 容器创建并启动(docker run)命令格式：docker run [参数] 镜像 [容器执行命令] [执行命令提供的参数] 常用参数：1234-t 分配一个虚拟终端-i 保持输入打开-d 容器后台运行，并打印容器id--rm 容器结束后自动删除容器 注意：因为我们学习docker主要还是为我们之后基于docker的Python开发服务的，所以关于docker run 以及 docker create/start 的一些参数的具体的区别就不在文章里赘述，有疑问的朋友可以参考官方文档，当然也欢迎和咸鱼在留言区讨论，大家在这里简单记两个公式就好： 12docker run = doker create + docker start -adocker run -d = docker create + docker start 这里咸鱼推荐大家使用docker run -dti来启动所需容器。 容器暂停(docker pause/unpause)docker pause 暂停一个或多个容器 docker unpause 启动一个或多个暂停中的容器 命令格式：docker pause/unpause 容器[容器..] 容器关闭(docker stop)关闭一个或多个容器。 docker stop 关闭一个或多个容器 命令格式：docker stop 容器[容器..]常用参数：1-t 关闭前的等待时间，默认是10秒 容器终止(docker kill)强制并立即关闭一个或多个处于暂停或运行状态的容器。 命令格式：docker kill [参数] 容器[容器..] 常用参数：-s 指定发给容器的关闭信号，默认为“kill” 容器重启(docker restart)重启一个或多个处于运行状态、暂停状态、关闭状态或创建状态的容器。 命令格式：docker restart [参数] 容器[容器..] 常用参数：1-t 关闭前的等待时间，默认是10秒，实则是关闭前的等待时间 容器删除(docker container rm / docker rm )命令格式：docker container rm [参数] 容器[容器..] 常用参数：12-f 强制删除-v 删除容器的同时删除容器的数据卷 docker kill 和 docker stop 区别在上面关于docker容器生命周期管理中stop 和 kill 都是关闭容器，但是其中的kill是怎么实现强制杀死运行中的容器的呢？ 这里需要说明下关linux下关于终止进程的信号:SIGTERM 和 SIGKILL SIGKILL信号：无条件终止进程信号。进程接收到该信号会立即终止，不进行清理和暂存工作。该信号不能被忽略、处理和阻塞，它向系统管理员提供了可以杀死任何进程的方法。 SIGTERM信号：程序终结信号，可以由kill命令产生。与SIGKILL不同的是，SIGTERM信号可以被阻塞和终止，以便程序在退出前可以保存工作或清理临时文件等。 docker stop 会先发出SIGTERM信号给进程，告诉进程即将会被关闭。在-t指定的等待时间过了之后，将会立即发出SIGKILL信号，直接关闭容器。 docker kill 直接发出SIGKILL信号关闭容器。但也可以通过-s参数修改发出的信号。 docker restart 中同样可以设置 -t 等待时间，当等待时间过后会立刻发送SIGKILL信号，直接关闭容器。 因此会发现在docker stop的等待过程中，如果终止docker stop的执行，容器最终没有被关闭。而docker kill几乎是立刻发生，无法撤销。 总结关于docker容器的生命周期管理，咸鱼踩坑之后认为需要加深理解的部分是关于docker run/start/create 以及 docker kill/stop/restart这两大部分。 关于docker kill 和 docker stop 区别希望大家能着重看下上面docker kill/stop区别部分，至于docker run/start/create最好能够动手敲一下感受下不同参数创建的后容器区别。 以上就是咸鱼关于容器生命周期管理部分的踩坑总结。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker网络]]></title>
    <url>%2F2019%2F03%2F19%2FDocker-network%2F</url>
    <content type="text"><![CDATA[为什么需要容器的网络管理？容器的网络默认与宿主机、与其他容器相互隔离，且容器中可以运行一些网络应用，比如nginx、web应用、数据库等，如果需要让外部也可以访问这些容器中运行的网络应用，那么就需要配置网络来实现。 同样的，不同需求下，容器与宿主机的通信有不同的业务状态这时候就需要容器网络管理以达成管理不同业务下相关的网络配置。 Docker中的网络驱动模式有那些？bridge network(网桥)模式:默认的网络模式，类似虚拟机的nat模式 host network(主机)模式:容器与宿主机之间的网络无隔离，即容器直接使用宿主机网络 none network模式:容器禁用所有网络 overlay network(覆盖)模式:利用vxlan实现的bridge模式 macvlan network模式:容器具备MAC地址，使其在外部看来是一台真实的网络设备 Docker网络管理命令浅析查看网络(docker network ls)命令格式: docker network ls [参数] 常用参数: 1-q 只显示网络对象的ID 注意: docker安装之后，会自动创建bridge、host、none三种网络驱动。 创建网络(docker network create)命令格式:docker network create [参数] 网络 常用参数:1234-d 指定网络的驱动，不指定默认为bridge-- subnet 指定子网网段(192.168.0.0/16)-- ip-range 指定容器的IP范围-- gateway 子网的网关 注意: 创建网络部分参数，涉及到网络工程师相关的知识，这里不做详细介绍有需要可以参考「计算机网络」这本书，内有详解。 host、none模式的网络只能存在一个，再次创建会报错。 overlay网络创建依赖于docker swarm(集群负载均衡)服务 网络删除(docker network rm)删除一个或多个网络 命令格式:docker network rm 网络[网络..] 查看网络详细信息(docker network inspect)命令格式: docker network inspect [参数] 网络 docker inspect [参数] 网络 常用参数:1-f 根据format输出结果 使用网络(docker run)命令格式:docker run/create –network 网络 注意:默认情况下，docker创建或启动容器时，会默认使用名为bridge的网络 网络连接与断开(docker network connect /disconnect)命令格式: docker network connect/disconnect [参数] 网络 容器 常用参数:1-f 强制断开连接 Docker网络模式简介bridge网络模式 bridge网络模式的特点： 宿主机上需要单独的bridge网卡，如默认docker默认创建的docker0。 容器之间、容器与主机之间的网络通信，是借助为每一个容器生成的一对veth pair虚拟网络设备对，进行通信的。一个在容器上，另一个在宿主机上。（这是网桥） 每创建一个基于bridge网络的容器，都会自动在宿主机上创建一个veth**虚拟网络设备。外部无法直接访问容器。需要建立端口映射才能访问。(可以理解为网卡) 容器借由veth虚拟设备通过如docker0这种bridge网络设备进行通信。 每一容器具有单独的IP bridge网络模式下宿主机与容器服务使用的端口可以重复 bridge网络模式下的端口映射根据上面关于bridge的特点我们可以知道访问bridge网络模式的设备，需要端口映射。 端口映射的命令格式:docker run/create -P/-p 命令参数:12-P 将容器内部所有暴露端口进行随机映射-p 手动指定端口映射 (-p [宿主机IP] : [宿主机端口] : 容器端口) 关于-p参数的举例:12-p ::80 将容器的80端口随机映射到宿主机的随机IP上-p :8000:6379 将容器的6379端口映射到宿主机的任意IP的8000端口上 host网络模式host网络模式的特点: 容器完全共享宿主机的网络，网络没有隔离。宿主机的网络就是容器的网络。 容器、主机上的应用所使用的端口不能重复。 外部可以直接访问容器，不需要端口映射 容器IP就是宿主机的IP 除了普通的host网络模式外，同样还有特殊版本的host网络模式(container网络模式)。 container网络模式的特点: 其实就是容器共享其他容器的网络,相当于该容器,在网络层面上，将其他容器作为“主机”。它们之间的网络没有隔离。 container网络模式的使用: docker run/create –network container:容器 none、overlay与macvlan网络模式none网络模式的特点: 容器上没有网络，也无需任何网络设备 如果需要使用网络，需要用户自行安装与配置 overlay网络模式的特点: overlay网络模式实现方案有很多种，在Docker自身集成了一种，基于VXLAN隧道技术实现 overlay网络主要用于实现跨主机容器之间的通信 macvlan网络模式的特点: macvlan的主要特点就是通信直接基于mac地址进行转发 在macvlan中宿主机担任的角色是一台二层交换机，docker会维护一张mac地址表，当宿主机收到数据包时，直接根据mac地址找到对应的容器 而在容器内部互相通信的时候，容器直接使用IP互通，所以每个容器对于外面的网络都是一台真实的网络设备 总结这次文章主要复习了docker的网络管理的内容，其中host,bridge,container是主要的复习内容，且在三种网络模式中建议优先选用host网络模式，其使用性能最优。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker容器与镜像的关系]]></title>
    <url>%2F2019%2F03%2F18%2FDocker-rongqi-and-image%2F</url>
    <content type="text"><![CDATA[容器与镜像的关系在实际操作中，我们配置好需要的容器之后可以将它转化为镜像提交到仓库，以便之后使用。 先看一下容器与镜像的转换关系图: 容器提交(docker commit)根据容器生成一个新的镜像 命令格式:docker commit [参数] 容器[版本] 常用参数:1234-a 添加作者-c 为创建的镜像加入Dockerfile命令-m 类似git commit -m-p 提交时暂停容器 容器导出(docker export)将当前容器导出为TAR文件 命令格式:docker export [参数] 容器 常用参数:-o 指定写入的文件 容器导入(docker import)将之前导出的容器文件导入并创建为一个镜像 命令格式:docker import [参数] 文件|链接|[版本信息] 常用参数:12-m 导入时添加提交信息-c 为创建的镜像加入Dockerfile命令 docker import 与 docker commit 的区别当你使用docker import 时，导入的镜像是一个全新镜像，是无法使用docker history查看到镜像的历史信息，使用docker commit 导入时，生成的镜像可以使用docker history查看到镜像的历史信息的。 深入理解Docker容器和镜像在docker文章的第一篇，我和大家简单的比喻了镜像和容器的关系，有位读者就这个问题和我讨论了一番，这里就这个问题做一个简单的描述，深入理解下镜像和容器。 在镜像层面上: 当我们查看镜像详细信息时，可以看到以下信息: 这里的Layers指的就是一个个只读的文件系统，镜像就是由这样一个个文件系统组成的，我们把镜像运行起来就会成为一个个容器，当我们在容器中做了修改并commit为镜像后，就会不断在原有的Layers层上新增一个Layer层，就像下面看到的这样。 所以当我们commit后，我们看到的镜像就是最上层的Layer。 在容器层面上: 当我们使用docker create [image id]用指定镜像创建容器时，可以理解为在镜像的最上层创建了一个可读可写的Layer，当我们修改完，使用commit提交后，这个容器的可读可写的Layer层就会转化为镜像的只读的Layer层。 而当我们使用docker inspect查看容器的时候只能查看到最上层容器的信息而无法查看到像镜像那样的Layers，这是因为在容器的视角中Layer层是这样的: 当容器中有进程运行时: 容器内部是这样的: 总结关于深入理解镜像和容器的部分，如若有理解错误的地方希望大家指正，这一部分有些抽象大家可以结合实际操作加深理解，也欢迎各路大佬一起交流探讨。]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔试题(二)]]></title>
    <url>%2F2019%2F02%2F09%2Fexam1%2F</url>
    <content type="text"><![CDATA[本系列文章素材，题目来源于真实企业笔试题,具体哪来的？平时多水群就好 第一题下图中的打印结果分别是什么？ 第二题Python中静态方法、类方法、成员函数作用是什么？ 第三题说说对缺省参数的理解？* args是什么？** kwargs是什么？ 答案第一题前三个是True,最后一个会报错。 具前三个为什么输出True,不了解的可以参考下图: 最后一个为什么报错，我们可以先看下issubclass的用法: 123456789101112issubclass方法用于判断参数 class 是否是类型参数 classinfo 的子类issubclass(class, classinfo)参数class -- 类。classinfo -- 类。返回值如果 class 是 classinfo 的子类返回 True，否则返回 False。以上内容参考:菜鸟教程 - http://www.runoob.com/python/python-func-issubclass.html 而在题目中b并不是一个类,所以报错。 第二题我们先理清楚静态函数、类函数分别是什么？ 静态方法是一种普通函数，就位于类定义的命名空间中，它不会对任何实例类型进行操作。使用装饰器@staticmethod定义静态方法。类对象和实例都可以调用静态方法。 类方法是将类本身作为对象进行操作的方法。类方法使用@classmethod装饰器定义，其第一个参数是类，约定写为cls。 代码实例可以参考:https://blog.csdn.net/baidu_20351223/article/details/80160868 第三题缺省参数是指在调用函数的时候没有传入参数的情况下，调用默认的参数，在调用函数的同时赋值时，所传入的参数会替代默认参数。 *args 是不定长参数，可以表示输入参数是不确定的，可以任意多个 ** kwargs 是关键词参数，赋值的时候是以键 = 值 的方式，参数是可以任意多对。 以上两个的使用是在定义函数的时候不确定会传入多少参数时使用。 小结有读者朋友在后台留言，面试题怎么这么简单，说实话，我认真翻了10几份面试题，每一份的题量都不多，其中大部分都是这类基础题，少部分是专业导向比较强的题目和面试的岗位相关，剩下一部分就是算法与数据结构相关的题目。所以咸鱼觉的打好基础才是通关笔试的第一步，当然像宇宙条那种通篇算法题的笔试题也有，这里就刨除这些另谈了。]]></content>
      <categories>
        <category>Python笔试题</category>
      </categories>
      <tags>
        <tag>Python笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python笔试题(一)]]></title>
    <url>%2F2019%2F02%2F09%2Fexma2%2F</url>
    <content type="text"><![CDATA[题目 以下两段代码，运行结束后的结果是什么？是否相同？原因是什么？ 4G内存怎么读取一个5G的数据？ 浅述深浅拷贝 答案1.不相同 第一段结果为:1[&#123;'num':0&#125;,&#123;'num':1&#125;,&#123;'num':2&#125;,&#123;'num':3&#125;,&#123;'num':4&#125;,&#123;'num':5&#125;,&#123;'num':6&#125;,&#123;'num':7&#125;,&#123;'num':8&#125;,&#123;'num':9&#125;] 第二段结果为:1[&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;,&#123;'num':9&#125;] 字典是可变类型，这里l.append(a)相当于执行了浅拷贝,每改变一次a中num的值,所有a的值都将改变 2.实现方法有两种。 实现方法一: 方法一的实现思路是通过生成器,一次读取少量数据,标准答案是根据文件实现，这里给出的例子是咸鱼学习bobby老师的课程时记录的例子,不知道如何使用生成器完成这个需求的朋友可以参考: 实现方法二: 在linux系统下使用split可以分割文件,对于多行文件可以使用按行分割的方式,对于单行的大文件可以采用按文件大小分割。 按文件行数分割:split -l 300 large_file.log smallfile_prefix 按文件大小分割:split -b 10m large_file.log smallfile_prefix 之后再按文件读取即可。 3.这个之前的文章讲过,直接点下面的直通车即可。]]></content>
      <categories>
        <category>Python笔试题</category>
      </categories>
      <tags>
        <tag>Python笔试题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析思维]]></title>
    <url>%2F2019%2F02%2F09%2Fshujv-fenxi%2F</url>
    <content type="text"><![CDATA[数据分析的三种核心思维 结构化 公式化 业务化 结构化 单兵作战 思维导图 团体作战 查看资料及背景将结论列成一张表/卡片 把表上的结论，根据主题分类 将同一分类的结论按顺序区分 讨论同一级别的共通结论，将其结论放入上一段位置 公式化 加(不同业务的叠加) 减(业务间的逻辑关系) 乘除(各种比例或比率) 业务化思维方式 象限法 核心：象限法是一种策略驱动的思维 优点：直观清晰对数据进行人工的划分,划分结果可以直接应用与策略 适用范围：产品分析，客户管理等 多维法 核心：多维法是一种精细驱动的思维 优点：处理大数据量，维度丰富且复杂的数据有较好的效果，但是维度过多会消耗不少的时间 适用范围:只要数据齐全且丰富即可适用(注意辛普森悖论) 假设法 核心：假设是一种启发思考的思维 优点：当没有直观数据戒者线索能分析时，以假设先行的方式进行推断，这是一个论证的过程 应用：它更多是一种思考方弅，假设—验证—判断]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(六)]]></title>
    <url>%2F2019%2F01%2F30%2Fpandas6%2F</url>
    <content type="text"><![CDATA[时间序列的索引与切片索引时间序列的索引方法同样是适用于Dataframe，而且在时间序列中由于按照时间先后排序，故不用考虑顺序问题。 基本位置索引,使用的方法和列表类似：1234567891011121314151617181920from datetime import datetimerng = pd.date_range('2017/1','2017/3')ts = pd.Series(np.random.rand(len(rng)), index = rng)print(ts.head())print(ts[0])print(ts[:2])&gt;&gt;&gt;2017-01-01 0.1077362017-01-02 0.8879812017-01-03 0.7128622017-01-04 0.9200212017-01-05 0.317863Freq: D, dtype: float640.1077359450272017-01-01 0.1077362017-01-02 0.887981Freq: D, dtype: float64 除了基本位置索引之外还有时间序列标签索引：12345678910111213from datetime import datetimerng = pd.date_range('2017/1','2017/3')ts = pd.Series(np.random.rand(len(rng)), index = rng)print(ts['2017/1/2'])print(ts['20170103'])print(ts['1/10/2017'])print(ts[datetime(2017,1,20)])&gt;&gt;&gt;0.8879807578120.7128617789660.7883366749480.93070380011 切片切片的使用操作在上面索引部分的基本位置索引中有提到和Series按照index索引原理一样，也是末端包含。12345678910111213141516171819202122232425262728rng = pd.date_range('2017/1','2017/3',freq = '12H')ts = pd.Series(np.random.rand(len(rng)), index = rng)print(ts['2017/1/5':'2017/1/10'])&gt;&gt;&gt;2017-01-05 00:00:00 0.4620852017-01-05 12:00:00 0.7786372017-01-06 00:00:00 0.3563062017-01-06 12:00:00 0.6679642017-01-07 00:00:00 0.2468572017-01-07 12:00:00 0.3869562017-01-08 00:00:00 0.3282032017-01-08 12:00:00 0.2608532017-01-09 00:00:00 0.2249202017-01-09 12:00:00 0.3974572017-01-10 00:00:00 0.1587292017-01-10 12:00:00 0.501266Freq: 12H, dtype: float64# 在这里我们可以传入月份可以直接获取整个月份的切片print(ts['2017/2'].head())&gt;&gt;&gt;2017-02-01 00:00:00 0.2439322017-02-01 12:00:00 0.2208302017-02-02 00:00:00 0.8961072017-02-02 12:00:00 0.4765842017-02-03 00:00:00 0.515817Freq: 12H, dtype: float64 重复索引的时间序列1234567891011121314dates = pd.DatetimeIndex(['1/1/2015','1/2/2015','1/3/2015','1/4/2015','1/1/2015','1/2/2015'])ts = pd.Series(np.random.rand(6), index = dates)print(ts)# 我们可以通过is_unique检查值或index是否重复print(ts.is_unique,ts.index.is_unique)&gt;&gt;&gt;2015-01-01 0.3002862015-01-02 0.6038652015-01-03 0.0179492015-01-04 0.0266212015-01-01 0.7914412015-01-02 0.526622dtype: float64True False 按照上面的结果，可以看出在上面的时间序列中，出现了index(ts.index.is_unique)重复但值(ts.is_unique)不重复的情况。 我们可以通过时间序列把重复索引对应的值取平均值来解决索引重复的问题：12345678print(ts.groupby(level = 0).mean())# 通过groupby做分组，重复的值这里用平均值处理&gt;&gt;&gt;2015-01-01 0.5458632015-01-02 0.5652442015-01-03 0.0179492015-01-04 0.026621dtype: float64]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(五)]]></title>
    <url>%2F2019%2F01%2F23%2Fpandas5%2F</url>
    <content type="text"><![CDATA[Pandas时期 - Periodpd.Period()创建时期生成一个以2017-01开始，月为频率的时间构造器：1234p = pd.Period('2017', freq = 'M')print(p, type(p))&gt;&gt;&gt;2017-01 &lt;class 'pandas._period.Period'&gt; 我们可以通过加减整数，将周期整体移动：1234567p = pd.Period('2017', freq = 'M')print(p, type(p))print(p + 1)print(p - 2)&gt;&gt;&gt;2017-022016-11 pd.period_range()创建时期范围创建指定时期范围：1234567prng = pd.period_range('1/1/2011', '1/1/2012', freq='M')print(prng,type(prng))&gt;&gt;&gt;PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01'], dtype='int64', freq='M') &lt;class 'pandas.tseries.period.PeriodIndex'&gt; 结合上面的时期序列，创建时间序列：12345678910111213141516171819202122ts = pd.Series(np.random.rand(len(prng)), index = prng)print(ts,type(ts))print(ts.index)&gt;&gt;&gt;2011-01 0.3425712011-02 0.8261512011-03 0.3705052011-04 0.1371512011-05 0.6799762011-06 0.2659282011-07 0.4165022011-08 0.8740782011-09 0.1128012011-10 0.1125042011-11 0.4484082011-12 0.8510462012-01 0.370605Freq: M, dtype: float64 &lt;class 'pandas.core.series.Series'&gt;PeriodIndex(['2011-01', '2011-02', '2011-03', '2011-04', '2011-05', '2011-06', '2011-07', '2011-08', '2011-09', '2011-10', '2011-11', '2011-12', '2012-01'], dtype='int64', freq='M') pd.period - asfreq：频率转换通过.asfreq(freq, method=None, how=None)方法可以将之前生成的频率转换成别的频率12345678p = pd.Period('2017','A-DEC')print(p)print(p.asfreq('M', how = 'start')) # 也可写 how = 's'print(p.asfreq('D', how = 'end')) # 也可写 how = 'e'&gt;&gt;&gt;20172017-012017-12-31 asfreq也可以转换TIMESeries的index:12345prng = pd.period_range('2017','2018',freq = 'M')ts1 = pd.Series(np.random.rand(len(prng)), index = prng)ts2 = pd.Series(np.random.rand(len(prng)), index = prng.asfreq('D', how = 'start'))print(ts1.head(),len(ts1))print(ts2.head(),len(ts2)) 时间戳与时期之间的转换使用pd.to_period()、pd.to_timestamp()可以实现时间戳与时期之间的转换。12345678910111213141516171819202122232425262728293031323334353637rng = pd.date_range('2017/1/1', periods = 10, freq = 'M')prng = pd.period_range('2017','2018', freq = 'M')ts1 = pd.Series(np.random.rand(len(rng)), index = rng)print(ts1.head())print(ts1.to_period().head())# 每月最后一日，转化为每月ts2 = pd.Series(np.random.rand(len(prng)), index = prng)print(ts2.head())print(ts2.to_timestamp().head())# 每月，转化为每月第一天&gt;&gt;&gt;2017-01-31 0.1252882017-02-28 0.4971742017-03-31 0.5731142017-04-30 0.6656652017-05-31 0.263561Freq: M, dtype: float642017-01 0.1252882017-02 0.4971742017-03 0.5731142017-04 0.6656652017-05 0.263561Freq: M, dtype: float642017-01 0.7486612017-02 0.0958912017-03 0.2803412017-04 0.5698132017-05 0.067677Freq: M, dtype: float642017-01-01 0.7486612017-02-01 0.0958912017-03-01 0.2803412017-04-01 0.5698132017-05-01 0.067677Freq: MS, dtype: float64]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(四)]]></title>
    <url>%2F2019%2F01%2F22%2Fpandas4%2F</url>
    <content type="text"><![CDATA[Pandas时间戳索引-DatetimeIndexpd.DatetimeIndex()与TimeSeries时间序列pd.DatetimeIndex()可以直接生成时间戳索引，支持使用str、datetime.datetime。单个时间戳的类型为Timestamp，多个时间戳的类型为DatetimeIndex，示例如下：12345678rng = pd.DatetimeIndex(['12/1/2017','12/2/2017','12/3/2017','12/4/2017','12/5/2017'])print(rng,type(rng))print(rng[0],type(rng[0]))&gt;&gt;&gt;DatetimeIndex(['2017-12-01', '2017-12-02', '2017-12-03', '2017-12-04', '2017-12-05'], dtype='datetime64[ns]', freq=None) &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;2017-12-01 00:00:00 &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; 什么是TimeSeries时间序列？以DatetimeIndex为index的Series，为TimeSries时间序列举个栗子：12345678910111213st = pd.Series(np.random.rand(len(rng)), index = rng)print(st,type(st))print(st.index)&gt;&gt;&gt;2017-12-01 0.0819202017-12-02 0.9217812017-12-03 0.4897792017-12-04 0.2576322017-12-05 0.805373dtype: float64 &lt;class 'pandas.core.series.Series'&gt;DatetimeIndex(['2017-12-01', '2017-12-02', '2017-12-03', '2017-12-04', '2017-12-05'], dtype='datetime64[ns]', freq=None) pd.date_range()-生成日期范围pd.date_range()生成日期范围有两种生成方式(默认频率是day)： 起始时间(start) + 结束时间(end) 起始时间(start)/结束时间(end) + 偏移量(periods) 举个栗子：1234567891011121314151617181920212223date1 = pd.date_range('2017/1/1','2017/10/1',normalize=True)print(date1)date2 = pd.date_range(start = '1/1/2017', periods = 10)print(date2)date3 = pd.date_range(end = '1/30/2017 15:00:00', periods = 10,normalize=True) # 增加了时、分、秒print(date3)&gt;&gt;&gt;DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10', ... '2017-09-22', '2017-09-23', '2017-09-24', '2017-09-25', '2017-09-26', '2017-09-27', '2017-09-28', '2017-09-29', '2017-09-30', '2017-10-01'], dtype='datetime64[ns]', length=274, freq='D')DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10'], dtype='datetime64[ns]', freq='D')DatetimeIndex(['2017-01-21', '2017-01-22', '2017-01-23', '2017-01-24', '2017-01-25', '2017-01-26', '2017-01-27', '2017-01-28', '2017-01-29', '2017-01-30'], dtype='datetime64[ns]', freq='D') 1pd.date_range(start=None, end=None, periods=None, freq='D', tz=None, normalize=False, name=None, closed=None, **kwargs) 其中常用参数含义如下： start：开始时间 end：结束时间 periods：偏移量 freq：频率，默认天，pd.date_range()默认频率为日历日，pd.bdate_range()默认频率为工作日 tz：时区 normalize：时间参数值正则化到午夜时间戳 closed：默认为None的情况下，左闭右闭，left则左闭右开，right则左开右闭 举个栗子对normalize参数进行实际运用：1234567rng4 = pd.date_range(start = '1/1/2017 15:30', periods = 10, name = 'hello world!', normalize = True)print(rng4)&gt;&gt;&gt;DatetimeIndex(['2017-01-01', '2017-01-02', '2017-01-03', '2017-01-04', '2017-01-05', '2017-01-06', '2017-01-07', '2017-01-08', '2017-01-09', '2017-01-10'], dtype='datetime64[ns]', name='hello world!', freq='D') freq的使用(1) - 固定频率时间序列的生成基础使用如下：1234567print(pd.date_range('2017/1/1','2017/1/4')) # 默认freq = 'D'：每日历日print(pd.date_range('2017/1/1','2017/1/4', freq = 'B')) # B：每工作日print(pd.date_range('2017/1/1','2017/1/2', freq = 'H')) # H：每小时print(pd.date_range('2017/1/1 12:00','2017/1/1 12:10', freq = 'T')) # T/MIN：每分print(pd.date_range('2017/1/1 12:00:00','2017/1/1 12:00:10', freq = 'S')) # S：每秒print(pd.date_range('2017/1/1 12:00:00','2017/1/1 12:00:10', freq = 'L')) # L：每毫秒（千分之一秒）print(pd.date_range('2017/1/1 12:00:00','2017/1/1 12:00:10', freq = 'U')) # U：每微秒（百万分之一秒） 进阶使用如下：123456print(pd.date_range('2017/1/1','2017/2/1', freq = 'W-MON')) # W-MON：从指定星期几开始算起，每周# 星期几缩写：MON/TUE/WED/THU/FRI/SAT/SUNprint(pd.date_range('2017/1/1','2017/5/1', freq = 'WOM-2MON')) # WOM-2MON：每月的第几个星期几开始算，这里是每月第二个星期一 freq的使用(2) - 多样化生成需要的时间序列生成指定频率的日历日：123456789print(pd.date_range('2017','2018', freq = 'M')) print(pd.date_range('2017','2020', freq = 'Q-DEC')) print(pd.date_range('2017','2020', freq = 'A-DEC'))print('------')# M：每月最后一个日历日# Q-月：指定月为季度末，每个季度末最后一月的最后一个日历日# A-月：每年指定月份的最后一个日历日# 月缩写：JAN/FEB/MAR/APR/MAY/JUN/JUL/AUG/SEP/OCT/NOV/DEC# 所以Q-月只有三种情况：1-4-7-10,2-5-8-11,3-6-9-12 生成指定频率的工作日：1234567print(pd.date_range('2017','2018', freq = 'BM')) print(pd.date_range('2017','2020', freq = 'BQ-DEC')) print(pd.date_range('2017','2020', freq = 'BA-DEC'))print('------')# BM：每月最后一个工作日# BQ-月：指定月为季度末，每个季度末最后一月的最后一个工作日# BA-月：每年指定月份的最后一个工作日 生成指定规律的特殊时间：123456789101112131415print(pd.date_range('2017','2018', freq = 'MS')) print(pd.date_range('2017','2020', freq = 'QS-DEC')) print(pd.date_range('2017','2020', freq = 'AS-DEC'))print('------')# M：每月第一个日历日# QS-月：指定月为季度末，每个季度末最后一月的第一个日历日# AS-月：每年指定月份的第一个日历日print(pd.date_range('2017','2018', freq = 'BMS')) print(pd.date_range('2017','2020', freq = 'BQS-DEC')) print(pd.date_range('2017','2020', freq = 'BAS-DEC'))print('------')# BMS：每月第一个工作日# BQS-月：指定月为季度末，每个季度末最后一月的第一个工作日# BAS-月：每年指定月份的第一个工作日 freq的使用(3) - 复合频率的使用生成指定复合频率的时间序列：123print(pd.date_range('2017/1/1','2017/2/1', freq = '7D')) # 7天print(pd.date_range('2017/1/1','2017/1/2', freq = '2h30min')) # 2小时30分钟print(pd.date_range('2017','2018', freq = '2M')) # 每间隔2个月的第一个日历日 asfreq - 时期频率转换以天为间隔频率的时间序列如何修改为更小单位间隔的时间序列？123456ts = pd.Series(np.random.rand(4), index = pd.date_range('20170101','20170104'))print(ts)print(ts.asfreq('4H',method = 'ffill'))# 改变频率，这里是D改为4H# method：插值模式，None不插值，ffill用之前值填充，bfill用之后值填充 如何超前/滞后数据？下面栗子超前/滞后的数据移动的是数值：1234567891011121314151617181920212223ts = pd.Series(np.random.rand(4), index = pd.date_range('20170101','20170104'))print(ts)print(ts.shift(2))print(ts.shift(-2))print('------')# 正数：数值后移（滞后）；负数：数值前移（超前）&gt;&gt;&gt;2017-01-01 0.5750762017-01-02 0.5149812017-01-03 0.2215062017-01-04 0.410396Freq: D, dtype: float642017-01-01 NaN2017-01-02 NaN2017-01-03 0.5750762017-01-04 0.514981Freq: D, dtype: float642017-01-01 0.2215062017-01-02 0.4103962017-01-03 NaN2017-01-04 NaNFreq: D, dtype: float64 而加上freq偏移参数则偏移的的是前面的索引时间戳而不是数值：123print(ts.shift(2, freq = 'D'))print(ts.shift(2, freq = 'T'))# 加上freq参数：对时间戳进行位移，而不是对数值进行位移]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(三)]]></title>
    <url>%2F2019%2F01%2F20%2Fpandas3%2F</url>
    <content type="text"><![CDATA[时间模块datetime在学习时间序列之前我们需要先了解一下datetime模块的基本使用，datetime模块不是pandas库中所包含的。 但为了能更好的学习pandas中的时间序列，我们可以先学习datetime的基本使用以打好基础，datetime我们主要掌握一下几种方法：datetime.date(), datetime.datetime(), datetime.timedelta() datetime.date我们可以使用datetime.date.today方法返回当前时间，其数据类型为datetime.date12345import datetime today = datetime.date.today()print(today,type(today))&gt;&gt;&gt;2018-10-28 &lt;class 'datetime.date'&gt; 同时我们可以使用datetime.date(年，月，日)的方式输出我们想要的日期的datetime类型。123456import datetimet = datetime.date(2016,6,1)print(t)# (年，月，日) → 直接得到当时日期&gt;&gt;&gt;2016-06-01 datetime.datetime我们使用datetime.datetime.now方法返回当前时间，其数据类型为datetime.datetime12345import datetimenow = datetime.datetime.now()print(now,type(now))&gt;&gt;&gt;2018-10-28 17:34:07.467642 &lt;class 'datetime.datetime'&gt; 同样的我们可以使用datetime.datetime(年，月，日，时，分，秒)的形式输出我们想要的datetime.datetime类型的数据。12345t1 = datetime.datetime(2016,6,1)t2 = datetime.datetime(2014,1,1,12,44,33)print(t1,t2)&gt;&gt;&gt;2016-06-01 00:00:00 2014-01-01 12:44:33 datetime.timedeltadatetime.timedelta代表的数据是时间差数据类型。时间差主要用作时间的加减法，相当于可被识别的时间“差值”1234567891011# datetime.timedelta：时间差today = datetime.datetime.today() # datetime.datetime也有today()方法yestoday = today - datetime.timedelta(1) #print(today)print(yestoday)print(today - datetime.timedelta(7))&gt;&gt;&gt;2018-11-04 14:48:28.9671492018-11-03 14:48:28.9671492018-10-28 14:48:28.967149 日期解析方法：parser.parse除了上面转换方法之外，我们还需要其他关于字符串解析为时间格式的方法。 可以使用parse方法将字符串转换为datetime.datetime类型的数据。123456from dateutil.parser import parsedate = '12-21-2017't = parse(date)print(t,type(t))&gt;&gt;&gt;2017-12-21 00:00:00 &lt;class 'datetime.datetime'&gt; 同样的我们还可以将其他格式的字符串转化为datetime.datetime,但无法解析中文123456789101112print(parse('2000-1-1'),'\n', parse('5/1/2014'),'\n', parse('5/1/2014', dayfirst = True),'\n', # 国际通用格式中，日在月之前，可以通过dayfirst来设置 parse('22/1/2014'),'\n', parse('Jan 31, 1997 10:45 PM'))&gt;&gt;&gt;2000-01-01 00:00:00 2014-05-01 00:00:00 2014-01-05 00:00:00 2014-01-22 00:00:00 1997-01-31 22:45:00 Pandas时刻数据：Timestamp 时刻数据代表时间点，是pandas的数据类型 是将值与时间点相关联的最基本类型的时间序列数据。 Timestamp和上面的datetime基本没有区别，仅仅区别于一个是pandas模块，一个是datetime模块。 pandas.Timestapepandas.Timestape可以直接生成pandas的时刻数据1234567891011import numpy as npimport pandas as pddate1 = datetime.datetime(2016,12,1,12,45,30) # 创建一个datetime.datetimedate2 = '2017-12-21' # 创建一个字符串t1 = pd.Timestamp(date1)t2 = pd.Timestamp(date2)print(t1,type(t1))print(t2,type(t2))&gt;&gt;&gt;2016-12-01 12:45:30 &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;2017-12-21 00:00:00 &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; pandas.to_datetimepandas.to_datetime可以将如果是单个的时间数据，转换成pandas的时刻数据，数据类型为Timestamp，如果是多个的时间数据，将会转换为pandas的DatetimeIndex。 单个时间数据实例：1234567891011from datetime import datetimedate1 = datetime(2016,12,1,12,45,30)date2 = '2017-12-21't1 = pd.to_datetime(date1)t2 = pd.to_datetime(date2)print(t1,type(t1))print(t2,type(t2))&gt;&gt;&gt;2016-12-01 12:45:30 &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt;2017-12-21 00:00:00 &lt;class 'pandas._libs.tslibs.timestamps.Timestamp'&gt; 多个时间数据实例：12345lst_date = [ '2017-12-21', '2017-12-22', '2017-12-23']t3 = pd.to_datetime(lst_date)print(t3,type(t3))&gt;&gt;&gt;DatetimeIndex(['2017-12-21', '2017-12-22', '2017-12-23'], dtype='datetime64[ns]', freq=None) &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt; 当多个时间序列中有其他的数据格式时,我们可以使用error参数返回，errors = &#39;ignore&#39;:不可解析时返回原始输入，这里就是直接生成一般数组12345date3 = ['2017-2-1','2017-2-2','2017-2-3','hello world!','2017-2-5','2017-2-6']t3 = pd.to_datetime(date3, errors = 'ignore')print(t3,type(t3))&gt;&gt;&gt;['2017-2-1' '2017-2-2' '2017-2-3' 'hello world!' '2017-2-5' '2017-2-6'] &lt;class 'numpy.ndarray'&gt; 当errors = &#39;coerce&#39;时，不同数据类型的数据将会返回NaT，结果认为DatetimeIndex1234567date3 = ['2017-2-1','2017-2-2','2017-2-3','hello world!','2017-2-5','2017-2-6']t4 = pd.to_datetime(date3, errors = 'coerce')print(t4,type(t4))&gt;&gt;&gt;DatetimeIndex(['2017-02-01', '2017-02-02', '2017-02-03', 'NaT', '2017-02-05', '2017-02-06'], dtype='datetime64[ns]', freq=None) &lt;class 'pandas.core.indexes.datetimes.DatetimeIndex'&gt;]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(二)]]></title>
    <url>%2F2019%2F01%2F19%2Fpandas2%2F</url>
    <content type="text"><![CDATA[Pandas数据结构Series-基本技巧数据查看1234567#查看前五的数据s = pd.Series(np.random.rand(15))print(s.head()) #默认查看数据前五条# 查看后5条数据print(s.tail()) #默认查看数据的后五条# 查看前10条数据print(s.head(10)) 重新索引重新索引的作用是根据新的索引重新排序，若新的索引不存在则引入缺失值。123456789101112131415161718# .reindex将会根据索引重新排序，如果当前索引不存在，则引入缺失值s = pd.Series(np.random.rand(5),index=['a','b','c','d','e'])print(s)# 将索引值修改为['c','d','a','f']s1 = s.reindex(['c','d','a','f'])print(s1)&gt;&gt;&gt;a 0.972218b 0.820531c 0.940448d 0.009572e 0.462811dtype: float64c 0.940448d 0.009572a 0.972218f NaNdtype: float64 如果不想引入缺失值可以使用fill_value指定不存在的索引值为0或其他值123456789s2 = s.reindex(['c','d','a','f','aaaaa'], fill_value=0)print(s2)&gt;&gt;&gt;c 0.940448d 0.009572a 0.972218f 0.000000aaaaa 0.000000dtype: float64 数据对齐对齐两列数据，当数据索引不同时存在需要对齐的Series的时，数据值以缺失值填充。123456789101112# Series 和 ndarray 之间的主要区别是，Series 上的操作会根据标签自动对齐# index顺序不会影响数值计算，以标签来计算# 空值和任何值计算结果扔为空值s1 = pd.Series(np.random.rand(3),index=['jack','marry','tom'])s2 = pd.Series(np.random.rand(3),index=['wang','jack','marry'])print(s1+s2)&gt;&gt;&gt;jack 1.261341marry 0.806095tom NaNwang NaNdtype: float64 删除使用.drop删除元素的时候，默认返回的是一个副本(inplace=False)1234567891011121314151617181920212223242526272829s = pd.Series(np.random.rand(5), index = list('ngjur'))print(s)s1 = s.drop('n')s2 = s.drop(['g','j'])print(s1)print(s2)print(s)&gt;&gt;&gt;n 0.876587g 0.594053j 0.628232u 0.360634r 0.454483dtype: float64g 0.594053j 0.628232u 0.360634r 0.454483dtype: float64n 0.876587u 0.360634r 0.454483dtype: float64n 0.876587g 0.594053j 0.628232u 0.360634r 0.454483dtype: float64 添加方法一：直接通过下标索引/标签index添加值1234567891011121314151617181920212223242526272829303132333435s1 = pd.Series(np.random.rand(5))s2 = pd.Series(np.random.rand(5), index = list('ngjur'))print(s1)print(s2)s1[5] = 100s2['a'] = 100print(s1)print(s2)&gt;&gt;&gt;0 0.5164471 0.6993822 0.4695133 0.5898214 0.402188dtype: float64n 0.615641g 0.451192j 0.022328u 0.977568r 0.902041dtype: float640 0.5164471 0.6993822 0.4695133 0.5898214 0.4021885 100.000000dtype: float64n 0.615641g 0.451192j 0.022328u 0.977568r 0.902041a 100.000000dtype: float64 方法二：使用.append()方法添加，可以直接添加一个数组，且生成一个新的数组，不改变之前的数组。123456789101112131415161718192021222324s3 = s1.append(s2)print(s3)print(s1)&gt;&gt;&gt;0 0.2385411 0.8436712 0.4527393 0.3122124 0.8789045 100.000000n 0.135774g 0.530755j 0.886315u 0.512223r 0.551555a 100.000000dtype: float640 0.2385411 0.8436712 0.4527393 0.3122124 0.8789045 100.000000dtype: float64 修改series可以通过索引直接修改，类似序列1234567891011121314s = pd.Series(np.random.rand(3), index = ['a','b','c'])print(s)s['a'] = 100s[['b','c']] = 200print(s)&gt;&gt;&gt;a 0.873604b 0.244707c 0.888685dtype: float64a 100.0b 200.0c 200.0dtype: float64]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas基础(一)]]></title>
    <url>%2F2019%2F01%2F09%2Fpandas1%2F</url>
    <content type="text"><![CDATA[Pandas是什么？Pandas是数据分析的核心工具包，基于Numpy创建，为数据分析而存在。 一维数组Series + 二维数组Dataframe 可直接读取数据并做处理(高效简单) 兼容各种数据库 支持各种分析方法 Pandas基本数据结构-Series的基本概念先举个栗子:123456789101112ar = np.random.rand(5)s = pd.Series(ar)print(ar)print(s,type(s))&gt;&gt;&gt;[0.1206055 0.83147658 0.88649587 0.2162775 0.31466148]0 0.1206051 0.8314772 0.8864963 0.2162774 0.314661dtype: float64 &lt;class 'pandas.core.series.Series'&gt; 在这里可以看到这里的Series相比与之前学习的ndarray是一个自带索引index的数组 = 一维的数组 + 对应的索引，当pd.Series单单只看values时就是一个ndarray。 在具体操作方面，Series和ndarray基本相似，包括索引切片的操作差别并不大。 Pandas基本数据结构-Series的创建方法字典创建Series1234# 字典创建Seriesdic = &#123;'a':1, 'b':2, 'c':3&#125;s = pd.Series(dic)print(s) 数组创建Series12345# 数组创建Seriesarr = np.random.rand(5)*200s = pd.Series(arr)print(arr)print(s,type(s)) Series的参数设置我们可以通过指定Series的index以及dtype参数创建符合我们要求的Series。123456# Series的参数设置s = pd.Series(arr,index=list('abcde'),dtype=np.int,name=test)print(s)# index参数：设置index，长度保持一致# dtype参数：设置数值类型# 那么参数：设置名称 通过标量创建Series12s = pd.Series(arr,index=np.arange(5))print(s) Pandas基本数据结构-Series的索引位置下标索引位置下标从0开始，索引结果为numpy.float格式并且可以通过float()格式转换为float格式，且位置下标索引是没有负数的。12345678910111213s = pd.Series(np.random.rand(5))print(s)print(s[0],type(s[0]),s[0].dtype)print(float(s[0]),type(float(s[0])))&gt;&gt;&gt;0 0.4953611 0.1521952 0.2175913 0.7483944 0.093389dtype: float640.49536125725281055 &lt;class 'numpy.float64'&gt; float640.49536125725281055 &lt;class 'float'&gt; 标签索引123456789101112131415161718192021# 标签索引s = pd.Series(np.random.rand(5), index = ['a','b','c','d','e'])print(s)print(s['a'],type(s['a']),s['a'].dtype)# 如果需要选择多个标签的值，用[[]]来表示（相当于[]中包含一个列表）# 多标签索引结果是新的数组sci = s[['a','b','e']]print(sci,type(sci))&gt;&gt;&gt;a 0.714630b 0.213957c 0.172188d 0.972158e 0.875175dtype: float640.714630383451 &lt;class 'numpy.float64'&gt; float64a 0.714630b 0.213957e 0.875175dtype: float64 &lt;class 'pandas.core.series.Series'&gt; 切片索引12345678910111213141516171819202122232425262728293031323334s1 = pd.Series(np.random.rand(5))s2 = pd.Series(np.random.rand(5), index = ['a','b','c','d','e'])print(s1[1:4],s1[4])print(s2['a':'c'],s2['c'])print(s2[0:3],s2[3])print('-----')# 注意：用index做切片是末端包含print(s2[:-1])print(s2[::2])# 下标索引做切片，和list写法一样&gt;&gt;&gt;1 0.8659672 0.1145003 0.369301dtype: float64 0.411702342342a 0.717378b 0.642561c 0.391091dtype: float64 0.39109096261a 0.717378b 0.642561c 0.391091dtype: float64 0.998978363818-----a 0.717378b 0.642561c 0.391091d 0.998978dtype: float64a 0.717378c 0.391091e 0.957639dtype: float64 布尔型索引1234567891011121314151617181920212223242526272829303132333435363738394041424344# 布尔型索引# 数组做判断之后，返回的是一个由布尔值组成的新的数组# .isnull() / .notnull() 判断是否为空值 (None代表空值，NaN代表有问题的数值，两个都会识别为空值)# 布尔型索引方法：用[判断条件]表示，其中判断条件可以是 一个语句，或者是 一个布尔型数组！s = pd.Series(np.random.rand(3)*100)s[4] = None # 添加一个空值print(s)bs1 = s &gt; 50bs2 = s.isnull()bs3 = s.notnull()print(bs1, type(bs1), bs1.dtype)print(bs2, type(bs2), bs2.dtype)print(bs3, type(bs3), bs3.dtype)print('-----')print(s[s &gt; 50])print(s[bs3])&gt;&gt;&gt;0 2.038021 40.39892 25.20014 Nonedtype: object0 False1 False2 False4 Falsedtype: bool &lt;class 'pandas.core.series.Series'&gt; bool0 False1 False2 False4 Truedtype: bool &lt;class 'pandas.core.series.Series'&gt; bool0 True1 True2 True4 Falsedtype: bool &lt;class 'pandas.core.series.Series'&gt; bool-----Series([], dtype: object)0 2.038021 40.39892 25.2001dtype: object]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Selenium实例学习]]></title>
    <url>%2F2018%2F12%2F29%2Flearn-selenium%2F</url>
    <content type="text"><![CDATA[安装selenium1pip install selenium 下载驱动(chromedriver)下载前需要确认适配自己的浏览器版本1https://chromedriver.storage.googleapis.com/index.html selenium的基本用法基本用法看官方文档，讲述的很清楚了，推荐1https://selenium-python.readthedocs.io/installation.html 常用的实例 设置浏览器不加载图片123456789101112from selenium import webdriver# 设置不加载图片chrome_opt = webdriver.ChromeOptions()prefs = &#123; "profile.managed_default_content_settings.images": 2&#125;chrome_opt.add_experimental_option("prefs", prefs)driver = webdriver.Chrome(chrome_options=chrome_opt)driver.get("https://www.taobao.com") 点击弹窗与点击下拉列表123456789from selenium import webdriverfrom selenium.webdriver.support.select import Selectdriver = webdriver.Chrome()# 点击接受弹窗driver.switch_to.alert.accept()# 点击下拉列表sel = driver.find_element_by_id("nr")Select(sel).select_by_index(2) 切换窗口12345678910from selenium import webdriverdriver = webdriver.Chrome()first_win = driver.current_window_handleall_win = driver.current_window_handlefor win in all_win: if win != first_win: driver.switch_to.window(win) 自动下拉列表(以开源中国的博客栏目为例)很多页面是下拉加载更多信息，我们如何模拟这个下拉操作：123456789101112from selenium import webdriverimport timedriver = webdriver.Chrome()driver.get('https://www.oschina.net/blog')time.sleep(5)# 实现自动下拉刷新 下拉三页for i in range(3): driver.execute_script('window.scrollTo(0,document.body.scrollHeight); var lenOfPage=document.body.scrollHeight; return lenOfPage;') time.sleep(3) 如何模拟手机访问？1234567891011121314151617181920from selenium import webdriver# 模拟手机mobilesetting = &#123;"deviceName":"iPhone 6 Plus"&#125;options = webdriver.ChromeOptions()options.add_experimental_option("mobileEmulation", mobilesetting)driver = webdriver.Chrome(chrome_options=options)# 设置大小driver.set_window_size(400, 800)# driver.maximize_window()driver.get("https://www.taobao.com")# 后退driver.back()# 前进driver.forward()# 刷新driver.refresh() 如何为selenium设置代理？(连接无用户名密码认证的代理)1234567# 设置代理from selenium import webdriveroptions = webdriver.ChromeOptions()options.add_argument("--proxy-server=http://ip:port")driver = webdriver.Chrome(chrome_options=options)driver.get("http://httpbin.org/ip")print(driver.page_source) 如何为selenium设置代理？(有用户名和密码的连接)推荐几篇文章： https://www.cnblogs.com/roystime/p/6935543.htmlhttps://stackoverflow.com/questions/29983106/how-can-i-set-proxy-with-authentication-in-selenium-chrome-web-driver-using-pyth#answer-30953780https://cuiqingcai.com/4880.html scrapy + selenium 模拟登录csdn其实，没啥技术含量。只是简单运用，敲一遍加深印象。 spider.py12345678910111213141516171819202122232425# -*- coding: utf-8 -*-import scrapyfrom selenium import webdriverclass CsdnSpider(scrapy.Spider): name = 'csdn' allowed_domains = ['csdn.net'] start_urls = ['https://passport.csdn.net/account/login','https://i.csdn.net/#/account/index'] def __init__(self): # mobilsetting = &#123;"deviceName":"iPhone 6 Plus"&#125; # options = webdriver.ChromeOptions() # options.add_experimental_option("mobileEmulation", mobilsetting) self.browser = None self.cookies = None # self.browser.set_window_size(400,800) super(CsdnSpider, self).__init__() def spider_closed(self, response): print("spider close") self.brower.close() def parse(self, response): print(response.url) print(response.body.decode("utf-8","ignore")) middlewares.py123456789101112131415161718192021222324252627282930313233343536373839404142434445from scrapy import signalsfrom selenium import webdriverfrom scrapy.http import HtmlResponseimport timeimport requestsclass LoginMiddleware(object): def process_request(self, request, spider): if spider.name == "csdn": if request.url.find("login") != -1: spider.browser = webdriver.Chrome() spider.browser.get(request.url) switch = spider.browser.find_element_by_xpath('//a[@class="login-code__open js_login_trigger login-user__active"]') if switch.text == '账号登录': switch.click() time.sleep(3) username = spider.browser.find_element_by_id('username') password = spider.browser.find_element_by_id('password') time.sleep(2) username.send_keys("") time.sleep(1) password.send_keys("") time.sleep(2) click = spider.browser.find_element_by_class_name("logging") time.sleep(2) click.click() time.sleep(8) spider.cookies = spider.browser.get_cookies() return HtmlResponse( url=spider.browser.current_url, body=spider.browser.page_source, encoding="utf-8" ) else: req = requests.session() for cookie in spider.cookies: req.cookies.set(cookie['name'], cookie['value']) req.headers.clear() newpage = req.get(request.url) print(request.url) print(newpage.text) return HtmlResponse( url=request.url, body=newpage.text, encoding="utf-8" )]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python与MongoDB交互]]></title>
    <url>%2F2018%2F12%2F19%2Fdatabase-mongo%2F</url>
    <content type="text"><![CDATA[mongoDB的优势 易扩展 大数据量,高性能 灵活的数据模型###安装与启动安装mongodb：sudo apt-get install -y mongodb-org 安装可视化管理界面：https://robomongo.org/download 查看帮助：mongod –help 启动服务：sudo service mongod start 停止服务：sudo service mongod stop 重启服务：sudo service mongod restart 查看进程：ps ajx|grep mongod 配置文件的位置：/etc/mongod.conf 默认端口：27017 日志的位置：/var/log/mongodb/mongod.log mongodb数据库操作数据库操作查看当前的数据库：db 查看所有的数据库：show dbs /show databases 切换数据库：use db_name 删除当前的数据库：db.dropDatabase() 集合操作当集合不存在时，插入任何一条数据集合自动创建。 或者手动创建集合：db.createCollection(name,[options]) 其中options：12参数capped： 默认值为false表示不设置上限,值为true表示设置上限参数size： 当capped值为true时,需要指定此参数,表示上限大小,当文档达到上限时,会将之前的数据覆盖,单位为字节 当集合存在时： 查看集合：show collections 删除集合：db.集合名称.drop() mongodb数据类型Object ID：文档ID String： 字符串,最常使用,必须是有效的UTF-8 Boolean： 存储一个布尔值,true或false Integer： 整数可以是32位或64位,这取决于服务器 Double： 存储浮点值 Arrays： 数组或列表， 多个值存储到一个键 Object： 用于嵌入式的文档， 即一个值为一个文档 Null： 存储Null值 Timestamp： 时间戳,表示从1970-1-1到现在的总秒数 Date： 存储当前日期或时间的UNIX时间格式 注意点：1234567891011创建日期语句如下 ：参数的格式为YYYY-MM-DD new Date(&apos;2017-12-20&apos;)每个文档都有一个属性,为_id,保证每个文档的唯一性可以自己去设置_id插入文档,如果没有提供,那么MongoDB为每个⽂档提供了一个独特的_id,类型为objectIDobjectID是一个12字节的十六进制数： 前4个字节为当前时间戳 接下来3个字节的机器ID 接下来的2个字节中MongoDB的服务进程id 最后3个字节是简单的增量值 mongodb数据操作新增插入数据(字段_id存在就报错)：db.集合名称.insert(document) 插入数据(字段_id存在就更新)：db.集合名称.save(document) 举个栗子：123456#插入文档时,如果不指定_id参数,MongoDB会为文档分配一个唯一的ObjectIddb.xianyu.insert(&#123;name:"xianyuplus",age:"3"&#125;)#插入文档时,可以指定_id参数db.xianyu.insert(&#123;_id:"10001",name:"xianyuplus",age:"30"&#125;)#更新了上面_id为1001的文档db.xianyu.save(&#123;_id:"10001",name:"xianyuplus",age:"40"&#125;) 查询查询数据：db.集合名称.find() 举个栗子：1db.xianyu.find() 更新更新数据：db.集合名称.update(&lt;query&gt; ,&lt;update&gt;,{multi: &lt;boolean&gt;})123参数query:查询条件参数update:更新操作符参数multi:可选,默认是false,表示只更新找到的第一条记录,值为true表示把满足条件的文档全部更新 举个栗子：1234567891011121314原有内容：&#123; "_id" : ObjectId("5b66f05f1194e110103bc283"), "name": "xianyuplus", "age": "40" &#125;# 将name为xianyuplus的值替换为xianyuplus1db.xianyu.update(&#123;name:"xianyuplus"&#125;,&#123;name:"xianyuplus1"&#125;)操作后内容：&#123; "_id" : ObjectId("5b66f05f1194e110103bc283"), "name": "xianyuplus1" &#125; 可以看到单单使用update更新数据会导致原有数据被新数据替换，所以我们应该搭配$set使用,指定更新对应的键值。举个栗子：123456789101112131415原有内容：&#123; "_id" : ObjectId("5b66f05f1194e110103bc283"), "name": "xianyuplus", "age": "40" &#125;# 将name为xianyuplus的值更新为xianyuplus1db.xianyu.update(&#123;name:"xianyuplus"&#125;,&#123;$set:&#123;name:"xianyuplus1"&#125;&#125;)操作后内容：&#123; "_id" : ObjectId("5b66f05f1194e110103bc283"), "name": "xianyuplus1", "age": "40" &#125; 更新多条数据：使用参数multi:true 举个栗子：12# 更新全部数据的name值为xianyuplus1db.stu.update(&#123;&#125;,&#123;$set:&#123;name:"xianyuplus1"&#125;&#125;,&#123;multi:true&#125;) 注意：multi update only works with $ operators 即multi只要和$搭配使用时才能起效。 删除删除数据：db.集合名称.remove(&lt;query&gt;,{justOne: &lt;boolean&gt;}) 12参数query:可选,删除的文档的条件参数justOne:可选,如果设为true或1,则只删除一条,默认fals,表示删除多条 举个栗子：12# 把name值为xianyuplus的数据全部删掉db.xianyu.remove(&#123;name:"xianyuplus"&#125;) mongodb高级查询mongodb查询方法查询文档：db.集合名称.find({条件文档}) 查询一条数据:db.集合名称.findOne({条件文档}) 格式化查询：db.集合名称.find({条件文档}).pretty() 举个栗子：12345# 查询name为xianyuplus的数据db.xianyu.find(&#123;name:"xianyuplus"&#125;)# 查询一条name为xianyuplus的数据db.xianyu.findOne(&#123;name:"xianyuplus"&#125;) mongodb的比较运算符等于：如上述栗子 大于：$gt ( greater than ) 大于等于：$gte ( greater than equal ) 小于：$lt ( less than ) 小于等于：$lte ( less than equal ) 不等于：$nt ( not equal ) 举个栗子：12345678910# 查询age大于20的数据db.xianyu.find(&#123;age:&#123;$gt:20&#125;&#125;)# 查询age大于等于20的数据db.xianyu.find(&#123;age:&#123;$gte:20&#125;&#125;)# 查询age小于20的数据db.xianyu.find(&#123;age:&#123;$lt:20&#125;&#125;)# 查询age小于等于20的数据db.xianyu.find(&#123;age:&#123;$lte:20&#125;&#125;)# 查询age不等于20的数据db.xianyu.find(&#123;age:&#123;$ne:20&#125;&#125;) mongodb逻辑运算符and：在find条件文档中写入多个字段条件即可 or：使用$or 举个栗子：12345678#查找name为xianyuplus且age为20的数据db.xianyu.find(&#123;name:"xianyuplus",age:20&#125;)#查找name为xianyuplus或age为20的数据db.xianyu.find(&#123;$or:[&#123;name:"xianyuplus"&#125;,&#123;age:20&#125;]&#125;)#查找name为xianyuplus或age大于20的数据db.xianyu.find(&#123;$or:[&#123;age:&#123;$gt:20&#125;&#125;,&#123;name:"xianyuplus"&#125;]&#125;)#查找age大于等于20或gender为男并且name为xianyuplus的数据db.xianyu.find(&#123;$or:[&#123;gender:"true"&#125;,&#123;age:&#123;$gte:18&#125;&#125;],name:"xianyuplus"&#125;) mongodb范围运算符使用$in与$nin判断是否在某一范围内 举个栗子：12#查询年龄为18、28的数据db.xianyu.find(&#123;age:&#123;$in:[]18,28&#125;&#125;) mongodb使用正则表达式使用//或$regex编写正则表达式 举个栗子：123# 查询name以xian开头的数据db.xianyu.find(&#123;name:/^xianyu/&#125;)db.xianyu.find(&#123;name:&#123;$regex:'^xianyu'&#125;&#125;) mongodb分页与跳过查询前n条数据：db.集合名称.find().limit(NUMBER) 跳过n条数据：db.集合名称.find().skip(NUMBER) 举个栗子：123456#查询前3条数据db.xianyu.find().limit(3)#查询3条后的数据db.xianyu.find().skip(3)#skip和limit可以搭配使用,查询4,5,6条数据db.xianyu.find().skip(3).limit(3) mongodb自定义查询使用$where自定义查询,这里使用的是js语法 举个栗子：12345//查询age大于30的数据db.xianyu.find(&#123; $where:function() &#123; return this.age&gt;30;&#125;&#125;) mongodb投影投影：在查询结果中只显示你想要看到的数据字段内容。 db.集合名称.find({},{字段名称:1,...}) 想显示的字段设置为1,不想显示的字段不设置,而_id这个字段比较特殊,想要他不显示需要设置_id为0。 12#查询结果中只显示name字段，不显示agedb.xianyu.find(&#123;&#125;,&#123;name:1&#125;) mongodb排序排序：db.集合名称.find().sort({字段:1,...}) 将需要排序的字段设置值：升序为1,降序为-1 举个栗子：12#先按照性别降序排列再按照年龄升序排列db.xianyu.find().sort(&#123;gender:-1,age:1&#125;) mongodb计数统计数目：db.集合名称.find({条件}).count()db.集合名称.count({条件}) 举个栗子：1234#查询age为20的数据个数db.xianyu.find(&#123;age:20&#125;).count()#查询age大于20，且性别为nan的数据个数db.xianyu.count(&#123;age:&#123;$gt:20&#125;,gender:true&#125;) mongodb去重去重：db.集合名称.distinct(&#39;去重字段&#39;,{条件}) 举个栗子：12#去除家乡相同，且年龄大于18的数据db.xianyu.distinct('hometown',&#123;age:&#123;$gt:18&#125;&#125;) mongodb管道与聚合聚合(aggregate)是基于数据处理的聚合管道，每个文档通过一个由多个阶段（stage）组成的管道，可以对每个阶段的管道进行分组、过滤等功能，然后经过一系列的处理，输出相应的结果。 用法：db.集合名称.aggregate({管道:{表达式}}) 常用管道：1234567$group： 将集合中的文档分组， 可用于统计结果$match： 过滤数据， 只输出符合条件的文档$project： 修改输出文档的结构， 如重命名、 增加、 删除字段、 创建计算结果$sort： 将输出文档排序后输出$limit： 限制聚合管道返回的文档数$skip： 跳过指定数量的文档， 并返回余下的文档$unwind： 将数组类型的字段进行拆分 常用表达式：表达式：&quot;列名&quot;1234567$sum： 计算总和， $sum:1 表示以一倍计数$avg： 计算平均值$min： 获取最小值$max： 获取最大值$push： 在结果文档中插入值到一个数组中$first： 根据资源文档的排序获取第一个文档数据$last： 根据资源文档的排序获取最后一个文档数据 聚合之$group$group:将文档进行分组以便于统计数目 用法：_id表示分组依据,_id:&quot;$字段名&quot; 举个栗子：1234#按照hometown分组，并计数db.xianyu.aggregate(&#123;$group:&#123;_id:"$hometown", count:&#123;$sum:1&#125;&#125;&#125;)#将集合中所有的内容分为一组，统计个数db.xianyu.aggregate(&#123;$group:&#123;_id:null, count:&#123;$sum:1&#125;&#125;&#125;) 聚合之$project$project：修改输入文档的结构,如：重命名,增加、删除字段等 举个栗子：123456#按照hometown分组，并计数#分组输出，只显示count字段 db.xianyu.aggregate( &#123;$group:&#123;_id:"$hometown", count:&#123;$sum:1&#125;&#125;&#125;, &#123;$project:&#123;_id:0,count:1&#125;&#125; ) 聚合之$match$match:用于过滤数据,只输出符合条件的文档,功能和find类似,但是match是管道命令，能将结果交给后一个管道，但是find不可以。 举个栗子：12345678#查询age大于20#按照hometown分组，并计数#分组输出，只显示count字段 db.xianyu.aggregate( &#123;$match:&#123;age:&#123;$gte:20&#125;&#125;&#125;, &#123;$group:&#123;_id:"$hometown", count:&#123;$sum:1&#125;&#125;&#125;, &#123;$project:&#123;_id:0,count:1&#125;&#125; ) 聚合之$sort$sort:将输入文档排序后输出 举个栗子:12345678910#查询age大于20#按照hometown分组，并计数#分组输出，只显示count字段#按照计数升序排序 db.xianyu.aggregate( &#123;$match:&#123;age:&#123;$gte:20&#125;&#125;&#125;, &#123;$group:&#123;_id:"$hometown", count:&#123;$sum:1&#125;&#125;&#125;, &#123;$project:&#123;_id:0,count:1&#125;&#125;, &#123;$sort:&#123;count:1&#125;&#125; ) 聚合之$limit与$skip$limit:限制聚合管道返回的文档数 $skip:跳过指定数量的文档数,返回剩下的文档 举个栗子：1234567891011#查询age大于20#按照hometown分组，并计数#按照计数升序排序#跳过前一个文档,返回第二个 db.xianyu.aggregate( &#123;$match:&#123;age:&#123;$gte:20&#125;&#125;&#125;, &#123;$group:&#123;_id:"$hometown", count:&#123;$sum:1&#125;&#125;&#125;, &#123;$sort:&#123;count:1&#125;&#125;, &#123;$skip:1&#125;, &#123;$limit:1&#125; ) 聚合之$unwind$unwind：将文档中的某一个数组类型字段拆分成多条， 每条包含数组中的一个值 db.集合名称.aggregate({$unwind:&#39;$字段名称&#39;}) 举个栗子：123456db.xianyu.insert(&#123;_id:1,item:'t-shirt',size:['S','M','L']&#125;)db.xianyu.aggregate(&#123;$unwind:'$size'&#125;)输出：&#123; "_id" : 1, "item" : "t-shirt", "size" : "S" &#125;&#123; "_id" : 1, "item" : "t-shirt", "size" : "M" &#125;&#123; "_id" : 1, "item" : "t-shirt", "size" : "L" &#125; 聚合使用注意事项 $group对应的字典中有几个键，结果中就有几个键 分组依据需要放到_id后面 取不同的字段的值需要使用$,$gender,$age 取字典嵌套的字典中的值的时候$_id.country 能够同时按照多个键进行分组{$group:{_id:{country:&quot;$字段&quot;,province:&quot;$字段&quot;}}} 结果是：{_id:{country:&quot;&quot;,province:&quot;&quot;} mongodb索引用法：db.集合.ensureIndex({属性:1})，1表示升序， -1表示降序 创建唯一索引:db.集合.ensureIndex({&quot;属性&quot;:1},{&quot;unique&quot;:true}) 创建唯一索引并消除：db.集合.ensureIndex({&quot;属性&quot;:1},{&quot;unique&quot;:true,&quot;dropDups&quot;:true}) 建立联合索引：db.集合.ensureIndex({属性:1,age:1}) 查看当前集合的所有索引：db.集合.getIndexes() 删除索引：db.集合.dropIndex(&#39;索引名称&#39;) mongodb数据备份与恢复mongodb数据备份备份：mongodump -h dbhost -d dbname -o dbdirectory 123-h： 服务器地址,也可以指定端口号-d： 需要备份的数据库名称-o： 备份的数据存放位置,此目录中存放着备份出来的数据 mongodb数据恢复恢复：mongorestore -h dbhost -d dbname --dir dbdirectory123-h： 服务器地址-d： 需要恢复的数据库实例--dir： 备份数据所在位置 mongodb与python交互安装与导入安装：pip install pymongo 导入模块：from pymongo import MongoClient 实例化实例化对象以链接数据库,连接对象有host,port两个参数。123456from pymongo import MongoClientclass clientMongo: def __init__(self): client = MongoClient(host="127.0.0.1", port=27017) #使用[]括号的形式选择数据库和集合 self.cliention = client["xianyu"]["xianyuplus"] 插入数据插入单条数据：返回ObjectId123def item_inser_one(self): ret = self.cliention.insert(&#123;"xianyu":"xianyuplus","age":20&#125;) print(ret) 插入多条数据：123def item_insert_many(self): item_list = [&#123;"name":"xianyuplus&#123;&#125;".format(i)&#125; for i in range(10000)] items = self.cliention.insert_many(item_list) 查询数据查询单条数据：123def item_find_one(self): ret = self.cliention.find_one(&#123;"xianyu":"xianyuplus"&#125;) print(ret) 查询多条数据：1234def item_find_many(self): ret = self.cliention.find(&#123;"xianyu":"xianyuplus"&#125;) for i in ret: print(i) 更新数据更新一条数据：12def item_update_one(self): self.cliention.update_one(&#123;"xianyu":"xianyuplus"&#125;,&#123;"$set":&#123;"xianyu":"xianyu"&#125;&#125;) 更新全部数据：12def item_update(self): self.cliention.update_many(&#123;"xianyu":"xianyuplus"&#125;,&#123;"$set":&#123;"xianyu":"xianyu"&#125;&#125;) 删除数据删除一条数据：12def item_delete_one(self): self.cliention.delete_one(&#123;"xianyu":"xianyuplus"&#125;) 删除符合条件的数据：12def item_delete_many(self): self.cliention.delete_many(&#123;"xianyu":"xianyuplus"&#125;)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy实战 - 分析单车数据中用户类别的占比]]></title>
    <url>%2F2018%2F12%2F19%2Fnumpy-shizhan3%2F</url>
    <content type="text"><![CDATA[本次复习的知识点如下： numpy的reshape以及shape在实战中的运用 matplotlib饼图绘制 分析目标观察上次的数据，数据中有的数据有会员与非会员两种用户类别。这次我们主要分析一下两种类别用户在数据中占比。 数据读取与数据清洗根据流程示意图我们主要遵循下面几个步骤： 在过去两次的文章中已经有关于数据读取和数据分析操作的详细代码讲解，所以不再赘述。此处代码为：1234567891011# 数据读取，数据清洗def read_clean_data(): clndata_arr_list = [] for data_filename in data_filenames: file = os.path.join(data_path, data_filename) data_arr = np.loadtxt(file, skiprows=1, delimiter=',', dtype=bytes).astype(str) cln_arr = np.core.defchararray.replace(data_arr[:, -1], '"', '') cln_arr = cln_arr.reshape(-1,1) clndata_arr_list.append(cln_arr) year_cln_arr = np.concatenate(clndata_arr_list) return year_cln_arr 这里需要注意两点： 因为数据较大，我们没有数据文件具体数据量，所以在使用numpy.reshape时我们可以使用numpy.reshape(-1,1)这样numpy可以使用统计或的具体数值替换-1。 我们对数据的需求不再是获取时间的平均值，只需获取数据最后一列并使用concatenate方法堆叠到一起以便下一步处理。 数据分析根据这次的分析目标，我们取出最后一列Member type。 在上一步我们已经获取了全部的数值，在本部只需筛选统计出会员与非会员的数值就可以了。 我们可以先看下完成后的这部分代码：1234567# 数据分析def mean_data(year_cln_arr): member = year_cln_arr[year_cln_arr == 'Member'].shape[0] casual = year_cln_arr[year_cln_arr == 'Casual'].shape[0] users = [member,casual] print(users) return users 同样，这里使用numpy.shape获取用户分类的具体数据。 结果展示生成的饼图： 下面是生成饼图的代码： 1234567# 结果展示plt.figure() plt.pie(users, labels=['Member', 'Casual'], autopct='%.2f%%', shadow=True, explode=(0.05, 0)) plt.axis('equal') plt.tight_layout() plt.savefig(os.path.join(output_path, './piechart.png')) plt.show()]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python与Redis交互]]></title>
    <url>%2F2018%2F12%2F10%2Fpython-and-redis%2F</url>
    <content type="text"><![CDATA[nosql与redis介绍nosql数据库： 不支持SQL语法 存储结构跟传统关系型数据库中的那种关系表完全不同，nosql中存储的数据都是KV形式 NoSQL的世界中没有一种通用的语言，每种nosql数据库都有自己的api和语法，以及擅长的业务场景 NoSQL中的产品种类相当多：Mongodb,Redis,Hbase hadoop,Cassandra hadoop等等。 redi数据库是一个开源的使用ANSI C语言编写、支持网络、可基于内存亦可持久化的日志型、Key-Value数据库，并提供多种语言的API。 安装与启动windows: 安装redis:https://github.com/MSOpenTech/redis/releases 安装redis管理客户端:https://redisdesktop.com/download linux: 安装:sudo apt-get -y install redis-server 连接redis：redis-cli 重要配置项说明：配置文件位置：/etc/redis/redis.conf1234567bind ip 绑定ip以配置远程访问port [num] 绑定端口daemonize 是否以守护进程运行，推荐设置为yes，不会在命令行阻塞，类似于服务dbfilename 数据文件名称dir /xx/xx/redis 设置数据文件存储的位置log file /xx/xx/xx/redis-server.log 日志文件位置slaveof ip port 主从复制的ip端口 启动redis：sudo server redis start 停止redis:sudo server redis stop 重启redis:sudo server redis restart 加载指定的redis配置：sudo redis-server /etc/redis/redis.conf 查看redis进程：ps-ef|grep redis 杀死进程：sudo kill -9 pid redis数据结构与操作redis数据结构 redis是key-value的数据结构，每条数据都是一个键值对 键的类型是字符串,且键不能重复 值的类型分为五种：字符串strin,哈希hash,列表list,集合set,有序集合zset redis数据操作redis键命令redis键命令对所有数据类型通用 查找键：keys [正则表达式] 查看全部键：keys * 判断键是否存在：exists key 查看键对应的值的类型：type key 删除键对应的值的类型：del key1 key2 key3... 设置键的过期时间：expire key seconds 查看键的有效时间：ttl key string类型相关操作字符串类型是Redis中最为基础的数据存储类型，它在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据或Json对象描述信息等。在Redis中字符串类型的Value最多可以容纳的数据长度是512M。 新增与更新设置键值(当键存在即为更新值):set key value 举个栗子1：12# 设置键为xianyu值为plus的数据set xianyu plus 设置键值以及过期时间：setex key seconds value 举个栗子2：12# 设置键为xianyu值为plus的数据,且3秒后过期setex xianyu 3 plus 设置多个键值：mset key1 value1 key2 value2... 举个栗子3：12# 设置键为xianyu[n]值为plus[n]的数据mset xianyu plus xianyu1 plus1 xianyu2 plus2 向现有的值追加其他值：append key value 举个栗子4：12# 向键名为xianyu的值中追加值1append xianyu 1 查看查看键值：get key 举个栗子1：12# 查看xianyu的值get xianyu 查看多个键的值：mget key1 key2 key3... 举个栗子2：12# 查看xianyu1,xianyu2,xianyu3的值mget xianyu1 xianyu2 xianyu3 删除删除键：del key hash哈希类型相关操作hash类型的值的类型为string 新增与修改设置单个值：hset key field value 举个栗子1：12# 设置键xianyu的name属性的值为xianyuplushset xianyu name xianyuplus 设置多个值：hmset key field1 value1 field2 value2 ... 举个栗子2：12# 设置xianyu的name值为xianyuplus age值为23hmset xianyu name xianyuplus age 23 获取获取单个键的所有属性：hkeys key 举个栗子1：12# 获取xianyu的所有属性hkeys xianyu 获取单个属性的值：hget key field 举个栗子2:12# 获取xianyu的name值hget xianyu name 获取多个属性的值：hmget key field1 field2 ... 举个栗子3:12# 获取xianyu的name值和age值hmget xianyu name age 获取所有属性的值：hvals key 举个栗子4：1hvals xianyu 删除删除单个键所有属性和值：del key 删除键的属性(对应的值也会被清空)：hdel field1 field2... 举个栗子：12# 删除xianyu的name和agehdel xianyu name age list列表类型相关操作list类型的值为string,值按照插入顺序排序 新增在list左边插入数据：lpush key value1 value2 value3... 举个栗子1：12#插入1,2,3,4,5,6,lpush xianyu 1 2 3 4 5 6 在list右边插入数据：rpush key value1 value2 value3...举个栗子2：12#插入1,2,3,4,5,6,rpush xianyu 1 2 3 4 5 6 在指定元素前或后插入数据：linsert key before/after 现有元素 新元素 举个栗子3：12# 在1的前面插入alinsert xianyu before 1 a 获取列表元素获取列表指定范围内的值：lrange key start stop 注意：这里的列表和python中的列表索引方式相同,从左往右以0开始,索引支持负数 举个栗子1：1234# 获取键为xianyu的列表0到6的全部元素lrange xianyu 0 6# 获取键为xianyu的列表所有元素lrange xianyu 0 -1 修改设置指定索引位置的元素：lset key index value 举个栗子：1lset xianyu 0 1 删除删除指定元素：lrem key count value1234将列表中前count次出现的值为value的元素移除count &gt; 0: 从头往尾移除count &lt; 0: 从尾往头移除count = 0: 移除所有 举个栗子：12# 删除从头往尾数的两个1lrem xianyu 2 1 set集合类型相关操作 无序集合 元素为string类型 元素具有唯一性,不重复 说明：对于集合没有修改操作 新增添加元素：sadd key member1 member2 ... 举个栗子：12# 向键xianyu的集合中添加元素a,b,csadd xianyu a b c 获取获取所有值：smembers key 举个栗子：12# 获取键xianyu的集合所有的值smembers xianyu 删除删除指定的元素：srem key member 举个栗子：12# 删除键xianyu的集合值指定的元素srem xianyu a zset有序集合相关操作 sorted set,有序集合 元素为string类型 元素具有唯一性,不重复 每个元素都会关联一个double类型的score，表示权重，通过权重将元素从小到大排序 说明：没有修改操作 新增新增多个元素：zadd key score1 member1 score2 member2 ... 举个栗子：1zadd xianyu 2 name 1 age 查看这里的有序集合和列表相同都有索引值 查看有序集合的值：zrange key start stop 举个栗子：12345# 获取xianyu中0-6的值zrange xianyu 0 6# 获取xianyu中所有的值zrange xianyu 0 -1# 获取xianyu中权重最大最小中间的值 查看集合权重在指定范围内的值：zrangebyscore key min max 举个栗子：1zrangebyscore xianyu min max 返回成员member的score值：zscore key member 举个栗子：1zscore xianyu a 删除删除指定元素:zrem key member1 member2 ... 举个栗子：1zrem xianyu a 删除权重在指定范围的元素:zrem key min max 举个栗子：1zrem xianyu 1 2 python与redis交互安装与导入安装：pip install redis导入模块：from redis import * 创建StrictRedis通过init创建对象，指定参数host、port与指定的服务器和端口连接,host默认为localhost,port默认为6379，db默认为0,默认没有密码。 1red = StrictRedis(host='localhost', port=6379, db=0) 方法与操作string实例(其他类型操作类似)这里不同类型拥有的方法和上面redis中讲解的方法相同，这里不再赘述。 举个栗子：123456789101112131415161718192021222324252627282930313233# 链接redis，创建stricredis对象from redis import *if __name__=="__main__": try: #创建StrictRedis对象，与redis服务器连接 redis=StrictRedis() # 新增一个string类型 result=redis.set('name','xianyuplus') # 成功打印True,失败打印False print(result) #获取键name的值 result = redis.get('name') #输出键的值，如果键不存在则返回None print(result) #设置键name的值，如果键已经存在则进行修改，如果键不存在则进行添加 result = redis.set('name','xianyu') #输出响应结果，如果操作成功则返回True，否则返回False print(result) result = redis.delete('name') #输出响应结果，如果删除成功则返回受影响的键数，否则则返回0 print(result) #获取所有的键 result=sr.keys() #输出响应结果，所有的键构成⼀个列表，如果没有键则返回空列表 print(result) except Exception as e: print(e) redis搭建主从服务(ubuntu)什么是主从服务 一个master可以拥有多个slave,一个slave可以拥有多个slave，如此下去，形成了多级服务器集群架构 master用来写数据,slave用来读数据,经统计：网站的读写比率是10:1 通过主从配置可以实现读写分离 master和slave都是一个redis实例(redis服务) 配置主1234vim etc/redis/redis.confbind [本机ip]sudo service redis stopredis-server redis.conf 配置从123456sudo cp redis.conf ./slave.confvim slave.confbind [主配置的ip]slaveof [主配置的ip 端口号]port 6378 --这个端口号不能和主配置的相同sudo redis-server slave.conf 查看主从关系1redis-cli -h [主配置的ip] info Replication 主从测试12主配置上写入数据：set xianyu xianyuplus从配置上读取数据：get xianyu]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy实战 - 分析不同用户季度平均使用时间]]></title>
    <url>%2F2018%2F12%2F09%2Fnumpy-shizhan2%2F</url>
    <content type="text"><![CDATA[本次复习的知识点如下： 布尔型数组及数据过滤 多维数组的构造 使用numpy保存文本文件 matplotlib折线图绘制 matplotlib图表常用属性的设置方法 图表的保存 关于数据源上次的文章发出之后发现忘了补充数据源的链接，之后咸鱼补充在留言区了，有需要动手实践的朋友可以自取，下面是直通车： 分析目标观察上次的数据，数据中有的数据有会员与非会员两种用户类别。这次我们主要分析一下两种类别用户的平均骑行时间对比。 数据读取与数据清洗根据上次的流程示意图我们主要遵循下面几个步骤： 但是在实际操作中发现，本次的实战数据非常干净，完全可以把我们的数据读取和数据清洗代码结合到一起来实现代码简化的目的。 此处代码可以简化为：123456789# 数据读取，数据清洗def data_collection(): clndata_arr_list = [] for data_filename in data_filenames: file = os.path.join(data_path, data_filename) data_arr = np.loadtxt(file, skiprows=1, delimiter=',', dtype=bytes).astype(str) cln_data_arr = np.core.defchararray.replace(data_arr, '"', '') clndata_arr_list.append(cln_data_arr) return clndata_arr_list 在上一篇的文章中，具体的实现原理已经做过说明这里就不多做赘述。 数据分析根据这次的分析目标，我们除了取出第一列的Duration (ms)同时还要取出最后一列Member type。 我们可以使用布尔型数组对我们的数据进行过滤，具体的使用方法可以参考下面这篇文章： 我们可以先看下完成后的这部分代码：123456789# 数据分析def mean_data(clndata_arr_list, member_type): duration_mean_list = [] for cln_data in clndata_arr_list: bool_arr = cln_data[:, -1] == member_type filter_arr = cln_data[bool_arr] duration_mean = np.mean(filter_arr[:, 0].astype('float')/ 1000 / 60) duration_mean_list.append(duration_mean) return duration_mean_list 这里我们传入一个member_type，用单个标量member_type和取出的向量cln_data[:, -1]做比较，以筛选出我们需要的数据。 在数学上标量和向量是没办法比对的,毕竟维度不同，但是在numpy中它的广播机制很好的为我们实现了这一需求，numpy可以将单个标量变成比对数据同样的数据维度，这样就可以进行一对一比对，达到使用布尔型数组筛选数据的需求了。 至于其余关于数据如何取出平均值等操作在上一篇文章已经做了介绍，接下来进入数据可视化展示的部分。 结果展示生成的折线图： 生成的csv表格： 生成的数据图表上次美化不少，具体实现可以看下下面的代码，具体美化属性都标注在注释当中。 123456789101112131415161718192021222324252627# 结果展示def show_data(member_mean_duration_list, casual_mean_duration_list): # 构造多维数组 # 使用fmt方式输出指定格式的数据格式，默认输出科学计数的格式 mean_duraion_arr = np.array([member_mean_duration_list, casual_mean_duration_list]).transpose() np.savetxt('./mean_duration.csv', mean_duraion_arr, delimiter=',', header='Member Mean Duraion, Casual Mean Duraion', fmt='%.4f', comments='') # 生成空白画布 plt.figure() # color指定显示的折线颜色 # linestyle指定折线的样式 # marker指定节点样式 plt.plot(member_mean_duration_list, color='g', linestyle='-', marker='o', label='Member') plt.plot(casual_mean_duration_list, color='r', linestyle='--', marker='*', label='Casual') plt.title('Member vs Casual') # rotation指定下标的倾斜角度 plt.xticks(range(0, 4), ['1st', '2nd', '3rd', '4th'], rotation=45) # xlabel x，y轴的标题 plt.xlabel('Quarter') plt.ylabel('Mean duration (min)') plt.legend(loc='best') plt.tight_layout() plt.savefig('./duration_trend.png') plt.show()]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python与Mysql交互]]></title>
    <url>%2F2018%2F12%2F09%2Fpython-and-mysql%2F</url>
    <content type="text"><![CDATA[安装与启动安装：sudo apt-get install mysql-server 查看服务：ps ajx | grep mysql 停止服务:sudo service mysql stop 开启服务:sudo service mysql start 重启服务:sudo service mysql restart 链接数据库：mysql -uroot -p后输入密码 查看版本：select version(); 常见数据库语句查看数据库：show database; 创建数据库：create database 库名 [charset = UTF8]; 查看建库语句：show create database 库名; 使用数据库：use 库名; 删除数据库：drop 库名; 常见数据表语句查看表：show table; 查看表结构：desc 表名; 创建表：12345678CREATE TABLE table_name( column1 datatype contrai, column2 datatype, column3 datatype, ..... columnN datatype, PRIMARY KEY(one or more columns)); 创建表常用属性字段：1234-- auto_inorement 表示自动增长-- not null 表示不为空-- primary key 表示为主键-- defaul 表示默认值 删除表：drop table; 修改表结构：1234添加字段：alter table 表名 add 列名 类型;修改字段(重命名)：alter table 表名 change 原名 新名 类型及约束;修改字段(不重命名)：alter table 表名 modify 列名 类型及约束;删除字段：alter table 表名 drop 列名; 常用增删改查基本查询查看所有列：select * from 表名; 查看指定列：select 列1,列2,... from 表名; 新增全列插入：insert into 表名 values(...); --需要给主键留下占位符,用0或null皆可。 部分列插入：insert into 表名(列1,...) values(值1,...); 插入多行全列数据：insert into 表名 values(...),(...)...; 插入多行部分列数据：insert into 表名(列1,...) values(值1,...),(值1,...)...; 更新更新操作：update 表名 set 列1=值1,列2=值2... where 条件; 删除删除操作(不推荐)：delete from 表名 where 条件; 逻辑删除：update 字段名 set isvalid=0 where id=1; --设置删除字段,执行删除字段的操作即对该字段更新。 mysql查询详解查询消除重复行：select distinct 列1,... from 表名; 条件查询where条件查询：select * from 表名 where 条件; where可以与比较运算符、逻辑运算符、模糊查询、范围查询、空判断搭配使用 比较运算符123456等于: =大于: &gt;大于等于: &gt;=小于: &lt;小于等于: &lt;=不等于: != 或 &lt;&gt; 举个栗子：1234select * from students where id &gt; 1;select * from students where id &lt;= 2;select * from students where name != '咸鱼';select * from students where is_delete=0; 逻辑运算符123andornot 举个栗子：123select * from students where id &gt; 3 and gender=0;select * from students where id &lt; 4 or is_delete=0;select * from students where id not 4; 模糊查询1234like% 表示任意多个任意字符_ 表示一个任意字符rlike 举个栗子：123select * from students where name like '咸%'; --查询以咸字开头的内容select * from students where name like '咸_'; --查询以咸字开头且后面只有一个字的内容select * from students where name like '咸%' or name like '%鱼'; -- 查询以咸字开头或以鱼字结尾的内容 范围查询1234in 表示在一个非连续的范围内no in 表示不在一个非连续的范围内between ... and ... 表示在一个连续的范围内rlike 表示正则查询，可以使用正则表达式查询数据 举个栗子：1234select * from students where id in(1,3,8); -- 查询 id 在 1,3,8 当中的内容select * from students where id not in(1,3,8); -- 查询 id 不在 1,3,8 当中的内容select * from students where id between 3 and 8; -- 查询 id 在3到8之间的内容select * from students where name rlike "^咸"; -- 查询 name 是以咸字开头的内容 空判断1判断是否为空 is null 举个栗子：1select * from students where height is null; 以上几种预算符优先级为:优先级由高到低的顺序为：小括号、not、比较运算符、逻辑运算符。and比or先运算，如果同时出现并希望先算or，需要结合()使用。 排序12asc 升序desc 降序 举个栗子：1select * from students order by age desc,height desc; --显示所有的学生信息，先按照年龄从大到小排序，当年龄相同时 按照身高从高到矮排序 聚合函数12345count(*)查询总数max(列)表示求此列的最大值min(列)表示求此列的最小值sum(列)表示求此列的和avg(列)表示求此列的平均值 举个栗子：123456select count(*) from students;select max(id) from students where gender=2;select min(id) from students where is_delete=0;select sum(age) from students where gender=1;select sum(age)/count(*) from students where gender=1; --求平均年龄select avg(id) from students where is_delete=0 and gender=2; 分组12group by 将查询结果按照1个或多个字段进行分组，字段值相同的为一组group_concat 表示分组之后，根据分组结果，使用group_concat()来放置每一组的某字段的值的集合 举个栗子：123456789101112131415161718192021select gender from students group by gender; -- 将学生按照性别分组输出结果：+--------+| gender |+--------+| 男 || 女 || 中性 || 保密 |+--------+select gender,group_concat(name) from students group by gender;输出结果：+--------+-----------------------------------------------------------+| gender | group_concat(name) |+--------+-----------------------------------------------------------+| 男 | 小彭,小刘,小周,小程,小郭 || 女 | 小明,小月,小蓉,小贤,小菲,小香,小杰 || 中性 | 小金 || 保密 | 小凤 |+--------+-----------------------------------------------------------+ 分页1select * from 表名 limit start,count 举个栗子：1select * from students where gender=1 limit 0,3; --查询前三行的数据 连接查询语法：1select * from 表1 inner/left/right join 表2 on 表1.列 = 表2.列 其中：123inner join(内连接查询)：查询的结果为两个表匹配到的数据right join(右连接查询)：查询的结果为两个表匹配到的数据，右表特有的数据，对于左表中不存在的数据使用null填充left join(左连接查询)：查询的结果为两个表匹配到的数据，左表特有的数据，对于右表中不存在的数据使用null填充 举个栗子：123select * from students inner join classes on students.cls_id = classes.id;select * from students as s left join classes as c on s.cls_id = c.id;select * from students as s right join classes as c on s.cls_id = c.id; 子查询在一个 select 语句中,嵌入了另外一个 select 语句, 那么被嵌入的 select 语句称之为子查询语句。 子查询可以和 in 搭配使用1主查询 where 条件 in (子查询) 数据库的备份与恢复数据库备份1mysqldump –uroot –p 数据库名 &gt; 备份文件名.sql; 数据库恢复1mysql -uroot –p 新数据库名 &lt; 备份文件名.sql Python与mysql交互 安装与导入安装相关库：pip install pymysql导入：from pymysql import * 创建connection对象1connection = connect(host, port, database, user, password, charset) 其中参数如下：123456host：连接的mysql主机，如果本机是&apos;localhost&apos;port：连接的mysql主机的端口，默认是3306database：数据库的名称user：连接的用户名password：连接的密码charset：通信采用的编码方式，推荐使用utf8 connection对象方法如下：123close()关闭连接commit()提交cursor()返回Cursor对象，用于执行sql语句并获得结果 获取cursor1cursor=connection.cursor() 其中常用方法：1234close():关闭cursorexecute(operation [, parameters ]):执行语句，返回受影响的行数，主要用于执行insert、update、delete语句，也可以执行create、alter、drop等语句。fetchone():执行查询语句时，获取查询结果集的第一个行数据，返回一个元组fetchall():执行查询时，获取结果集的所有行，一行构成一个元组，再将这些元组装入一个元组返回 举个栗子：123456789101112from pymysql import *def main(): conn = connect(host='localhost',port=3306,database='xianyuplus',user='root',password='mysql',charset='utf8') cs1 = conn.cursor() count = cs1.execute('insert into xianyufans(name) values("666")') conn.commit() cs1.close() conn.close()if __name__ == '__main__': main() mysql视图什么是视图？视图是对若干张基本表的引用，一张虚表，查询语句执行的结果，不存储具体的数据。 视图语句1234创建视图:create view 视图名称 as select语句; --建议视图以v_开头查看视图：show tables;使用视图：select * from 视图名称;删除视图：drop view 视图名称; 视图作用 提高了重用性，就像一个函数 对数据库重构，却不影响程序的运行 提高了安全性能，可以对不同的用户 让数据更加清晰 mysql事务什么是事务？事务,它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。 事务有什么特点？ 原子性，一个事务必须被视为一个不可分割的最小工作单元，整个事务中的所有操作要么全部提交成功，要么全部失败回滚，对于一个事务来说，不可能只执行其中的一部分操作，这就是事务的原子性。 一致性，数据库总是从一个一致性的状态转换到另一个一致性的状态。 隔离性，一个事务所做的修改在最终提交以前，对其他事务是不可见的。 持久性，一旦事务提交，则其所做的修改会永久保存到数据库。 事务相关命令123开启事务：start transaction; 或者 begin;提交事务：commit;回滚事务：rollback; mysql索引什么是索引？数据库索引好比是一本书前面的目录，能加快数据库的查询速度 索引相关命令123创建索引：create index 索引名称 on 表名(字段名称(长度)) --当指定索引的字段类型为字符串时，应填写长度查看索引：show index from 表名;删除索引：drop index 索引名称 on 表名; 注意事项 建立太多的索引将会影响更新和插入的速度，因为它需要同样更新每个索引文件。对于一个经常需要更新和插入的表格，就没有必要为一个很少使用的where字句单独建立索引了，对于比较小的表，排序的开销不会很大，也没有必要建立另外的索引。 建立索引会占用磁盘空间。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy实战 - 分析单车季度平均使用时间]]></title>
    <url>%2F2018%2F12%2F05%2Fnumpy-shizhan1%2F</url>
    <content type="text"><![CDATA[关于数据科学的学习，咸鱼也进行了一段时间，但是光学不练是学一点忘一点，所以咸鱼找了一些某共享单车的数据进行一点简单的数据分析。 思路整理咸鱼也是第一次动手写数据分析相关的代码，所以咸鱼上网找了一张大致的流程图，且以此整理思路分隔代码。 在企业实际开发中各个步骤的代码不会像咸鱼下面的代码一样各块分隔的那么清楚，肯定是相互交织且复杂的。 实战分析目的看标题就知道了，分析各季度共享单车的骑行时间。 数据收集因为这次的数据源自网络，所以先简单看下数据的结构： 可以看到数据有9个字段：1&quot;Duration (ms)&quot;,&quot;Start date&quot;,&quot;End date&quot;,&quot;Start station number&quot;,&quot;Start station&quot;,&quot;End station number&quot;,&quot;End station&quot;,&quot;Bike number&quot;,&quot;Member type&quot; 按照我们的目标，我们只需要第一个字段Duration(ms)。所以第一步先读取已经下载好的数据之后在第二步数据清洗中取出需要的字段： 12345678# 数据收集def data_collection(): data_arr_list = [] for data_filename in data_filenames: file = os.path.join(data_path, data_filename) data_arr = np.loadtxt(file,dtype=bytes,delimiter=',', skiprows=1).astype(str) data_arr_list.append(data_arr) return data_arr_list 这里关于numpy的用法，可以参考之前的几篇关于numpy的文章： 数据清洗因为数据是整理后导出的数据所以不需要清洗缺失值等操作，我们直接取出需要的字段，做一些处理即可。 这里骑行时间单位为ms,所以需要转化为min需要/1000/60。123456789# 数据清洗def data_clean(data_arr_list): duration_min_list = [] for data_arr in data_arr_list: data_arr = data_arr[:,0] duration_ms = np.core.defchararray.replace(data_arr,'"','') duration_min = duration_ms.astype('float') / 1000 / 60 duration_min_list.append(duration_min) return duration_min_list 数据分析计算平均值在numpy中提供了计算函数，直接调用即可。1234567# 数据分析def mean_data(duration_min_list): duration_mean_list = [] for duration_min in duration_min_list: duration_mean = np.mean(duration_min) duration_mean_list.append(duration_mean) return duration_mean_list 结果展示这里可视化展示使用的是matplotlib.pyplot库,咸鱼目前还没有写相关的入门文章，可以上网看下文档学习下简单使用即可，之后会有系列文章写可视化的内容。 123456# 数据展示def show_data(duration_mean_list): plt.figure() name_list = ['第一季度', '第二季度', '第三季度', '第四季度'] plt.bar(range(len(duration_mean_list)),duration_mean_list,tick_label = name_list) plt.show() 成果展示 单单从上面的图可以看到以炎热的夏季和凉爽的秋季为主调的二三季度的骑行时间要高于春冬为主调的一四季度，以此判断气温变化对人们使用的共享单车的影响。 一些踩过的坑关于数据读取(一)在python中字符串是有字节字符串和文本字符串之分的，我们通常说的字符串是指文本字符串。而使用numpy的loadtxt函数读取的字符串默认是字节字符串，输出的话字符串前面会有个b，形如b’……’。通常是需要转换的，如果不转换将会出现问题。 在数据收集部分如果不注意这一点，在数据清洗部分，字段的格式就会因为Duration的值多了一个b转化上就会报错。 处理方式： numpy.loadtxt读入的字符串总是bytes格式，总是在前面加了一个b原因：np.loadtxt and np.genfromtxt operate in byte mode, which is the default string type in Python 2. But Python 3 uses unicode, and marks bytestrings with this b. numpy.loadtxt中也声明了：Note that generators should return byte strings for Python 3k.解决：使用numpy.loadtxt从文件读取字符串，最好使用这种方式np.loadtxt(filename, dtype=bytes).astype(str) 作者：Cameron链接：https://www.zhihu.com/question/28690341/answer/164344688来源：知乎 关于数据读取上的坑（二）可以看到咸鱼在读取数据的时候使用的是numpy.loadtxt,这样的操作固然方便，但是代价就是内存直接爆掉，还好这次的数据才500M，所以不推荐大家使用我这个方法，之后会加以改进（如果我会的话） 这里分享一段代码，来自慕课网bobby老师的实战课，如何使用生成器读取大文本文件：12345678910111213141516171819#500G, 特殊 一行def myreadlines(f, newline): buf = "" while True: while newline in buf: pos = buf.index(newline) yield buf[:pos] buf = buf[pos + len(newline):] chunk = f.read(4096) if not chunk: #说明已经读到了文件结尾 yield buf break buf += chunkwith open("input.txt") as f: for line in myreadlines(f, "&#123;|&#125;"): print (line) 关于matplotlib.pyplot使用上的坑在可视化的时候，柱状图的标识是中文，在显示的时候直接显示的是方块，无法显示中文。如下： 处理方法：123456789101112131415161718解决方式一：修改配置文件(1)找到matplotlibrc文件（搜索一下就可以找到了）(2)修改：font.serif和font.sans-serif，我的在205,206行font.serif: SimHei, Bitstream Vera Serif, New Century Schoolbook, Century Schoolbook L, Utopia, ITC Bookman, Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, serif Bookman, Nimbus Roman No9 L, Times New Roman, Times, Palatino, Charter, seriffont.sans-serif: SimHei, Bitstream Vera Sans, Lucida Grande, Verdana, Geneva, Lucid, Arial, Helvetica, Avant Garde, sans-serif解决方式二：在代码中修改import matplotlib指定默认字体matplotlib.rcParams[‘font.sans-serif’] = [‘SimHei’]matplotlib.rcParams[‘font.family’]=’sans-serif’解决负号’-‘显示为方块的问题matplotlib.rcParams[‘axes.unicode_minus’] = False---------------------来源：CSDN原文：https://blog.csdn.net/weixin_40283480/article/details/81613008]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis数据库存入其他数据库]]></title>
    <url>%2F2018%2F11%2F29%2Fdatabase-redis%2F</url>
    <content type="text"><![CDATA[将redis数据库导入mongodb数据库1234567891011121314151617181920212223242526import json, redis, pymongodef main(): # 指定Redis数据库信息 rediscli = redis.StrictRedis(host='127.0.0.1', port=6379, db=0) # 指定MongoDB数据库信息 mongocli = pymongo.MongoClient(host='localhost', port=27017) # 创建数据库名 db = mongocli['sina'] # 创建表名 sheet = db['sina_items'] offset = 0 while True: # FIFO模式为 blpop，LIFO模式为 brpop，获取键值 source, data = rediscli.blpop(["sinainfospider_redis:items"]) item = json.loads(data.decode("utf-8")) sheet.insert(item) offset += 1 print(offset) try: print("Processing: %s " % item) except KeyError: print("Error procesing: %s" % item)if __name__ == '__main__': main() 将redis数据存入mysql数据库123456789101112131415161718192021222324import redis, json, timefrom pymysql import connect# redis数据库链接redis_client = redis.StrictRedis(host="127.0.0.1", port=6379, db=0)# mysql数据库链接mysql_client = connect(host="127.0.0.1", user="root", password="mysql", database="sina", port=3306, charset='utf8')cursor = mysql_client.cursor()i = 1while True: print(i) time.sleep(1) source, data = redis_client.blpop(["sinainfospider_redis:items"]) item = json.loads(data.decode()) print("source===========", source) print("item===========", item) sql = "insert into sina_items(parent_url,parent_title,sub_title,sub_url,sub_file_name,son_url,head,content,crawled,spider) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)" params = [item["parent_url"], item["parent_title"], item["sub_title"], item["sub_url"], item["sub_file_name"], item["son_url"], item["head"], item["content"], item["crawled"], item["spider"], ] cursor.execute(sql, params) mysql_client.commit() i += 1]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy实用demo实例]]></title>
    <url>%2F2018%2F11%2F29%2Fscrapy-use-demo%2F</url>
    <content type="text"><![CDATA[搭建ip代理池(简易版)推荐两个scrapy代理的项目第一个是免费的代理插件，无需付费https://github.com/aivarsk/scrapy-proxies 第二个是需要付费的代理插件https://github.com/scrapy-plugins/scrapy-crawlera 撸视频的时候学到的代理池实例获取西刺代理的代理列表并存入mysql数据库：1234567891011121314151617181920212223242526272829303132333435363738def crawl_xici(): headers = &#123;"User-Agent":"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/68.0.3440.106 Safari/537.36"&#125; for i in range(3411): res = requests.get('http://www.xicidaili.com/nn/&#123;&#125;'.format(i), headers = headers) ip_list = [] selector = Selector(text=res.text) all_trs = selector.css("#ip_list tr") for tr in all_trs[1:]: speed_str = tr.css(".bar::attr(title)").extract()[0] if speed_str: speed = float(speed_str.split("秒")[0]) all_texts = tr.css("td::text").extract() ip = all_texts[0] port = all_texts[1] proxy_type = all_texts[5] ip_list.append((ip,port,proxy_type,speed)) # print(ip_list) # for ip_info in ip_list: # cursor.execute( # "insert into proxy_ip(ip, port, speed, proxy_type) VALUES ('&#123;0&#125;', '&#123;1&#125;', &#123;2&#125;, '&#123;3&#125;')".format( # ip_info[0], ip_info[1], ip_info[3], ip_info[2] # ) # ) # conn.commit() for ip_info in ip_list: insert_sql = """ insert into proxy_ip(ip, port, speed, proxy_type) VALUES (%s, %s, %s, %s) ON DUPLICATE KEY UPDATE ip=VALUES(ip), port=VALUES(port), speed=VALUES(speed), proxy_type=VALUES(proxy_type) """ params = (ip_info[0], ip_info[1], ip_info[3], ip_info[2]) cursor.execute(insert_sql,params) conn.commit() # print("入库成功") 定义随机获取ip的类方法(包括删除无效代理)1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556class GetIP(object): # 从数据库删除ip def delete_ip(self, ip): delete_sql = """ delete from proxy_ip WHERE ip=&#123;0&#125; """.format(ip) cursor.execute(delete_sql) conn.commit() print("删除成功") return True # 验证ip def judge_ip(self, ip, port): http_url = "http://www.baidu.com" proxy_url = "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) try: proxy_dict = &#123; "http":proxy_url &#125; res = requests.get(http_url, proxies=proxy_dict) except Exception as e: print("invalid ip and port") self.delete_ip(ip) return False else: code = res.status_code if code &gt;= 200 and code &lt; 300: print("effective ip") return True else: print("invalid ip and port") self.delete_ip(ip) return False # 从数据库获取随机ip def get_random_ip(self): select_sql = """ SELECT ip,port from proxy_ip ORDER BY RAND() LIMIT 1 """ result = cursor.execute(select_sql) for ip_info in cursor.fetchall(): ip = ip_info[0] port = ip_info[1] judge_re = self.judge_ip(ip,port) if judge_re: return "http://&#123;0&#125;:&#123;1&#125;".format(ip, port) else: return self.get_random_ip()# crawl_xici()if __name__ == '__main__': get_ip = GetIP() get_ip.get_random_ip() 写入middleware文件中1234567# 使用前要记得在setting中添加RadomProxyMiddlewarefrom tools.crawl_xici_ip import GetIP# 随机ip代理class RadomProxyMiddleware(object): def process_request(self, request, spider): get_ip = GetIP() request.meta['proxy'] = get_ip.get_random_ip() 自定义pipline的使用实例pipline存储json(自定义json存储)1234567891011import codecsclass JsonWithEncodingPipeline(object): #自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item def spider_closed(self, spider): self.file.close() pipline存储json(使用scrapy自带的组件)123456789101112131415from scrapy.exporters import JsonItemExporterclass JsonExporterPipleline(object): #调用scrapy提供的json export导出json文件 def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item pipline中的存储mysql(阻塞)123456789101112131415import MySQLdbimport MySQLdb.cursorsclass MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('192.168.0.106', 'root', 'root', 'article_spider', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) self.conn.commit() pipline中存储mysql(异步)12345678910111213141516171819202122232425262728293031323334353637383940414243class MysqlTwistedPipline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USER"], passwd = settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): # 处理异步插入的异常 print (failure) def do_insert(self, cursor, item): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) # 想使用下面的插入方法需要在item中定义insert_sql # def do_insert(self, cursor, item): # #执行具体的插入 # #根据不同的item 构建不同的sql语句并插入到mysql中 # insert_sql, params = item.get_insert_sql() # print (insert_sql, params) # cursor.execute(insert_sql, params) 如何在scrapy中随机切换UA？随机UA下载中间件(初始版)setting文件：123DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543,&#125; middlewares文件：1234567891011121314from fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 数据存错怎么办？将redis数据库导入mongodb数据库1234567891011121314151617181920212223242526import json, redis, pymongodef main(): # 指定Redis数据库信息 rediscli = redis.StrictRedis(host='127.0.0.1', port=6379, db=0) # 指定MongoDB数据库信息 mongocli = pymongo.MongoClient(host='localhost', port=27017) # 创建数据库名 db = mongocli['sina'] # 创建表名 sheet = db['sina_items'] offset = 0 while True: # FIFO模式为 blpop，LIFO模式为 brpop，获取键值 source, data = rediscli.blpop(["sinainfospider_redis:items"]) item = json.loads(data.decode("utf-8")) sheet.insert(item) offset += 1 print(offset) try: print("Processing: %s " % item) except KeyError: print("Error procesing: %s" % item)if __name__ == '__main__': main() 将redis数据存入mysql数据库123456789101112131415161718192021222324import redis, json, timefrom pymysql import connect# redis数据库链接redis_client = redis.StrictRedis(host="127.0.0.1", port=6379, db=0)# mysql数据库链接mysql_client = connect(host="127.0.0.1", user="root", password="mysql", database="sina", port=3306, charset='utf8')cursor = mysql_client.cursor()i = 1while True: print(i) time.sleep(1) source, data = redis_client.blpop(["sinainfospider_redis:items"]) item = json.loads(data.decode()) print("source===========", source) print("item===========", item) sql = "insert into sina_items(parent_url,parent_title,sub_title,sub_url,sub_file_name,son_url,head,content,crawled,spider) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)" params = [item["parent_url"], item["parent_title"], item["sub_title"], item["sub_url"], item["sub_file_name"], item["son_url"], item["head"], item["content"], item["crawled"], item["spider"], ] cursor.execute(sql, params) mysql_client.commit() i += 1]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[随机切换UA中间件实例]]></title>
    <url>%2F2018%2F11%2F29%2Fua-downloadmiddler%2F</url>
    <content type="text"><![CDATA[随机UA下载中间件(初始版)setting文件：123DOWNLOADER_MIDDLEWARES = &#123; 'ArticleSpider.middlewares.RandomUserAgentMiddleware': 543,&#125; middlewares文件：1234567891011121314from fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): return getattr(self.ua, self.ua_type) request.headers.setdefault('User-Agent', get_ua()) 随机UA、ip下载中间件(升级版)setting文件：1234DOWNLOADER_MIDDLEWARES = &#123; # 'ArticleSpider.middlewares.MyCustomDownloaderMiddleware': 543, 'ArticleSpider.middlewares.rotate_user_agent.RandomUserAgentMiddleware': 400,&#125; middlewares文件：1234567891011121314151617181920212223242526272829from fake_useragent import UserAgentclass RandomUserAgentMiddleware(object): def __init__(self, crawler): super(RandomUserAgentMiddleware, self).__init__() self.ua = UserAgent() self.per_proxy = crawler.settings.get('RANDOM_UA_PER_PROXY', False) self.ua_type = crawler.settings.get('RANDOM_UA_TYPE', 'random') self.proxy2ua = &#123;&#125; @classmethod def from_crawler(cls, crawler): return cls(crawler) def process_request(self, request, spider): def get_ua(): '''Gets random UA based on the type setting (random, firefox…)''' return getattr(self.ua, self.ua_type) if self.per_proxy: proxy = request.meta.get('proxy') if proxy not in self.proxy2ua: self.proxy2ua[proxy] = get_ua() logger.debug('Assign User-Agent %s to Proxy %s' % (self.proxy2ua[proxy], proxy)) request.headers.setdefault('User-Agent', self.proxy2ua[proxy]) else: ua = get_ua() request.headers.setdefault('User-Agent', get_ua())]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy基础(三)]]></title>
    <url>%2F2018%2F11%2F09%2Fnumpy3%2F</url>
    <content type="text"><![CDATA[numpy读取/写入数组数据在我们使用numpy处理了数据之后，可以将数组保存为保存为Numpy专用的二进制格式，当我们这样操作之后，就不能用notepad++等打开看了（乱码）。 np.load和np.save是读写磁盘数组数据的两个主要函数，默认情况下，数组是以未压缩的原始二进制格式保存在扩展名为.npy的文件中。 存储数组数据(npy)123ar = np.random.rand(5,5)print(ar)np.save('arraydata.npy', ar) 读取数组数据(npy)12ar_load =np.load('arraydata.npy')print(ar_load) ####numpy读取/写入文本数据除了保存为npy文件外，我们还可以将数据保存为txt格式的文本文件,np可以读写1维和2维的数组同时可以指定各种分隔符、针对特定列的转换器函数、需要跳过的行数等。 存储文本数据(txt)这里需要注意的是关于文件保存的默认分隔符是空格，缺省按照’%.18e’格式保存数据。123456ar = np.random.rand(5,5)np.savetxt('array.txt',ar, delimiter=',')# 改为以整数形式保存np.savetxt("a.txt",a,fmt="%d",delimiter=",")# np.savetxt(fname, X, fmt='%.18e', delimiter=' ', newline='\n', header='', footer='', comments='# ')：存储为文本txt文件 读取文本数据(txt)同样这里要注意的是读取也要标注分隔符的值，如果与保存时不同会报错。12345678ar_loadtxt = np.loadtxt('array.txt', delimiter=',')print(ar_loadtxt)&gt;&gt;&gt;[[ 0.28280684 0.66188985 0.00372083 0.54051044 0.68553963] [ 0.9138449 0.37056825 0.62813711 0.83032184 0.70196173] [ 0.63438739 0.86552157 0.68294764 0.2959724 0.62337767] [ 0.67411154 0.87678919 0.53732168 0.90366896 0.70480366] [ 0.00936579 0.32914898 0.30001813 0.66198967 0.04336824]]]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy-redis 详解]]></title>
    <url>%2F2018%2F11%2F09%2Fscrapy-redis%2F</url>
    <content type="text"><![CDATA[什么是scrapy-redisredis-based components for scrapy ###为什么要学习scrapy-redis？scrapy redis能够实现requests去重，爬虫持久化，能够轻松实现分布式。 scrapy-redis 详解流程图 scrapy-redis example-projectscrapy-redis的源码中提供了scrapy-redis的示例项目,我们下载下来学习一下。 https://github.com/rmax/scrapy-redis/tree/master/example-project 在example-project中有三个demo,分别是dmoz,myspider_redis,以及mycrawler_redis。 本次主要是对dmoz这个demo进行学习和实战练习。 dmoz spider文件解析1234567891011121314151617181920212223from scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Ruleclass DmozSpider(CrawlSpider): """Follow categories and extract links.""" name = 'dmoz' allowed_domains = ['dmoztools.net'] start_urls = ['http://dmoztools.net/'] rules = [ Rule(LinkExtractor( restrict_css=('.top-cat', '.sub-cat', '.cat-item') ), callback='parse_directory', follow=True), ] def parse_directory(self, response): for div in response.css('.title-and-desc'): yield &#123; 'name': div.css('.site-title::text').extract_first(), 'description': div.css('.site-descr::text').extract_first().strip(), 'link': div.css('a::attr(href)').extract_first(), &#125; 可以看到,dmoz项目和我们平时创建的scrapy项目并没有太大的区别,之所以能够实现持久化爬虫主要的不同之处在setting中设置了去重类和scheduler队列。 dmoz setting文件解析上面提到的setting中设置了去重类和scheduler队列的操作主要就是在setting文件中添加下面这些代码。1234567# 去重类--指定哪个去重方法给request对象去重DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"# 队列--指定scheduler队列，调度器内存的是待爬取链接和已爬取对象指纹。SCHEDULER = "scrapy_redis.scheduler.Scheduler"# 队列内容是否持久化保存--为False的时候，关闭redis的时候清空redisSCHEDULER_PERSIST = TrueREDIS_URL="redis://127.0.0.1:6379" dmoz redis 数据库存取项我们运行一下这个示例项目,并打开redis数据库,查看爬取到的结果。redis数据库中出现以下三个键，分别是： dmoz request： 待爬取项(先把爬取对象序列化存入数据库，再反序列化成爬取对，Scheduler队列，存放的待请求的request对象，获取的过程是pop操作，即获取一个会去除一个) dmoz items：爬取的内容(通过scrapy_redis.pipelines.RedisPipeline保存,屏蔽之后可以实现自定义对象存取位置,存放的获取到的item信息，在pipeline中开启RedisPipeline才会存入) dmoz dumpfilter：抓到过的request对象指纹(指纹集合，存放的是已经进入scheduler队列的request对象的指纹，指纹默认由请求方法，url和请求体组成) dumpfilter的数量减去request的数量是已经抓爬取过的数量 关闭redispipeline之后,redis数据库中数据量变化： dmoz:requests 有变化(变多或者变少或者不变) dmoz:dupefilter 变多 dmoz:items 不变 redispipeline中仅仅实现了item数据存储到redis的过程,我们可以新建一个pipeline（或者修改默认的ExamplePipeline）,让数据存储到任意地方,但是权重应该小于redis存储的pipline。 scrapy-redis 源码详解scrapy redis 如何生成指纹的？1234import hashlibf = hashlib.hsa1()f.update(url.encode())f.hexdigest() #####scrapy-redis 判断request对象是否入队1234567891011def enqueue_request(self, request): if not request.dont_filter and self.df.request_seen(request): # dont_filter=False Ture True request指纹已经存在 #不会入队 # dont_filter=False Ture False request指纹已经存在 全新的url #会入队 # dont_filter=Ture False #会入队 self.df.log(request, self.spider) return False if self.stats: self.stats.inc_value('scheduler/enqueued/redis', spider=self.spider) self.queue.push(request) return True dont_filter = True ,构造请求的时候，把dont_filter置为True，该url会被反复抓取（url地址对应的内容会更新的情况） 一个全新的url地址被抓到的时候，构造request请求 url地址在start_urls中的时候，会入队，不管之前是否请求过 构造start_url地址的请求时候，dont_filter = True scrapy-redis如何去重12345fp = hashlib.sha1()fp.update(to_bytes(request.method)) #请求方法fp.update(to_bytes(canonicalize_url(request.url))) #请求链接fp.update(request.body or b'') # 请求体return fp.hexdigest() 使用sha1加密request得到指纹 把指纹存在redis的集合中 下一次新来一个request，同样的方式生成指纹，判断指纹是否存在reids的集合中 判断数据是否存在redis的集合中，不存在插入12added = self.server.sadd(self.key, fp)return added != 0 scrapy-redis实战京东图书爬取结果截图 页面分析分析分类聚合页打开待爬取页面：https://book.jd.com/booksort.html 如下图： 查看页面源代码,发现待爬取的内容存在其中,所以我们可以通过分析源码写出提取相应字段的xpath。12345678910111213141516def parse(self, response): dl_list = response.xpath("//div[@class='mc']/dl/dt") for dl in dl_list: item = JdbookspiderItem() item['book_sort'] = dl.xpath("./a/text()").extract_first() em_list = dl.xpath("./following-sibling::dd/em") for em in em_list: item['book_cate'] = em.xpath("./a/text()").extract_first() item['book_cate_url'] = em.xpath("./a/@href").extract_first() if item['book_cate_url'] is not None: item['book_cate_url'] = 'https:' + item['book_cate_url'] yield scrapy.Request( item['book_cate_url'], callback=self.parse_cate_url, meta=&#123;"item": deepcopy(item)&#125; ) 通过抓取分类页面分类链接,我们可以爬取到分类下的书籍列表页,这个页面包含书籍的全部信息,同样是可以使用xpath解析出来的。 分析书籍列表页通过分析列表页的请求,可以发下列表页的请求重定向到了一个新的链接,所以我们只需要分析新请求链接的响应即可,scrapy可以自动帮我们执行重定向的操作。 分析书籍列表页,可以发现列表页除了价格字段外其余字段都可以在链接的响应中提取到。 所以我们只需要找到价格字段的请求,就可以爬取到书籍的全部字段了。我们可以直接在相应中查找价格以查看是否有相关的响应。 查找结果如上所示,根据我们查找到的结果,我试着请求一下对应的链接,很幸运这里返回的是json字符串。 根据以上分析我们可以写出相应的分析代码：1234567891011121314151617181920212223def parse_cate_url(self, response): item = response.meta["item"] li_list = response.xpath("//div[@id='plist']/ul/li") for li in li_list: item['book_img'] = li.xpath(".//div[@class='p-img']//img/@src").extract_first() if item['book_img'] is None: item['book_img'] = li.xpath(".//div[@class='p-img']//img/@data-lazy-img").extract_first() item['book_img'] = "https:" + item['book_img'] if item['book_img'] is not None else None item['book_name'] = li.xpath(".//div[@class='p-name']/a/em/text()").extract_first().strip() item['book_author'] = li.xpath(".//span[@class='author_type_1']/a/text()").extract_first() item['publish_time'] = li.xpath(".//span[@class='p-bi-date']/text()").extract_first().strip() item['book_store'] = li.xpath(".//span[@class='p-bi-store']/a/@title").extract_first().strip() item['book_sku'] = li.xpath("./div/@data-sku").extract_first() yield scrapy.Request( 'https://p.3.cn/prices/mgets?skuIds=J_&#123;&#125;'.format(item['book_sku']), callback=self.parse_book_price, meta=&#123;"item": deepcopy(item)&#125; )def parse_book_price(self, response): item = response.meta["item"] item['book_price'] = json.loads(response.body.decode())[0]["op"] yield item 建下一页请求这里就比较简单,对比最后一页与其他页,发现最后一页是没有下一页链接的,根据以上分析可以成功构建相应代码：123456789# 下一页地址构建next_url = response.xpath("//a[@class='pn-next']/@href")if next_url: next_url = urllib.parse.join(response.url, next_url) yield scrapy.Request( next_url, callback=self.parse_cate_url, meta=&#123;"item": item&#125; ) 数据入库1234567class JdbookspiderPipeline(object): def process_item(self, item, spider): if isinstance(item, JdbookspiderItem): print(item) collection.insert(dict(item)) return item 实现持久化爬虫在setting文件中添加去重类和scheduler队列,同时修改redis数据库链接,并保证数据库是可用的。12345678910DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"SCHEDULER = "scrapy_redis.scheduler.Scheduler"SCHEDULER_PERSIST = TrueREDIS_URL = "redis://127.0.0.1:6379"# 同时为保证能够把数据同时存储到mongodb中我们还要设置相关piplineITEM_PIPELINES = &#123; 'jdbookSpider.pipelines.JdbookspiderPipeline': 500, 'scrapy_redis.pipelines.RedisPipeline': 400&#125; 以上就是主要部分的代码,全部代码已经上传github,点击阅读原文即可获取。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[带密码的代理怎么使用]]></title>
    <url>%2F2018%2F10%2F29%2Fpassword_proxy_use%2F</url>
    <content type="text"><![CDATA[使用代理处理器1234567891011import urllib.requestdef money_proxy_use(): #第一种方式付费代理发送请求 #1.代理ip money_proxy =&#123;"http":"username:pwd@192.168.12.11:8080"&#125; #2.代理的处理器 proxy_handler=urllib.request.ProxyHandler(money_proxy) #3.通过处理器创建opener opener = urllib.request.build_opener(proxy_handler) #4.open发送请求 opener.open("http://www.baidu.com") 使用密码管理器12345678910111213141516use_name = "abcname"pwd = "123456"proxy_money = "123.158.63.130:8888"def money_proxy_use(): #2.创建密码管理器,添加用户名和密码 password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm() #uri定位 uri&gt;url #url 资源定位符 password_manager.add_password(None,proxy_money,use_name,pwd) #3.创建可以验证代理ip的处理器 handle_auth_proxy = urllib.request.ProxyBasicAuthHandler(password_manager) #4.根据处理器创建opener opener_auth = urllib.request.build_opener(handle_auth_proxy) #5.发送请求 response = opener_auth.open("http://www.baidu.com") print(response.read()) 认证登录12345678910111213141516171819202122import urllib.requestdef auth_nei_wang(): #1.用户名密码 user = "admin" pwd = "adimin123" nei_url = "http://192.168.179.66" #2.创建密码管理器 pwd_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm() pwd_manager.add_password(None,nei_url,user,pwd) #创建认证处理器(requests) auth_handler = urllib.request.HTTPBasicAuthHandler(pwd_manager) opener = urllib.request.build_opener(auth_handler) response = opener.open(nei_url) print(response)auth_nei_wang()]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy安装报错与setting文件解读]]></title>
    <url>%2F2018%2F10%2F29%2Fscrapy_an_zhuang_and_setting_file%2F</url>
    <content type="text"><![CDATA[安装scrapy报错报错ModuleNotFoundError: No module named &#39;win32api&#39;错误 解决方案：pip install pypiwin32或者pip install -i https://pypi.douban.com/simple pypiwin32 setting文件详解123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108# -*- coding: utf-8 -*-# Scrapy settings for tencenthr project## For simplicity, this file contains only settings considered important or# commonly used. You can find more settings consulting the documentation:## https://doc.scrapy.org/en/latest/topics/settings.html# https://doc.scrapy.org/en/latest/topics/downloader-middleware.html# https://doc.scrapy.org/en/latest/topics/spider-middleware.html#爬虫名字BOT_NAME = 'tencenthr' #爬虫的位置SPIDER_MODULES = ['tencenthr.spiders'] #新建爬虫的位置NEWSPIDER_MODULE = 'tencenthr.spiders'# Crawl responsibly by identifying yourself (and your website) on the user-agent#浏览器的用户标识USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.62 Safari/537.36'# Obey robots.txt rules#是否遵守robot协议，默认是遵守ROBOTSTXT_OBEY = True# Configure maximum concurrent requests performed by Scrapy (default: 16)#设置最大并发请求，默认是16#CONCURRENT_REQUESTS = 32# Configure a delay for requests for the same website (default: 0)# See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay# See also autothrottle settings and docs#下载延时#DOWNLOAD_DELAY = 3# The download delay setting will honor only one of:#每个域名的最大并发请求数#CONCURRENT_REQUESTS_PER_DOMAIN = 16#每个ip的最大并发请求数#CONCURRENT_REQUESTS_PER_IP = 16# Disable cookies (enabled by default)#默认是开启的，即请求下一个URL地址带上cookie#COOKIES_ENABLED = False# Disable Telnet Console (enabled by default)# 控制台插件#TELNETCONSOLE_ENABLED = False# Override the default request headers:#默认请求头#DEFAULT_REQUEST_HEADERS = &#123;# 'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',# 'Accept-Language': 'en',#&#125;# Enable or disable spider middlewares# See https://doc.scrapy.org/en/latest/topics/spider-middleware.html#爬虫中间件和权重#SPIDER_MIDDLEWARES = &#123;# 'tencenthr.middlewares.TencenthrSpiderMiddleware': 543,#&#125;# Enable or disable downloader middlewares# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#下载中间件和权重#DOWNLOADER_MIDDLEWARES = &#123;# 'tencenthr.middlewares.TencenthrDownloaderMiddleware': 543,#&#125;# Enable or disable extensions# See https://doc.scrapy.org/en/latest/topics/extensions.html#插件和权重#EXTENSIONS = &#123;# 'scrapy.extensions.telnet.TelnetConsole': None,#&#125;# Configure item pipelines# See https://doc.scrapy.org/en/latest/topics/item-pipeline.html#指定pipline,和权重ITEM_PIPELINES = &#123; 'tencenthr.pipelines.TencenthrPipeline': 300,&#125;LOG_LEVEL = 'WARNING'# Enable and configure the AutoThrottle extension (disabled by default)# See https://doc.scrapy.org/en/latest/topics/autothrottle.html#自动限速#AUTOTHROTTLE_ENABLED = True# The initial download delay#AUTOTHROTTLE_START_DELAY = 5# The maximum download delay to be set in case of high latencies#AUTOTHROTTLE_MAX_DELAY = 60# The average number of requests Scrapy should be sending in parallel to# each remote server#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0# Enable showing throttling stats for every response received:#AUTOTHROTTLE_DEBUG = False# Enable and configure HTTP caching (disabled by default)#http缓存# See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings#HTTPCACHE_ENABLED = True#HTTPCACHE_EXPIRATION_SECS = 0#HTTPCACHE_DIR = 'httpcache'#HTTPCACHE_IGNORE_HTTP_CODES = []#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage']]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask基础(4)-响应Response]]></title>
    <url>%2F2018%2F10%2F14%2Fflask-xiangying%2F</url>
    <content type="text"><![CDATA[关于响应（Response）视图函数的返回值会被自动转换为一个响应对象，Flask的转换逻辑如下： 如果返回的是一个合法的响应对象，则直接返回。 如果返回的是一个字符串，那么Flask会重新创建一个werkzeug.wrappers.Response对象，Response将该字符串作为主体，状态码为200，MIME类型为text/html，然后返回该Response对象。 如果返回的是一个元组，元祖中的数据类型是(response,status,headers)。status值会覆盖默认的200状态码，headers可以是一个列表或者字典，作为额外的消息头。 如果以上条件都不满足，Flask会假设返回值是一个合法的WSGIt应用程序，并通过Response.force_type(rv,request.environ)转换为一个请求对象。 以下将用例子来进行说明： 第一个例子：直接使用Response创建：123456from werkzeug.wrappers import Response @app.route('/about/') def about(): resp = Response(response='about page',status=200,content_type='text/html;charset=utf-8') return resp 第二个例子：可以使用make_response函数来创建Response对象，这个方法可以设置额外的数据，比如设置cookie，header信息等：12345from flask import make_response@app.route('/about/')def about(): return make_response('about page') 第三个例子：通过返回元组的形式：123@app.errorhandler(404)def not_found(): return 'not found',404 第四个例子：自定义响应。自定义响应必须满足三个条件： 必须继承自Response类。 实现类方法force_type(cls,rv,environ=None)。 必须指定app.response_class为你自定义的Response 以下将用一个例子来进行讲解，Restful API都是通过JSON的形式进行传递，如果你的后台跟前台进行交互，所有的URL都是发送JSON数据，那么此时你可以自定义一个叫做JSONResponse的类来代替Flask自带的Response类：12345678910111213141516171819202122from flask import Flask,jsonify from werkzeug.wrappers import Response app = Flask(__name__) class JSONResponse(Response): default_mimetype = 'application/json' @classmethod def force_type(cls,response,environ=None): if isinstance(response,dict): response = jsonify(response) return super(JSONResponse,cls).force_type(response,environ) app.response_class = JSONResponse @app.route('/about/') def about(): return &#123;"message":"about page"&#125; if __name__ == '__main__': app.run(host='0.0.0.0',port=8000) 此时如果你访问/about/这个URL,那么在页面中将会显示:123&#123; "message": "about page"&#125; 注意以上例子，如果不写app.response_class = JSONResponse，将不能正确的将字典返回给客户端。因为字典不在Flask的响应类型支持范围中，那么将调用app.response_class这个属性的force_type类方法，而app.response_class的默认值为Response，因此会调用Response.force_class()这个类方法，他有一个默认的算法转换成字符串，但是这个算法不能满足我们的需求。因此，我们要设置app.response_class=JSONResponse，然后重写JSONResponse中的force_type类方法，在这个方法中将字典转换成JSON格式的字符串后再返回。]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask基础(3)-URL与视图]]></title>
    <url>%2F2018%2F10%2F13%2FFlask-url-and-view%2F</url>
    <content type="text"><![CDATA[URL与函数的映射从新建的文件，我们已经看到，一个URL要与执行函数进行映射，使用的是`@app.route装饰器。@app.route装饰器中，可以指定URL的规则来进行更加详细的映射，比如现在要映射一个文章详情的URL，文章详情的URL是/article/id/`，id有可能为1、2、3…,那么可以通过以下方式：123@app.route('/article/&lt;id&gt;/')def article(id): return '%s article detail' % id 其中，尖括号是固定写法，语法为&lt;variable_name&gt;，variable_name默认的数据类型是字符串。如果需要指定类型，则要写成&lt;converter:variable_name&gt;，其中converter就是类型名称，可以有以下几种： string: 默认的数据类型，接受没有任何斜杠“\/”的文本。 int: 接受整形。 float: 接受浮点类型。 path： 和string的类似，但是接受斜杠。 uuid： 只接受uuid字符串。 any：可以指定多种路径1234@app.route('/&lt;any(article,blog):url_path&gt;/') def item(url_path): return url_path# 以上例子中，item这个函数可以接受两个URL,一个是/article/，另一个是/blog/。并且，一定要传url_path参数，当然这个url_path的名称可以随便。 如果不想定制子路径来传递参数，也可以通过传统的?=的形式来传递参数，例如：/article?id=xxx，这种情况下，可以通过request.args.get(&#39;id&#39;)来获取id的值。如果是post方法，则可以通过request.form.get(&#39;id&#39;)来进行获取。 构造URL（url_for）一般我们通过一个URL就可以执行到某一个函数。如果反过来，我们知道一个函数，怎么去获得这个URL呢？url_for函数就可以帮我们实现这个功能。url_for()函数接收两个及以上的参数，他接收函数名作为第一个参数，接收对应URL规则的命名参数，如果还出现其他的参数，则会添加到URL的后面作为查询参数。 通过构建URL的方式而选择直接在代码中拼URL的原因有两点： 将来如果修改了URL，但没有修改该URL对应的函数名，就不用到处去替换URL了。 url_for()函数会转义特殊字符和Unicode数据，这些工作都不需要我们自己处理。 自定义URL转换器刚刚在URL映射的时候，我们看到了Flask内置了几种数据类型的转换器，比如有int/string等。如果Flask内置的转换器不能满足你的需求，此时你可以自定义转换器。自定义转换器，需要满足以下几个条件： 转换器是一个类，且必须继承自werkzeug.routing.BaseConverter。 在转换器类中，实现to_python(self,value)方法，这个方法的返回值，将会传递到view函数中作为参数。 在转换器类中，实现to_url(self,values)方法，这个方法的返回值，将会在调用url_for函数的时候生成符合要求的URL形式。 URL唯一Flask的URL规则是基于Werkzeug的路由模块。这个模块的思想是基于Apache以及更早的HTTP服务器的主张，希望保证优雅且唯一的URL。 123@app.route('/projects/')def projects(): return 'project page' 上述例子中，当访问一个结尾不带斜线的URL会被重定向到带斜线的URL上去。这样有助于避免搜索引擎搜索同一个页面两次。123@app.route('/about')def about(): return 'about page' 以上例子中，当访问带斜线的URL（\/about\/）会产生一个404（”Not Found”）错误。 ####指定HTTP方法在@app.route()中可以传入一个关键字参数methods来指定本方法支持的HTTP方法，默认只响应GET请求，看以下例子：123@app.route('/login/',methods=['GET','POST'])def login(): return 'login' 以上装饰器将让login的URL既能支持GET又能支持POST。 页面跳转和重定向重定向分为永久性重定向和暂时性重定向，在页面上体现的操作就是浏览器会从一个页面自动跳转到另外一个页面。比如用户访问了一个需要权限的页面，但是该用户当前并没有登录，因此我们应该给他重定向到登录页面。 永久性重定向：http的状态码是301，多用于旧网址被废弃了要转到一个新的网址确保用户的访问，最经典的就是京东网站，你输入www.jingdong.com的时候，会被重定向到www.jd.com，因为jingdong.com这个网址已经被废弃了，被改成jd.com，所以这种情况下应该用永久重定向。 暂时性重定向：http的状态码是302，表示页面的暂时性跳转。比如访问一个需要权限的网址，如果当前用户没有登录，应该重定向到登录页面，这种情况下，应该用暂时性重定向。 在flask中，重定向是通过flask.redirect(location,code=302)这个函数来实现的，location表示需要重定向到的URL，应该配合之前讲的url_for()函数来使用，code表示采用哪个重定向，默认是302也即暂时性重定向，可以修改成301来实现永久性重定向。 以下来看一个例子，关于在flask中怎么使用重定向：123456789101112131415161718from flask import Flask,url_for,redirect app = Flask(__name__) app.debug = True @app.route('/login/',methods=['GET','POST']) def login(): return 'login page' @app.route('/profile/',methods=['GET','POST']) def profile(): name = request.args.get('name') if not name: # 如果没有name，说明没有登录，重定向到登录页面 return redirect(url_for('login')) else: return name]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask基础(2)-项目设置]]></title>
    <url>%2F2018%2F10%2F12%2Fflask-xiangmu%2F</url>
    <content type="text"><![CDATA[为什么学习FlaskFlask的灵活度非常之高，他不会帮你做太多的决策，即使做已经帮你做出选择，你也能非常容易的更换成你需要的，比如： 使用Flask开发数据库的时候，具体是使用SQLAlchemy还是MongoEngine或者是不用ORM而直接基于MySQL-Python这样的底层驱动进行开发都是可以的，选择权完全掌握在你自己的手中。区别于Django，Django内置了非常完善和丰富的功能，并且如果你想替换成你自己想要的，要么不支持，要么非常麻烦。 把默认的Jinija2模板引擎替换成Mako引擎或者是其他模板引擎都是非常容易的。新建Flask项目我们使用Pycharm专业版，新建第一个Flask项目： 默认生成代码解析1234567891011121314# 从flask框架中导入Flask类from flask import Flask# 传入__name__初始化一个Flask实例app = Flask(__name__)# app.route装饰器映射URL和执行的函数。这个设置将根URL映射到了hello_world函数上@app.route('/')def hello_world(): return 'Hello World!'if __name__ == '__main__': # 运行本项目，host=0.0.0.0可以让其他电脑也能访问到该网站，port指定访问的端口。默认的host是127.0.0.1，port为5000，开启debug模式 app.run(host='0.0.0.0',port=9000,debug=True) 注意：app.run这种方式只适合于开发，如果在生产环境中，应该使用Gunicorn或者uWSGI来启动。如果是在终端运行的，可以按ctrl+c来让服务停止。 Flask项目配置设置为DEBUG模式默认情况下flask不会开启DEBUG模式，开启DEBUG模式后，flask会在每次保存代码的时候自动的重新载入代码，并且如果代码有错误，会在终端进行提示。开启DEBUG模式有三种方式： 直接在应用对象上设置： 12app.debug = Trueapp.run() 在执行run方法的时候，传递参数进去： 1app.run(debug=True) 在config属性中设置：1234567app.config.update(DEBUG=True)如果一切正常，会在终端打印以下信息：* Restarting with stat* Debugger is active!* Debugger pin code: 294-745-044* Running on http://0.0.0.0:9000/ (Press CTRL+C to quit) 需要注意的是，只能在开发环境下开启DEBUG模式，因为DEBUG模式会带来非常大的安全隐患。 另外，在开启了DEBUG模式后，当程序有异常而进入错误堆栈模式，你第一次点击某个堆栈想查看变量值的时候，页面会弹出一个对话框，让你输入PIN值，这个PIN值在你启动的时候就会出现，比如在刚刚启动的项目中的PIN值为xxx-xxx-xxx，你输入这个值后，Werkzeug会把这个PIN值作为cookie的一部分保存起来，并在8小时候过期，8小时以内不需要再输入PIN值。这样做的目的是为了更加的安全，让调试模式下的攻击者更难攻击到本站。 配置文件Flask项目的配置，都是通过app.config对象来进行配置的。比如要配置一个项目处于DEBUG模式下，那么可以使用app.config[‘DEBUG] = True来进行设置，那么Flask项目将以DEBUG模式运行。在Flask项目中，有四种方式进行项目的配置： 直接硬编码： 12app = Flask(__name__)app.config['DEBUG'] = True 因为app.config是flask.config.Config的实例，而Config类是继承自dict，因此可以通过update方法： 1234app.config.update( DEBUG=True, SECRET_KEY='...') 如果你的配置项特别多，你可以把所有的配置项都放在一个模块中，然后通过加载模块的方式进行配置，假设有一个settings.py模块，专门用来存储配置项的，此时你可以通过app.config.from_object()方法进行加载，并且该方法既可以接收模块的的字符串名称，也可以模块对象： 12345# 1. 通过模块字符串app.config.from_object('settings')# 2. 通过模块对象import settingsapp.config.from_object(settings) 也可以通过另外一个方法加载，该方法就是app.config.from_pyfile()，该方法传入一个文件名，通常是以.py结尾的文件，但也不限于只使用.py后缀的文件： 12app.config.from_pyfile('settings.py',silent=True)# silent=True表示如果配置文件不存在的时候不抛出异常，默认是为False，会抛出异常。]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Flask基础(1)-web基础]]></title>
    <url>%2F2018%2F10%2F11%2Fflask-web%2F</url>
    <content type="text"><![CDATA[url详解URL是Uniform Resource Locator的简写，统一资源定位符。 一个URL由以下几部分组成： scheme://host:port/path/?query-string=xxx#anchor scheme：代表的是访问的协议，一般为http或者https以及ftp等。 host：主机名，域名，比如www.baidu.com。 port：端口号。当你访问一个网站的时候，浏览器默认使用80端口。 path：查找路径。比如：www.jianshu.com/trending/now，后面的trending/now就是path。 query-string：查询字符串，比如：www.baidu.com/s?wd=python，后面的wd=python就是查询字符串。 anchor：锚点，后台一般不用管，前端用来做页面定位的。注意：URL中的所有字符都是ASCII字符集，如果出现非ASCII字符，比如中文，浏览器会进行编码再进行传输。 web服务器和应用服务器以及web应用框架 web服务器：负责处理http请求，响应静态文件，常见的有Apache，Nginx以及微软的IIS. 应用服务器：负责处理逻辑的服务器。比如php、python的代码，是不能直接通过nginx这种web服务器来处理的，只能通过应用服务器来处理，常见的应用服务器有uwsgi、tomcat等。 web应用框架：一般使用某种语言，封装了常用的web功能的框架就是web应用框架，flask、Django以及Java中的SSH框架都是web应用框架。 Content-type和Mime-type的作用和区别两者都是指定服务器和客户端之间传输数据的类型，区别如下： Content-type：既可以指定传输数据的类型，也可以指定数据的编码类型，例如：text/html;charset=utf-8 Mime-type：不能指定传输的数据编码类型。例如：text/html 常用的数据类型如下： text/html（默认的，html文件） text/plain（纯文本） text/css（css文件） text/javascript（js文件） application/x-www-form-urlencoded（普通的表单提交） multipart/form-data（文件提交） application/json（json传输） application/xml（xml文件）]]></content>
      <categories>
        <category>Flask</category>
      </categories>
      <tags>
        <tag>Flask</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy基础(二)]]></title>
    <url>%2F2018%2F10%2F09%2Fnumpy2%2F</url>
    <content type="text"><![CDATA[Anaconda的基本用法按照上篇文章，相信大家都安装好了Anaconda，有朋友在留言区留言希望出一篇关于Anaconda的使用教程，其实Anaconda的基本使用非常简单，基本无需教程。 在windows下安装好Anaconda后，在所有程序中可以看到Anaconda下有以下几个组件： Anaconda Navigator：用于管理工具包和环境的图形界面。 Anaconda Prompt：用于管理包和环境的命令行界面。 Jupyter Noterbook：基于Web的交互式计算环境，用于展示数据分析的过程，并且生成容易阅读的文档。 Spyder：Python集成开发环境，布局类似于Matlab。 我们学习主要使用的是第三个Jupyter Noterbook。 这里简单普及一下常用的Anaconda命令(虽然我也不经常用)。 查看软件版本号 12python --version #查看Python版本conda --version #查看conda的版 添加镜像 1conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/ 更新conda 1conda upgrade --all 查看已经安装的packages 12conda listconda install [package name] #安装package，安装在默认的Python环境中 新手入门建议只安装Anaconda，可以省去很多不必要的麻烦，以上就是Anaconda的基本使用，欢迎大家在留言区补充。 Numpy索引及切片纠正下上一篇的错误：12# 正确的导入方式import numpy as np numpy的索引方式和Python中的列表索引相似,这里主要介绍普通数组索引/切片和布尔型数组的索引/切片。 一维数组的索引/切片一维数组的索引和切片和Python中的列表相同，索引都是从0开始，切片都是左闭右开。123456789import numpy as npar = np.arange(20)# 输出ar的第4个值print(ar[3])# 输出ar的前四个值print(ar[:4])&gt;&gt;&gt;4[0 1 2 3] 多维数组的索引/切片二维数组可以理解为两个一维数组横向堆叠在一起，所只要分别取对应索引即可。1234567891011121314151617181920212223import numpy as npar = np.arange(16).reshape(4,4)# 二维数组索引遵照先行后列(有以下两种写法)# 选取第二行第二列的值print(ar[2][2])print(ar[2,2])# 二维数组切片# 取出前两行的值print(ar[:2])# 取出前两行后两列的值print(ar[:2,2:])&gt;&gt;&gt;[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]]1010[[0 1 2 3] [4 5 6 7]][[2 3] [6 7]] 三位数组的索引、切片的取值方式相当与二维数组的进化版。123456789101112131415161718192021import numpy as npar = np.arange(12).reshape(3,2,2)print(ar)# 三维数组索引遵照先维度后行再列print(ar[2][0][1])print(ar[2,0,1])# 切片# 获取第一个数组的第一行的第一列的数print(ar[:1,:1,:1])&gt;&gt;&gt;[[[ 0 1] [ 2 3]] [[ 4 5] [ 6 7]] [[ 8 9] [10 11]]][[[0]]]99 布尔型的索引及切片布尔型数组的使用是本片文章的重点。12345678# 简单展示一下布尔型的一维数组长啥样i = np.array([True,False,True])j = np.array([True,True,False,False])print(i)print(j)&gt;&gt;&gt;[ True False True][ True True False False] 而我们经常见到的是这样的：12345678910ar = np.arange(12).reshape(3,4)print(ar)print(ar&gt;5)&gt;&gt;&gt;[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11]] [[False False False False] [False False True True] [ True True True True]] 当我们需要筛选出ar中大于3的值，就可以使用布尔值进行筛选，如下：1234ar = np.arange(12).reshape(3,4)print(ar[ar&gt;3])&gt;&gt;&gt;[ 4 5 6 7 8 9 10 11] Numpy随机数均匀分布和正态分布以均匀分布和正态分布的方式生成随机数1234567891011# numpy.random.rand() 生成一个0-1的随机浮点数或N维浮点数 --均匀分布a = np.random.rand()b = np.random.rand(4,4)print(a)print(b)&gt;&gt;&gt;0.5544023939180306[[0.46387648 0.97345876 0.12059175 0.7565951 ] [0.30192996 0.76633208 0.20107761 0.09315875] [0.79347118 0.26714404 0.08628158 0.72510313] [0.06606087 0.93260038 0.90268201 0.90941348]] 以正太分布的方式生成随机数1234567891011# numpy.random.randn() 生成一个0-1的随机浮点数或N维浮点数 --正态分布a = np.random.randn()b = np.random.randn(4,4)print(a)print(b)&gt;&gt;&gt;0.26901442604096687[[ 0.40261375 -0.23541184 0.96607489 -1.11253043] [-0.31670703 0.05841136 -0.01862511 1.72597729] [ 0.17052799 1.03537825 -0.94375417 1.32484928] [ 0.132761 0.44950533 0.44131534 -0.11319535]] 按照上面的写法相信大家对与.randn()和.rand()的认识还不够清晰，这里用可视化的方式展示一下：123456789101112#平均分布# numpy.random.rand() 生成一个0-1的随机浮点数或N维浮点数 --均匀分布data1 = np.random.rand(500)data2 = np.random.rand(500)#正态分布# numpy.random.randn() 生成一个浮点数或N维浮点数 --正态分布data3 = np.random.randn(500)data4 = np.random.randn(500)import matplotlib.pyplot as plt% matplotlib inlineplt.scatter(data1,data2)plt.scatter(data3,data4) 这是随机分布的图样： 这是正态分布的图样： 可以看到正态分布和随机分布的成像还是有较大不同的，当然这里只是加深大家对.randn()和.rand()的认识，可视化再之后会进一步学习。 Numpy随机数的其他用法12345678910#随机整数print(np.random.randint(2))#在2-10之间生成随机整数print((np.random.randint(2,10)))# 在0-10之间生成10个整数print((np.random.randint(10,size=10)))# 在0-10之间生成包含10个元素的二维数组print(np.random.randint(10,size=(2,5)))# 在10-50之间生成包含10个元素的二维数组print(np.random.randint(10,50,size=(2,5))) 巩固练习 创建2个包含10个元素的正太分布一维数组 请按照要求创建数组ar，再将ar[:2,:2]的值改为[0,1)的随机数 按照要求创建数组，通过索引，其ar[4]、ar[:2,3:]、ar[3][2]分别是多少 按照要求创建数组，筛选出元素值大于5的值并生成新的数组]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scrapy pipline 实战实例]]></title>
    <url>%2F2018%2F10%2F09%2Fscrapy-pipline%2F</url>
    <content type="text"><![CDATA[pipline中的使用实例pipline存储json(自定义json存储)1234567891011import codecsclass JsonWithEncodingPipeline(object): #自定义json文件的导出 def __init__(self): self.file = codecs.open('article.json', 'w', encoding="utf-8") def process_item(self, item, spider): lines = json.dumps(dict(item), ensure_ascii=False) + "\n" self.file.write(lines) return item def spider_closed(self, spider): self.file.close() pipline存储json(使用scrapy自带的组件)123456789101112131415from scrapy.exporters import JsonItemExporterclass JsonExporterPipleline(object): #调用scrapy提供的json export导出json文件 def __init__(self): self.file = open('articleexport.json', 'wb') self.exporter = JsonItemExporter(self.file, encoding="utf-8", ensure_ascii=False) self.exporter.start_exporting() def close_spider(self, spider): self.exporter.finish_exporting() self.file.close() def process_item(self, item, spider): self.exporter.export_item(item) return item pipline中的存储mysql(阻塞)123456789101112131415import MySQLdbimport MySQLdb.cursorsclass MysqlPipeline(object): #采用同步的机制写入mysql def __init__(self): self.conn = MySQLdb.connect('192.168.0.106', 'root', 'root', 'article_spider', charset="utf8", use_unicode=True) self.cursor = self.conn.cursor() def process_item(self, item, spider): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ self.cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) self.conn.commit() pipline中存储mysql(异步)1234567891011121314151617181920212223242526272829303132333435363738394041class MysqlTwistedPipline(object): def __init__(self, dbpool): self.dbpool = dbpool @classmethod def from_settings(cls, settings): dbparms = dict( host = settings["MYSQL_HOST"], db = settings["MYSQL_DBNAME"], user = settings["MYSQL_USER"], passwd = settings["MYSQL_PASSWORD"], charset='utf8', cursorclass=MySQLdb.cursors.DictCursor, use_unicode=True, ) dbpool = adbapi.ConnectionPool("MySQLdb", **dbparms) return cls(dbpool) def process_item(self, item, spider): #使用twisted将mysql插入变成异步执行 query = self.dbpool.runInteraction(self.do_insert, item) query.addErrback(self.handle_error, item, spider) #处理异常 def handle_error(self, failure, item, spider): # 处理异步插入的异常 print (failure) def do_insert(self, cursor, item): insert_sql = """ insert into jobbole_article(title, url, create_date, fav_nums) VALUES (%s, %s, %s, %s) """ cursor.execute(insert_sql, (item["title"], item["url"], item["create_date"], item["fav_nums"])) # def do_insert(self, cursor, item): # #执行具体的插入 # #根据不同的item 构建不同的sql语句并插入到mysql中 # insert_sql, params = item.get_insert_sql() # print (insert_sql, params) # cursor.execute(insert_sql, params)]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Numpy基础(一)]]></title>
    <url>%2F2018%2F10%2F01%2Fnumpy1%2F</url>
    <content type="text"><![CDATA[什么是Numpy？Numpy是Python开源的科学计算工具包，是高级的数值编程工具 强大的N维数组对象:ndarray 可以对数组结构数据进行运算(不用遍历循环) 有随机数、线性代数、傅里叶变换等功能 如何安装？安装anaconda科学计算环境咸鱼也是从新手一步一坑踩过来,深知新手配置环境的不易，所以这里推荐使用anaconda，里面集成了许多常用的库，并且在配置环境时更容易上手。 下载地址：https://www.anaconda.com/download/ 具体安装步骤,这里不再赘述,不懂的朋友可以在交流群中讨论，也可以参考下面的博文： https://cuiqingcai.com/5059.html 安装Numpy方法一：安装anaconda后，numpy是可以直接使用的，无需二次安装。 方法二：没有安装anaconda可以使用pip install numpy安装。 安装jupyter notebooks(推荐使用)方法一：安装anaconda后，jupyter notebooks是可以直接使用的，无需二次安装。 方法二：没有安装anaconda可以使用pip install jupyter安装。 Numpy基础数据结构导入推荐使用from numpy import np 不建议使用from numpy import *, 因为numpy中包含了大量与Python内建函数重名的函数。 生成ndarray可以使用array生成数组举个栗子：1234567import numpy as npar = np.array([[1,2,3,4],[1,2,3,4]])print(ar, type(ar))&gt;&gt;&gt;[[1 2 3 4] [1 2 3 4]] &lt;class 'numpy.ndarray'&gt; 除了np.array之外还有其他函数可以创建新数组，这里列出常用的几个：12345678arange # python range的数组版asarray # 将输入转换为ndarrayones # 根据给定的形状和类型生成全1的数组ones_like # 根据给定的数组生成形状一样的全1的数组zeros # 根据给定的形状和类型生成全0的数组zeros_like # 根据给定的数组生成形状一样的全1的数组eye # 生成一个N*N的特征矩阵(对角线为1，其余为0)linspance # 返回在间隔[开始，停止]上计算的num个均匀间隔的样本 这里以zeros，zeros_like以及linspance分别举例：12345678910111213141516171819202122232425262728293031arr = np.zeros((3,5))print(arr)&gt;&gt;&gt;[[0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.] [0. 0. 0. 0. 0.]]s = np.array([list(range(10)),list(range(10,20))])print(s)print(np.zeros_like(s))&gt;&gt;&gt;[[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]][[0 0 0 0 0 0 0 0 0 0] [0 0 0 0 0 0 0 0 0 0]]print(np.linspace(10,20,num = 21)) #在10，21之间生成print(np.linspace(10,20,num = 21, endpoint = False)) #endpoint默认为True，为False时不包含左边的值print(np.linspace(10,20,num = 21, retstep = True))# restep显示步长&gt;&gt;&gt;[10. 10.5 11. 11.5 12. 12.5 13. 13.5 14. 14.5 15. 15.5 16. 16.5 17. 17.5 18. 18.5 19. 19.5 20. ][10. 10.47619048 10.95238095 11.42857143 11.9047619 12.38095238 12.85714286 13.33333333 13.80952381 14.28571429 14.76190476 15.23809524 15.71428571 16.19047619 16.66666667 17.14285714 17.61904762 18.0952381 18.57142857 19.04761905 19.52380952](array([10. , 10.5, 11. , 11.5, 12. , 12.5, 13. , 13.5, 14. , 14.5, 15. , 15.5, 16. , 16.5, 17. , 17.5, 18. , 18.5, 19. , 19.5, 20. ]), 0.5) 这里除了常用的几个生成数组的函数外，列举一些常用的方法：1234567891011121314151617import numpy as npar = np.array([[1,2,3,4],[1,2,3,4]])print(ar, type(ar))print(ar.ndim)#返回数组的维度的个数print(ar.shape)#数组的维度，返回几行几列print(ar.size)#数组元素的个数print(ar.dtype)#元素的类型print(ar.itemsize)#数组中元素的大小&gt;&gt;&gt;[[1 2 3 4] [1 2 3 4]] &lt;class 'numpy.ndarray'&gt;2(2, 4)8int648 Numpy通用函数数组形状变换（.T/.reshape()/.resize()）.T是转置函数,转置函数对一维数组无影响1234567891011121314# .Timport numpy as npar1 = np.arange(10)ar2 = np.zeros((2,5))print(ar1.T)print(ar2.T)#转置函数&gt;&gt;&gt;[0 1 2 3 4 5 6 7 8 9][[0. 0.] [0. 0.] [0. 0.] [0. 0.] [0. 0.]] .reshape(),直接更改数组的形状，但更改前后数组元素个数必须相同123456789ar1 = np.arange(10)print(ar1.reshape(2,5))print(np.reshape(np.arange(16),(2,8)))&gt;&gt;&gt;[[0 1 2 3 4] [5 6 7 8 9]][[ 0 1 2 3 4 5 6 7] [ 8 9 10 11 12 13 14 15]] .resize()12345678910print(np.resize(np.arange(16),(3,5))) # resize当后面的数组元素个数小于前面生成的数量时，按照顺序迭代print(np.resize(np.arange(12),(3,5))) # resize当后面的数组元素个数大于前面的生成的数量，则随机填充&gt;&gt;&gt;[[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 12 13 14]][[ 0 1 2 3 4] [ 5 6 7 8 9] [10 11 0 1 2]] 数组的复制和python中的深浅拷贝类似： 数组的类型转化.astype()可以将数组中元素的类型进行转化,在numpy中元素类型有以下几种(太多了就不都写了)：123456789int8, uint8 #有符号和无符号的8整位整数int16, uint16 #有符号和无符号的16整位整数int32, uint32 #有符号和无符号的32整位整数int64, uint64 #有符号和无符号的64整位整数float16 #半精度float32 #单精度float64 #双精度bool #布尔..... 举个类型转换的栗子：123456ar1 = np.arange(10,dtype=float)ar2 = ar1.astype(np.int64)print(ar1,ar2)&gt;&gt;&gt;[0. 1. 2. 3. 4. 5. 6. 7. 8. 9.] [0 1 2 3 4 5 6 7 8 9] 数组的堆叠数组的堆叠有hstack(),vstack()以及stack(),下面分别举例：123456789101112131415161718192021222324252627282930313233343536a = np.arange(10)b = np.arange(10,20)print(ar1,ar2)# 横向链接print(np.hstack((a,b)))# 竖向链接a = np.array([[1],[2],[3]])b = np.array([['a'],['b'],['c']])print(np.vstack((a,b)))# 任意堆叠a = np.arange(10)b = np.arange(10,20)print(np.stack((a,b),axis=1)) # 竖向堆叠print(np.stack((a,b))) # 横向堆叠&gt;&gt;&gt;&gt;[0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19][ 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19][['1'] ['2'] ['3'] ['a'] ['b'] ['c']][[ 0 10] [ 1 11] [ 2 12] [ 3 13] [ 4 14] [ 5 15] [ 6 16] [ 7 17] [ 8 18] [ 9 19]][[ 0 1 2 3 4 5 6 7 8 9] [10 11 12 13 14 15 16 17 18 19]] 数组拆分数组拆分同样分为横向拆分和竖向拆分。123456789101112131415161718192021# 数组拆分ar = np.arange(16).reshape(4,4)print(ar)print(np.hsplit(ar,2)) #纵向拆分print(np.vsplit(ar,2)) #横向拆分&gt;&gt;&gt;[[ 0 1 2 3] [ 4 5 6 7] [ 8 9 10 11] [12 13 14 15]][array([[ 0, 1], [ 4, 5], [ 8, 9], [12, 13]]), array([[ 2, 3], [ 6, 7], [10, 11], [14, 15]])][array([[0, 1, 2, 3], [4, 5, 6, 7]]), array([[ 8, 9, 10, 11], [12, 13, 14, 15]])] 常用计算函数这里的计算函数与Python中的计算函数用法相同，这里不再过多论述。1234567#计算函数np.mean() #求平均值np.max() #最大值np.min() #最小值np.gtd() #标准差np.var() #方差np.sum() # 其中参数axis=0按列求和axis=1按行求和 巩固练习 生成一个一维数组、二维数组，并且查看其shape 生成一个一维数组，起始值为5，终点值为15，样本数为10个 创建一个20个元素的数组，分别改变成两个形状：(4,5),(5,6) 创建一个(4,4)的数组，把其元素类型改为字符型 创建一个二维数组ar，起始值为0，终点值为15，运用数组的运算方法得到结果：result = ar * 10 +100，并求出result的均值及求和]]></content>
      <categories>
        <category>数据科学</category>
      </categories>
      <tags>
        <tag>数据科学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[scrapy crawl 实战腾讯招聘]]></title>
    <url>%2F2018%2F09%2F01%2FScrapy-CrawlSpider-learn%2F</url>
    <content type="text"><![CDATA[为什么使用CrawlSpider类？回顾上一篇文章，我们大多时间都是在寻找下一页的url地址或者是内容的url地址上面，我们的大体思路是这样的： 从response中提取所有的a标签对应的url地址 自动的构造自己requests请求，发送给引擎 其实我们可以使用CrawlSpider类，让满足某个条件的url地址，我们才发送给引擎，同时能够指定callback函数。 CrawlSpider的使用使用scrapy genspider –t crawl [爬虫名] [all_domain]就可以创建一个CrawlSpider模版 CrawlSpider继承于Spider类，除了继承过来的属性外（name、allow_domains），还提供了新的属性和方法： RulesCrawlSpider使用rules来决定爬虫的爬取规则，并将匹配后的url请求提交给引擎。所以在正常情况下，CrawlSpider不需要单独手动返回请求了。 在Rules中包含一个或多个Rule对象，每个Rule对爬取网站的动作定义了某种特定操作，比如提取当前相应内容里的特定链接，是否对提取的链接跟进爬取，对提交的请求设置回调函数等。 如果多个Rule匹配了相同的链接，则根据规则在本集合中被定义的顺序，第一个会被使用。 12345678class scrapy.spiders.Rule( link_extractor, callback = None, cb_kwargs = None, follow = None, process_links = None, process_request = None) 其中： link_extractor：是一个Link Extractor对象，用于定义需要提取的链接。 callback： 从Link Extractor中每获取到链接时，参数所指定的值作为回调函数，该回调函数接受一个response作为其第一个参数。 注意：当编写爬虫规则时，避免使用parse作为回调函数。由于CrawlSpider使用parse方法来实现其逻辑，如果覆盖了 parse方法，crawl spider将会运行失败。 follow：是一个布尔(boolean)值，指定了根据该规则从response提取的链接是否需要跟进。 如果callback为None，follow 默认设置为True ，否则默认为False。 process_links：指定该spider中哪个的函数将会被调用，从link_extractor中获取到链接列表时将会调用该函数。该方法主要用来过滤。 process_request：指定该spider中哪个的函数将会被调用， 该规则提取到每个request时都会调用该函数。(用来过滤request) LinkExtractorsLink Extractors 的目的很简单：提取链接｡ 12345678910111213class scrapy.linkextractors.LinkExtractor( allow = (), deny = (), allow_domains = (), deny_domains = (), deny_extensions = None, restrict_xpaths = (), tags = ('a','area'), attrs = ('href'), canonicalize = True, unique = True, process_value = None) 其中： allow：满足括号中正则表达式的URL会被提取，如果为空，则全部匹配。 deny：满足括号中“正则表达式”的URL一定不提取（优先级高于allow）。 allow_domains：会被提取的链接的domains。 deny_domains：一定不会被提取链接的domains。 restrict_xpaths：使用xpath表达式，和allow共同作用过滤链接。 CrawlSpider类-实战腾讯招聘上一篇文章我们用scrapy spider类实现了腾讯招聘的爬取，这次就再用CrawlSpider再实现一次。 创建爬虫scrapy genspider –t crawl tthr tencent.com 分析页面这里我们只要找出详情页的链接规律和翻页的链接规律，所以可以找到以下链接： 1234# 详情页规律position_detail.php?id=43310&amp;keywords=&amp;tid=0&amp;lid=0# 翻页规律position.php?&amp;start=10#a 到这里我们就可以写出以下rule： 1234# 详情页链接规律Rule(LinkExtractor(allow=r'position_detail\.php\id=\d+&amp;keywords=&amp;tid=0&amp;lid=0'),callback='parse_item'),# 在列表页查找翻页链接规律Rule(LinkExtractor(allow=r'position\.php\?&amp;start=\d+#a'), follow=True) 注意： 在查找详情页Rule时follow=False，我们无需在详情页再查找详情页的地址，而在列表Rule时follow=False，意味着不断在列表页中查找下一页地址。 编写代码12345678910111213141516171819202122232425# -*- coding: utf-8 -*-import scrapyfrom scrapy.linkextractors import LinkExtractorfrom scrapy.spiders import CrawlSpider, Rulefrom tencenthr.items import TtItemclass TthrSpider(CrawlSpider): name = 'tthr' allowed_domains = ['tencent.com'] start_urls = ['https://hr.tencent.com/position.php'] rules = ( Rule(LinkExtractor(allow=r'position_detail\.php\?id=\d+&amp;keywords=&amp;tid=0&amp;lid=0'), callback='parse_item'), Rule(LinkExtractor(allow=r'position\.php\?&amp;start=\d+#a'), follow=True) ) def parse_item(self, response): item = TtItem() item['sharetitle'] = response.xpath('//td[@id="sharetitle"]/text()').extract_first() item['category'] = response.xpath('//span[text()="职位类别："]/../text()').extract_first() item['location'] = response.xpath('//span[text()="工作地点："]/../text()').extract_first() item['num'] = response.xpath('//span[text()="招聘人数："]/../text()').extract_first() item['duty'] = response.xpath('//div[text()="工作职责："]/../ul/li/text()').extract() item['claim'] = response.xpath('//div[text()="工作要求："]/../ul/li/text()').extract() return item 结果截图]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于多进程详解]]></title>
    <url>%2F2018%2F09%2F01%2Fduo-jin-cheng%2F</url>
    <content type="text"><![CDATA[进程详解并行与并发怎么理解？ 并发：当前任务数多于处理器的核心数，称之为并发 并行：当前任务数少于或等于处理器核心数，称之为并行 为了充分利用多核CPU资源，Python中大部分情况下都需要使用多进程。 在python中我们如何创建多进程？ 进程和线程的区别？ 我们经常迷惑于多进程和多线程，长的好像一样，但是他们有本质上的区别，很多大佬也对进程和线程的概念做了很多通俗易懂的解释，这里我们引用阮一峰老师的博文，大家可以先去看看，理清楚线程和进程的区别。 进程与线程的一个简单解释 python如何创建多进程？ 使用os.fork()的方法创建多进程 使用multiprocessing创建多进程 使用multiprocessing中Pool创建进程池对象 创建多进程的方法有什么区别？ 使用os 模块中的 os.fork()创建进程 123456import osres = os.fork()if res == 0: # 子进程返回的值恒等于0 print('这是子进程%s',os.getpid()) # 获取子进程IDelse: print('这是父进程%s',os.getppid()) # 获取父进程ID 在这里使用os.fork()创建进程后会返回两个值，其中一个值为零。另一个值为创建的子进程ID，在这里可以使用os.getpid()获取子进程的ID，可以使用os.getppid()获取父进程ID。 需要注意的是：在os.fork()创建的子进程中，父进程和子进程执行的是一样的任务，并且在执行的任务结束时，父进程会自行结束，不会等待子进程结束之后结束。 使用multiprocessing模块创建多进程 multiprocessing中提供了Process类来生成进程实例 1Process([group [, target [, name [, args [, kwargs]]]]]) group分组，实际上不使用 target表示调用对象，传入任务执行函数作为参数 args表示给调用对象以元组的形式提供参数，比如target是函数a，他有两个参数m，n，那么该参数为args=(m, n)即可，只有一个参数时。该参数为args=(m,) kwargs表示调用对象的字典 name是别名，相当于给这个进程取一个名字 同时在Process类中提供以下几个方法用于实现进程的操作： start()让进程执行target调用的对象 join()阻塞，默认主进程，不会等到子进程结束后结束，需要使用join()使得主进程等待子进程执行结束后结束 terminate()结束当前，无论任务是否完成 举个栗子： 123456789101112131415161718192021222324252627282930# multiprocessimport osfrom multiprocessing import Processdef test(i): print('-----1111-----%s----%s' % (os.getpid(),os.getppid()),i)if __name__ == '__main__': for i in range(1,10): p = Process(target=test,args=(i,)) print(os.getpid()) p.start()# 输出52245224522452245224522452245224-----1111-----4704----5224 1-----1111-----1292----5224 4-----1111-----5276----5224 2-----1111-----5152----5224 65224-----1111-----5196----5224 5-----1111-----5272----5224 3-----1111-----4640----5224 7-----1111-----5684----5224 8-----1111-----1044----5224 9[Finished in 1.3s] 使用multiprocessing中的Pool类创建进程池对象 使用Pool类创建进程的方法和使用Process类创建进程的方法基本类似。 注意以下几点： pool.apply_async()非阻塞 pool.apply()阻塞 pool.join()主进程，创建/添加任务之后，主进程默认不会等待进程池中的任务执行完后才结束，而是当主进程的任务完成后，立马结束，如果没有添加join()会导致进程池中的任务不执行。 举个栗子： 12345678910111213141516171819202122232425262728# poolfrom multiprocessing import Poolimport os,time,randomdef worker(msg): t_start = time.time() print("%s开始执行,进程号为%d"%(msg,os.getpid())) #random.random()随机生成0~1之间的浮点数 time.sleep(random.random()*2) t_stop = time.time() print(msg,"执行完毕，耗时%0.2f"%(t_stop-t_start))def main(): po=Pool(3) #定义一个进程池，最大进程数3 for i in range(0,10): #Pool.apply_async(要调用的目标,(传递给目标的参数元祖,)) #每次循环将会用空闲出来的子进程去调用目标 po.apply_async(worker,(i,)) print("----start----") po.close() #关闭进程池，关闭后po不再接收新的请求 po.join() #等待po中所有子进程执行完成，必须放在close语句之后 print("-----end-----")if __name__ == '__main__': main() 进程之间如何通信？进程间通信有很多中方式，包含但不限于命名管道，无名管道，共享内存，队列等。主要学习一下队列-Queue的使用： Queue的使用： Queue.qsize()：返回当前队列包含的消息数量； Queue.empty()：如果队列为空，返回True，反之False； Queue.full()：如果队列满了，返回True,反之False； Queue.get([block[, timeout]])：获取队列中的一条消息，然后将其从列队中移除，block默认值为True； Queue.get_nowait()：相当Queue.get(False)； Queue.put(item,[block[, timeout]])：将item消息写入队列，block默认值为True； Queue.put_nowait(item)：相当Queue.put(item, False)； 首先实例化Queue对象，例如：p = Queue(num)其中num可以为空或负数代表可接受的消息无上限。 如果block使用默认值，且没有设置timeout（单位秒），消息列队如果为空，此时程序将被阻塞（停在读取状态），直到从消息列队读到消息为止，如果设置了timeout，则会等待timeout秒，若还没读取到任何消息，则抛出Queue.Empty异常；如果block值为False，消息列队如果为空，则会立刻抛出Queue.Empty异常； 举个栗子： 12345678910111213141516171819202122232425262728#coding=utf-8from multiprocessing import Queueq=Queue(3) #初始化一个Queue对象，最多可接收三条put消息q.put("消息1")q.put("消息2")print(q.full()) #Falseq.put("消息3")print(q.full()) #True#因为消息列队已满下面的try都会抛出异常，第一个try会等待2秒后再抛出异常，第二个Try会立刻抛出异常try: q.put("消息4",True,2)except: print("消息列队已满，现有消息数量:%s"%q.qsize())try: q.put_nowait("消息4")except: print("消息列队已满，现有消息数量:%s"%q.qsize())#推荐的方式，先判断消息列队是否已满，再写入if not q.full(): q.put_nowait("消息4")#读取消息时，先判断消息列队是否为空，再读取if not q.empty(): for i in range(q.qsize()): print(q.get_nowait()) 再举个栗子： 123456789101112131415161718192021222324252627282930313233from multiprocessing import Process, Queueimport os, time, random# 写数据进程执行的代码:def write(q): for value in ['A', 'B', 'C']: print 'Put %s to queue...' % value q.put(value) time.sleep(random.random())# 读数据进程执行的代码:def read(q): while True: if not q.empty(): value = q.get(True) print 'Get %s from queue.' % value time.sleep(random.random()) else: breakif __name__=='__main__': # 父进程创建Queue，并传给各个子进程： q = Queue() pw = Process(target=write, args=(q,)) pr = Process(target=read, args=(q,)) # 启动子进程pw，写入: pw.start() # 等待pw结束: pw.join() # 启动子进程pr，读取: pr.start() pr.join() print '所有数据都写入并且读完' 进程池中的Queue的使用： Pool创建进程，就需要使用multiprocessing.Manager()中的Queue()，而不是multiprocessing.Queue()，否则会得到一条如下的错误信息： 1RuntimeError: Queue objects should only be shared between processes through inheritance. 举个栗子： 123456789101112131415161718192021222324#修改import中的Queue为Managerfrom multiprocessing import Manager,Poolimport os,time,randomdef reader(q): print("reader启动(%s),父进程为(%s)"%(os.getpid(),os.getppid())) for i in range(q.qsize()): print("reader从Queue获取到消息：%s"%q.get(True))def writer(q): print("writer启动(%s),父进程为(%s)"%(os.getpid(),os.getppid())) for i in "dongGe": q.put(i)if __name__=="__main__": print("(%s) start"%os.getpid()) q=Manager().Queue() #使用Manager中的Queue来初始化 po=Pool() #使用阻塞模式创建进程，这样就不需要在reader中使用死循环了，可以让writer完全执行完成后，再用reader去读取 po.apply(writer,(q,)) po.apply(reader,(q,)) po.close() po.join() print("(%s) End"%os.getpid())]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[requests与json的使用技巧]]></title>
    <url>%2F2018%2F09%2F01%2Frequests-and-json%2F</url>
    <content type="text"><![CDATA[requestsrequestse基本使用 关于requests基本使用方法参照中文版参考文档： http://docs.python-requests.org/zh_CN/latest/index.html response.text 和response.content的区别1234567891011response.text类型：str解码类型： 根据HTTP 头部对响应的编码作出有根据的推测，推测的文本编码如何修改编码方式：response.encoding=”gbk”response.content类型：bytes解码类型：没有指定如何修改编码方式：response.content.deocde(“utf8”)更推荐使用response.content.deocde()的方式获取响应的html页面 requests使用代理12345requests.get("http://www.baidu.com", proxies = proxies)proxies = &#123; "http": "http://12.34.56.79:9527", "https": "https://12.34.56.79:9527", &#125; 代理的基本原理 正向代理与反向代理正向代理：浏览器明确知道要访问的是什么服务器，只不过目前无法达到，需要通过代理来帮助完成这个请求操作。 反向代理：浏览器不知道任何关于要请求的服务器的信息，需要通过Nginx请求。 requests模拟登录的几种方法 实例化session，使用session发送post/get请求登录后的页面 12session = requests.session()response = session.get(url,headers) 在headers中添加cookie键，值为cookie字符串 在请求方法中添加cookie参数，接收字典形式的cookie cookie和session区别 cookie数据存放在客户的浏览器上，session数据放在服务器上 cookie不是很安全，别人可以分析存放在本地的cookie并进行cookie欺骗 session会在一定时间内保存在服务器上。当访问增多，会比较占用你服务器的性能 单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie requests的几个小技巧(我认为的)12345678910111213#把cookie对象转化为字典reqeusts.util.dict_from_cookiejar #把字典转化为cookie对象reqeusts.util.cookiejar_from_dict#url解码reqeusts.util.unquote()#url编码reqeusts.util.quote() #忽略SSL证书验证response = requests.get("https://www.12306.cn/mormhweb/ ", verify=False) #请求https的网站忽略SSL证书验证之后还是会出现警告信息，在请求前加上下面这句就可以禁用安全请求警告#InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings InsecureRequestWarning）requests.packages.urllib3.disable_warnings() 通用的解析链接函数123456789101112131415161718192021222324252627# coding=utf-8import requestsfrom retrying import retryheaders=&#123;"User-Agent":"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36"&#125;@retry(stop_max_attempt_number=3)def _parse_url(url,method,data,proxies): print("*"*20) if method=="POST": response = requests.post(url,data=data,headers=headers,proxies=proxies) else: response = requests.get(url,headers=headers,timeout=3,proxies=proxies) assert response.status_code == 200 return response.content.decode()def parse_url(url,method="GET",data=None,proxies=&#123;&#125;): try: html_str = _parse_url(url,method,data,proxies) except: html_str = None return html_strif __name__ == '__main__': parse_url(url) jsonjson的基本使用1234json.loads() #json字符串转化为python数据类型json.dumps() #python数据类型转化为json字符串json.load() #包含json的类文件对象转化为python数据类型json.dump() #python数据类型转化为包含json的类文件对象 什么是类文件对象？具有read()或者write()方法的对象就是类文件对象，f = open(“a.txt”,”r”) f就是类文件对象 如何优雅的查看json?我们经常在打印json数据的时候会遇到，像下图这样的情况 我们只要在打印的时候使用pprint就可以完美避开这种情况了，效果如下： 我们想要实现上图只需要pip install pprint并在打印时使用pprint即可，而在保存时只需要添加”indent=2”参数，就可以保存为pprint的样式。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[request模拟登陆]]></title>
    <url>%2F2018%2F09%2F01%2Frequets-login%2F</url>
    <content type="text"><![CDATA[requests模拟登录获取本地cookie12345678import resession = requests.session()session.cookies = cookielib.LWPCookieJar(filename="cookies.txt")try: session.cookies.load(ignore_discard=True)except: print ("cookie未能加载") 123allow_redirects=False --禁止重定向session.cookies.save() --调用save方法保存cookie 手动输入验证码(实例)12345678910111213141516171819def get_captcha(): import time t = str(int(time.time()*1000)) captcha_url = "https://www.zhihu.com/captcha.gif?r=&#123;0&#125;&amp;type=login".format(t) t = session.get(captcha_url, headers=header) with open("captcha.jpg","wb") as f: f.write(t.content) f.close() from PIL import Image try: im = Image.open('captcha.jpg') im.show() im.close() except: pass captcha = input("输入验证码\n&gt;") return captcha]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[初识Scrapy]]></title>
    <url>%2F2018%2F09%2F01%2Fscrapy1%2F</url>
    <content type="text"><![CDATA[初识Scrapy什么是Scrapy？Scrapy使用 Python 实现的一个开源爬虫框架，Scrapy基于 twisted这个高性能的事件驱动网络引擎框架，Scrapy爬虫拥有很高的性能。 Scrapy内置数据提取器（Selector），支持XPath和 Scrapy自己的 CSS Selector语法 并且支持正则表达式，方便从网页提取信息。 交互式的命令行工具，方便测试 Selector 和 debugging爬虫 支持将数据导出为 JSON，CSV，XML格式。 可推展性强，运行自己编写特定功能的插件 内置了很多拓展和中间件用于处理： cookies和 session HTTP的压缩，认证，缓存 robots.txt 爬虫深度限制 Scrapy流程图 其中： Scrapy Engine(引擎): 负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。 Scheduler(调度器): 它负责接受引擎发送过来的Request请求，并按照一定的方式进行整理排列，入队，当引擎需要时，交还给引擎。 Downloader（下载器）：负责下载Scrapy Engine(引擎)发送的所有Requests请求，并将其获取到的Responses交还给Scrapy Engine(引擎)，由引擎交给Spider来处理， Spider（爬虫）：它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给引擎，再次进入Scheduler(调度器)， Item Pipeline(管道)：它负责处理Spider中获取到的Item，并进行进行后期处理（详细分析、过滤、存储等）的地方. Downloader Middlewares（下载中间件）：你可以当作是一个可以自定义扩展下载功能的组件。 Spider Middlewares（Spider中间件）：你可以理解为是一个可以自定扩展和操作引擎和Spider中间通信的功能组件（比如进入Spider的Responses;和从Spider出去的Requests） 制作 Scrapy 爬虫 的步骤?1234新建项目(scrapy startproject xxx)：新建一个新的爬虫项目明确目标（编写items.py）：明确你想要抓取的目标制作爬虫（spiders/xxspider.py）：制作爬虫开始爬取网页存储内容（pipelines.py）：设计管道存储爬取内容 如何安装Scrapy?在windows系统下安装Scrapy在windows 64bit系统下安装Scrapy需要安装以下依赖库： 123456pip install wheellxml-4.2.1-cp36-cp36m-win_amd64.whlpyOpenSSL-17.5.0-py2.py3-none-any.whlpywin32-221.win-amd64-py3.6.exeTwisted-17.9.0-cp36-cp36m-win_amd64.whlpip install scrapy 在linux下安装Scrapy系统版本为ubuntu 16.04 12sudo apt-get install build-essential python3-dev libssl-dev libffi-dev libxml2 libxml2-dev libxslt1-dev zlib1g-devpip install scrapy Scrapy文件结构我们在windows命令行模式下输入以下命令创建Scrapy项目： 1scrapy startproject 项目名称 可以看到创建了以下文件： 其中： 123456scrapy.cfg ：项目的配置文件xxSpider/ ：项目的Python模块，将会从这里引用代码xxSpider/items.py ：项目的目标文件xxSpider/pipelines.py ：项目的管道文件xxSpider/settings.py ：项目的设置文件xxSpider/spiders/ ：存储爬虫代码目录 Scrapy单文件demo创建完Scrapy项目，还是要上手实验一下才能更好的理解，所以我根据之前我在楼+课程中的学习笔记写了一个Scrapy单文件Demo，使用这个单文件Demo能快速爬取实验楼全部课程信息。首先看下单文件的内容结构： 1234567891011121314# -*- coding:utf-8 -*-import scrapyclass ShiyanlouCoursesSpider(scrapy.Spider): """ 所有 scrapy 爬虫需要写一个 Spider 类，这个类要继承 scrapy.Spider 类。在这个类中定义要请求的网站和链接、如何从返回的网页提取数据等等。 """ # 爬虫标识符号，在 scrapy 项目中可能会有多个爬虫，name 用于标识每个爬虫，不能相同 name = 'shiyanlou-courses' def start_requests(self): """ 需要返回一个可迭代的对象，迭代的元素是scrapy.Request对象，可迭代对象可以是一个列表或者迭代器，这样 scrapy 就知道有哪些网页需要爬取了。scrapy.Request接受一个 url 参数和一个 callback 参数，url 指明要爬取的网页，callback 是一个回调函数用于处理返回的网页，通常是一个提取数据的 parse 函数。 """ def parse(self, response): """ 这个方法作为 `scrapy.Request` 的 callback，在里面编写提取数据的代码。scrapy 中的下载器会下载 `start_reqeusts` 中定义的每个 `Request` 并且结果封装为一个 response 对象传入这个方法。 """ pass 因为实验楼的网页结构还是很简单的，所以解析部分就不做赘述，直接上单文件完整代码： 12345678910111213141516171819202122232425# -*- coding:utf-8 -*-import scrapyclass ShiyanlouCoursesSpider(scrapy.Spider): def start_requests(self): # 课程列表页面 url 模版 url_tmpl = 'https://www.shiyanlou.com/courses/?category=all&amp;course_type=all&amp;fee=all&amp;tag=all&amp;page=&#123;&#125;' # 所有要爬取的页面 urls = (url_tmpl.format(i) for i in range(1, 23)) # 返回一个生成器，生成 Request 对象，生成器是可迭代对象 for url in urls: yield scrapy.Request(url=url, callback=self.parse) def parse(self, response): # 遍历每个课程的 div.course-body for course in response.css('div.course-body'): # 使用 css 语法对每个 course 提取数据 yield &#123; # 课程名称 'name': course.css('div.course-name::text').extract_first(), # 课程描述 'description': course.css('div.course-desc::text').extract_first(), # 课程类型，实验楼的课程有免费，会员，训练营三种，免费课程并没有字样显示，也就是说没有 span.pull-right 这个标签，没有这个标签就代表时免费课程，使用默认值 `免费｀就可以了。 'type': course.css('div.course-footer span.pull-right::text').extract_first(default='Free'), # 注意 // 前面的 .，没有点表示整个文档所有的 div.course-body，有 . 才表示当前迭代的这个 div.course-body 'students': course.xpath('.//span[contains(@class, "pull-left")]/text()[2]').re_first('[^\d]*(\d*)[^\d]*') &#125; 保存文件，使用scrapy runspider xx.py -o data.json运行代码，这里使用 -o参数将结果输出为json格式。]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[爬虫究竟是合法还是违法的？]]></title>
    <url>%2F2018%2F09%2F01%2Fspider-shi-he-fa-hai-shi-bu-he-fa%2F</url>
    <content type="text"><![CDATA[爬虫究竟是合法还是违法的？随着大数据人工智能的火热，爬虫行业竞争不仅“蒸蒸日上”，也越发地激烈。一篇《你的爬虫会送老板进监狱吗？》在程序猿圈子里被大量转载，甚至有的程序员因为非法获取数据的新闻从而放弃了这一行当。那么，爬虫是什么，它会是悬在程序员头上的达摩克利斯之剑吗？ 网络爬虫（英语：web crawler），也叫网络蜘蛛（spider），是一种用来自动浏览万维网的网络机器人。通俗来讲，爬虫就是一项计算机技术，方便用户自动化、高效率地浏览互联网并从互联网上获取数据。最早的爬虫程序是1994年休斯敦大学的Eichmann开发的RBSE。著名的谷歌公司使用的Google Crawler是当时还是斯坦福大学生Brin和Page在1998年用Python开发的。（见罗刚《网络爬虫全解析：技术、原理与实践》，电子工业出版社，第65-66页。） 爬虫作为一种计算机技术就决定了它的中立性，因此爬虫本身在法律上并不被禁止，但是利用爬虫技术获取数据这一行为是具有违法甚至是犯罪的风险的。所谓具体问题具体分析，正如水果刀本身在法律上并不被禁止使用，但是用来捅人，就不被法律所容忍了。 详细分析既然我们说爬取数据是有可能触犯法律的，那么我们就需要拆开分析一下到底什么情况下会被法律所制裁。爬取数据操作分为爬取的行为和爬取获得的数据两方面，我们将分别论述。 1. 爬取行为的法律风险1.1 民事风险爬虫目前能造成的技术上影响在于野蛮爬取，即多线程爬取，从而导致网站瘫痪或不能访问，这也是大多数网络攻击所使用的方法之一。 由于爬虫会批量访问网站，因此许多网站会采取反爬措施。例如：1.IP频率、流量限制；2.请求时间窗口过滤统计；3.识别爬虫等。 但这些手段都无法阻止爬虫开发人员优化代码、使用多IP池等方式规避反爬措施，实现大批量的数据抓取。由于网络爬虫会根据特定的条件访问页面，因而爬虫的使用将占用被访问网站的网络带宽并增加网络服务器的处理开销，甚至无法正常提供服务。在《反不正当竞争法》第十二条第二款中我们可以发现，法律会对爬虫的这种行为进行规制。 即经营者不得利用技术手段，通过影响用户选择或者其他方式，实施下列妨碍、破坏其他经营者合法提供的网络产品或者服务正常运行的行为：…（四）其他妨碍、破坏其他经营者合法提供的网络产品或者服务正常运行的行为。 虽然上述规定是兜底条款，但其体现了法律禁止通过技术手段进行对他人非法干绕的总体原则。 因此，如果网站运营者已经采取了一定的反扒措施，而爬虫开发人员基于经营的目的、强行突破网站运营者采取的反爬措施，并客观上导致了网站无法正常运行，则很有可能构成上述规定所表述的不正当竞争行为。 但是在此种情况下鉴别爬虫者身份仍然是追责的一大阻碍，很多网站由于反爬机制落后，因而在法院诉讼中无法举证证明爬虫者因而得不到法院的支持。（见北京知识产权法院 （2016）京73民终588号案件） 1.2 刑事风险强行突破某些特定的反爬技术措施，还会构成形式犯罪的行为。 《刑法》第二百八十五条规定，违反规定侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，不论情节严重与否，构成非法侵入计算机信息系统罪。《刑法》第二百八十六条还规定，违反国家规定，对计算机信息系统功能进行删除、修改、增加、干扰，造成计算机信息系统不能正常运行，后果严重的，构成犯罪，处五年以下有期徒刑或者拘役；后果特别严重的，处五年以上有期徒刑。而违反国家规定，对计算机信息系统中存储、处理或者传输的数据和应用程序进行删除、修改、增加的操作，后果严重的，也构成犯罪，依照前款的规定处罚。 如上所述规定，爬虫开发者在获取数据过程中，一旦突破某些技术防护措施并且操作不当，造成严重后果的，将会构成犯罪，面临牢狱之灾。 2. 爬取特定类型的信息的法律风险凡是能够被电子化记录的都是数据，数据分为两大类。 第一类：非个人数据（non-PII，non-personally identifiable information）即此类数据与个人信息无关（此处需注意，与个人信息无关不代表与个人无关，而是说不涉及个人隐私或者不能识别到具体个人）。此类数据通常是公开数据（当然也有作为国家秘密、商业秘密而采取保密措施不公开的秘密数据和秘密信息），因此不适用个人信息保护方面的法律法规。如企业工商注册信息、裁判文书（因涉密或个人隐私不公开的除外）、天气气象数据、环境监测数据、地理测绘、总体性的人口数据、网站访问记录等。第二类：个人数据（PII，personally identifiable information）。即此类数据与个人信息有关，数据的来源是个人信息，且能够或可能识别到个人。其中又包括两类：1.已识别个人身份数据（personally identified information）。此类数据完全适用个人数据保护的相关法律法规。如姓名、家庭住址、电话号码等能够确定识别、关联到特定个人的数据，需符合个人数据保护法全部合规要求，包括知情同意、允许用户访问和更正、数据处理正当合法、目的限制、保障安全等。2.可能识别个人身份的数据（personally identifiable information）。此类数据结合业务场景，灵活适用个人数据保护的相关法律法规。如业务场景中，识别风险较高，可按照第二类数据的合规性要求处理，需满足全部合规要求；如识别风险较低，则可选择部分适用。 2.1 个人信息（PII）的爬取对于PII信息，其关键点在于用户授权，用户未授权或者授权不充分带来的法律风险很大。 2.1.1 不正当竞争风险例如新浪微博和脉脉发生的案件，一个因为开放API爬取数据引发的案子。 2014年8月，当时刚刚起步的职场社交软件“脉脉”与新浪微博微博闹掰了。脉脉上线之初，曾与新浪微博合作，脉脉的新用户可以通过微博帐号和个人帐号注册登录脉脉，用户注册时还要想陌陌上传手机通讯录联系人。随后，新浪微博发现，脉脉用户的“一度人脉”中，直接显示大量非脉脉用户的微博头像、名称、职业、教育等个人信息。2015年3月，微博主体公司将脉脉主体公司告上法庭。 原来用户使用微博登录脉脉并上传个人通讯录之后，大量非脉脉用户的微博头条、昵称、职业、教育等信息出现在脉脉上，而这些信息并不在微博与脉脉开放API协议中，并且微博停止脉脉所有的接口权限后，脉脉依然在抓取相关数据，微博认为脉脉通过非法手段获取信息。最终法院判决脉脉停止不正当竞争行为，并赔偿损失。 这个案件中，法院确立了一个原则，即平台要获取用户信息必须获得授权，平台之间通过开放API获取数据必须经过“用户授权-网站授权-用户授权”的规则。 2.1.2 侵犯隐私权风险例如（2015）西民初字第28460号：王刃与北京奇虎科技有限公司隐私权纠纷案件中，原告王刃因个人手机登记为所投资公司联系电话，被奇虎科技360手机卫士标记手机号码功能标记显示为公司号码，导致原告王刃手机被被叫方误认为是诈骗电话，因之以侵犯隐私权起诉360手机安全卫士所属公司奇虎科技。 在该案件中，法院提出，“被告出示的证据可以证明原告所使用的号码已经在企业黄页被公开披露，原告在工商行政管理机关登记企业信息时，亦将该手机号码予以登记，以备信息查阅。被告通过大数据比对功能，确定该手机号码与浙江维特网络信息有限公司合肥分公司相对应，并进行标记，其信息并无错误，且软件标记的企业信息，而非公民个人信息。被告已证实其获取手机号码对应的标记信息均来源于公开渠道，因此亦不能认定被告标记号码的行为侵犯了其隐私权”。同时，法院还认为：“对于360手机卫士软件中主动标记企业信息的功能，本院认为仍存在一定改进之处，我国小微企业的业主为工作方便、节约资源，将私人电话作为办公电话使用是普遍情况，这并不意味着手机号码被登记在工商行政管理机关后，就专用于商务。如非号码所有人主动申请标记，建议针对被标记号码采取短信确认的方式，对所有人有所提示，有助于其获得相应知情权”。 因而我们可以知道，本案确立了公开获取数据的合法性，但又要求数据服务方对个人信息标记使用应获得用户同意，也即重申了“默示同意”的许可方式，同时又强调了“用户同意”即用户授权的原则。 2.1.3 刑事风险爬取个人信息还有可能会面临牢狱之灾，《中华人民共和国刑法》第二百五十三条之一所涉的“侵犯公民个人信息罪”及第二百八十六条之一所涉的“拒不履行信息网络安全管理义务罪”就是爬虫在个人信息爬取过程中经常触碰到的“红线”。 同时，与《网络安全法》同日实施的最高人民法院、最高人民检察院《关于办理侵犯公民个人信息刑事案件适用法律若干问题的解释》也明确了情节严重的几种类型： 非法获取、出售或者提供行踪轨迹信息、通信内容、征信信息、财产信息五十条以上的；——高度敏感信息 非法获取、出售或者提供住宿信息、通信记录、健康生理信息、交易信息等其他可能影响人身、财产安全的公民个人信息五百条以上的；——敏感信息 非法获取、出售或者提供第三项、第四项规定以外的公民个人信息五千条以上的便构成“侵犯公民个人信息罪”所要求的“情节严重”。 此外，未经被收集者同意，即使是将合法收集的公民个人信息向他人提供的，也属于刑法第二百五十三条之一规定的“提供公民个人信息”，可能构成犯罪。例如（2016）浙0602刑初1145号案中，当事人就采用非法手段获取淘宝和支付宝中的个人信息，并提供、转售给他人，非法获利了巨额的财产。 2.1.4 行政处罚风险 《网络安全法》第六十四条规定，违反本法第四十四条规定，窃取或者以其他非法方式获取、非法出售或者非法向他人提供个人信息，尚不构成犯罪的，由公安机关没收违法所得，并处违法所得一倍以上十倍以下罚款，没有违法所得的，处一百万元以下罚款。 即便是非法获取数据的严重性没有达到入罪的标准，但是大概率上也会被行政机关进行处罚的，并且额度还不低，通常都以“万”来计，就算程序员收入高，也受不了这种程度的处罚吧。 2.1.5 总结爬取涉及个人信息的数据总体风险较高，如果爬取数据没有获得用户授权（包括通过API接口爬取数据的情况）则存在侵犯人格权（民法总则已经明确个人信息权是一种人格权）的风险。同时，爬取存在竞争关系平台上的数据时，还可能因实质替代获取不正当竞争优势、干扰或破坏他人网络服务的正常运行，涉嫌不正当竞争；更严重的是，还可能因非法获取公民个人信息、非法侵入计算机信息系统、非法获取计算机信息系统数据等涉嫌犯罪，招致刑罚。 2.2 non-PII的爬取对于这类数据的爬取，目前市面上通常的做法都是爬取公开数据，例如靠爬虫发家的“聚信立”公司。爬取这类数据的风险系数相对较低，毕竟通常不会涉及个人隐私与个人信息，但也并非毫无风险。可能会有哪些风险呢？ 2.2.1 著作权侵权风险就著作权本身而言，无论是文章、图片、视频、用户评论以及网站自身的数据库，都有可能在具备独创性的情况下构成著作权法保护的作品。对这些信息的获取，是否构成著作权侵权需要拆分分析： 1）在访问页面的行为下，由于爬虫是模仿人工访问机制进行页面访问操作的，因而该访问行为不会构成侵权，但如上述分析，如果该访问行为造成被访问页面反爬措施失效或者网站瘫痪，则会构成侵权。2）对于数据保存而言，从著作权的角度上来说，抓取行为是对信息的复制，因此该行为有可能侵犯著作权人的复制权。3）就数据提取和使用行为而言，如果爬取的数据被用于展示在公开的网站或者渠道，则会侵犯著作权人的信息网络传播权。 大众点评网诉爱帮网的诉讼中，大众点评网在前两轮诉讼中就是以爱帮网侵犯原告享有著作权的商户介绍和点评为由，起诉了爱帮网，最终以爱帮网停止使用该作品胜诉。（2010）海民初字第4253号 2.2.2 不正当竞争风险同样是大众点评网，在2016年还起诉了百度，原因是百度未经许可，使用爬虫技术从大众点评网上大量获取用户点评信息，用于自家的百度地图及百度知道产品。 最终一审判决认定百度构成不正当竞争行为，停止侵权并赔偿323万元。有图有真相：也即，如果公司业务中存在可能爬取竞争对手数据的情况，要格外注意这项风险。法院会首先判断双方是否存在竞争关系，进而判断爬取数据的一方是否存在“不劳而获”和“搭便车”的行为。如果是，就是上面的结果。 2.2.3 刑事风险1 侵犯著作权罪 根据《中华人民共和国刑法》第二百一十七条规定，侵犯著作权罪是指以营利为目的，未经著作权人许可复制发行其文字、音像、计算机软件等作品，出版他人享有独占出版权的图书，未经制作者许可复制发行其制作的音像制品，制作、展览假冒他人署名的美术作品，违法所得数额较大或者有其他严重情节的行为。 案例：2014年3月，被告人何某设立“车城小说”网站，其通过租赁海外服务器并运行其从互联网上下载的“关关采集”抓取软件，在未获起点中文网许可的情况下，擅自抓取、复制650部文字作品，存储于自己的服务器上，供“车城小说”网站用户免费阅读。 何某通过在“车城小说”网站网页内刊登广告获取广告收益，非法营利数额达人民币19万余元。法院认定，何某抓取并通过信息网络传播作品的数量高于法定追诉标准的500件，且营利数额超过5万元，构成侵犯著作权罪，判处有期徒刑1年，并处罚金10万元。 （2015）闵刑（知）初字第59号 2 非法侵入计算机信息系统罪 根据《刑法》第二百八十五条规定，违反规定侵入国家事务、国防建设、尖端科学技术领域的计算机信息系统的，不论情节严重与否，构成非法侵入计算机信息系统罪。 案例：在严某犯非法侵入计算机信息系统罪一案中，严某是一位协警，通过侵入警局内网，获取并篡改数据，达到非法获利的目的，触犯了“非法侵入计算机信息系统罪”，锒铛入狱。（2014）广利州刑初字第260号 3 非法获取计算机信息系统数据罪 《刑法》第二百八十五条还规定，违反规定侵入普通的计算机信息系统或者采用其他技术手段，获取该计算机信息系统中存储、处理或者传输的数据的，情节严重的，构成非法获取计算机信息系统数据罪。 案例：南京同享网络法定代表人张某、副总经理沈某组织员工编写模拟程序，非法获取掌门科技“WIFI万能钥匙”数据库内的WIFI热点密码数据案，最终法院认定构成非法获取计算机信息系统数据罪，上述两人最终被判刑三年并处罚金。（2015）杨刑初字第232号 4 总结爬取non-PII数据总体风险较低，但仍不要大意。 轻则可能构成侵犯著作权（在被爬取的数据具有独创性构成作品的情况下），如果有竞争关系，还可能因实质替代获取不正当竞争优势、干扰或破坏他人网络服务的正常运行，涉嫌构成不正当竞争； 重则可能因绕开技术措施非法获取数据，涉嫌侵犯商业秘密（严重情形涉及刑事责任），涉嫌构成非法侵入计算机信息系统、非法获取计算机信息系统数据罪等罪。 对于广大程序员来说，如何避免爬虫所带来的法律风险？敲黑板！！重点来了！！！ 如前面所述，爬虫所带来的风险主要有： 违反网站意愿，例如网站采取反爬措施后，强行突破其反爬措施； 爬虫干扰了被访问网站的正常运营； 爬虫抓取了受到法律保护的特定类型的数据或信息。 其中，第（3）类风险主要来自于通过规避反爬虫措施抓取到了互联网上未被公开的信息。 因此，爬虫开发者在使用爬虫时应注意： 严格遵守网站设置的robots协议； 在规避反爬虫措施的同时，需要优化自己的代码，避免干扰被访问网站的正常运行； 在设置抓取策略时，应注意编码抓取视频、音乐等可能构成作品的数据，或者针对某些特定网站批量抓取其中的用户生成内容； 在使用、传播抓取到的信息时，应审查所抓取的内容，如发现属于用户的个人信息、隐私或者他人的商业秘密的，应及时停止并删除。 答谢大家都知道咸鱼也是一个爬虫爱好者，本文所论述的问题也是咸鱼关心的，对于这个问题，咸鱼在交流群里和群友讨论过也和其他的爬虫大佬交流过，但毕竟不是相关专业的难免害怕触碰红线心里有点虚，正好在咸鱼为数不多的读者中有一位从事法律行业的大佬，于是就有了这篇文章。 本文由群友不二辰投稿，刚刚得知他的职业之后我第一个问的问题就是，爬虫合法吗？之后大佬发给我的这篇文章正好解决了咸鱼心中的疑虑，希望对学习爬虫的你们有所帮助，再次感谢不二辰的辛苦付出！]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[通用爬虫套路]]></title>
    <url>%2F2018%2F09%2F01%2Ftongyong-spider%2F</url>
    <content type="text"><![CDATA[通用爬虫套路 准备url 准备start url 页码总数不确定，规律不明显 通过代码提取下一页地址 xpath 寻找url地址，部分参数在当前页面中，比如当前页码数和总页码数 准备url list 页码总数清楚 url 地址规律明显 发送请求，获取响应 添加随机user-agent，反反爬虫 添加随机代理ip，反反爬虫 在对方判断我们是爬虫之后应该添加更多的headers字段，包含cookie cookie的处理可以使用session来解决 准备一堆能用的cookie，组成cookie池。 如果不登录 准备刚刚开始能够成功请求的cookie，即接受对方网站设置在response的cookie 如果登录 准备多个账号 使用程序获取每个账号的cookie 之后请求登录之后才能访问的网站随机选择cookie 提取数据 确定数据的位置 如果数据在当前的url地址中 提取的是列表页中的数据 直接请求列表页的url地址，不用进入详情页 提取详情页的数据 1.确定url 2.发送请求 3.提取数据 4.返回 如果数据不在当前的url地址中 在其他响应中，寻找数据的位置 在network中从上而下找 使用chrome中的过滤条件，选择除了js，css, image之外的按钮 使用chrome的serach all file，搜索数字和英文 数据的提取 xpath，从html中提取整块的数据，先分组，之后每一组在提取 re，提取max_time，html等 json 保存数据 保存在本地json ，csv，text 保存在数据库]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[配置虚拟环境]]></title>
    <url>%2F2018%2F09%2F01%2Fvirtualenv-setting%2F</url>
    <content type="text"><![CDATA[配置虚拟环境的必要性？虚拟环境是一个将不同项目所需求的依赖分别放在独立的地方的一个工具，它给这些工程创建虚拟的Python环境。它解决了“项目X依赖于版本1.x，而项目Y需要项目4.x”的两难问题，而且使你的全局site-packages目录保持干净和可管理。 virtualenv 是一个创建隔绝的Python环境的工具，virtualenv创建一个包含所有必要的可执行文件的文件夹，用来使用Python工程所需的包。 不同系统下如何安装虚拟环境？文章中使用的系统版本为 window7 64bit系统 和 ubuntu 16.04 系统 使用virtualenv安装虚拟环境windows7 64bit确保已经安装了Python和pip，没有安装的同学面向搜索引擎学习一下。 安装： 123pip install virtualenv#或者使用豆瓣源安装更快pip install -i https://pypi.douban.com/simple/ virtualenv 创建虚拟环境： 可以使用virtualenv [虚拟环境名称]来创建虚拟环境。运行截图如下： 当本地环境存在多个python版本时，可以使用virtualenv -p [.../python.exe]来创建指定版本的虚拟环境。 进入虚拟环境： 使用cd命令切换到创建的虚拟环境文件夹下的script目录，执行activate.bat进入虚拟环境。操作如下： 可以看到命令行以（虚拟环境名称）开头，这就代表成功进入虚拟环境。 退出虚拟环境： 使用cd命令切换到创建的虚拟环境文件夹下的script目录，执行deactivate.bat进入虚拟环境，执行deactivate.bat退出虚拟环境，操作如下： 删除虚拟环境： 直接删除对应的文件夹即可。 ubuntu 16.04确保已经安装了Python和pip，没有安装的同学面向搜索引擎学习一下。 12345678#安装pipsudo apt-get install python3-pip#ubuntu下内置了python2.7，所以我们可以把python3设置为默认，也可以不用sudo update-alternatives --install /usr/bin/python python /usr/bin/python2 100sudo update-alternatives --install /usr/bin/python python /usr/bin/python3 150#切换回来sudo update-alternatives --config python#按照提示输入选择数字回车即可 安装： 1pip install virtualenv 创建虚拟环境： 这里使用的命令和windows下命令相同virtualenv [虚拟环境名称] 12#使用指定版本的python创建虚拟环境virtualenv -p /usr/local/bin/python3 [虚拟环境名称] 进入虚拟环境： 切换至虚拟环境目录下，切换至.../bin中，执行以下命令。 1source activate.sh 退出虚拟环境： 1deactivate 删除虚拟环境 1rm -rf [虚拟环境名称] 使用virtualenvwrapper安装虚拟环境如果你按照文章进行到这里，一定感受到virtualenv非常不方便管理，所以推荐直接使用virtualenvwrapper来创建管理虚拟环境。 windows7 64bit安装： 1pip install virtualenv-win 创建虚拟环境： 先配置一个环境变量，这样创建的虚拟环境默认都会创建在环境变量下，如图： 可以使用mkvirtualenv [虚拟环境名称]来创建虚拟环境。运行截图如下： 使用指定版本的python安装虚拟环境： 1mkvirtualenv --python=[python的安装目录/python.exe] 列举现有的全部虚拟环境： 进入和退出虚拟环境： 使用workon [虚拟环境名称]进入虚拟环境，使用deactivate退出虚拟环境，操作如下： ubuntu16.04安装： 1pip install virtualenvwrapper 配置： 1234sudo vim ~/.bashrcexport WORKON_HOME=$HOME/.virtualenvs # 所有虚拟环境存储的目录source /usr/local/bin/virtualenvwrapper.shsource ~/.bashrc 创建虚拟环境： 1mkvirtualenv env_name # env_name为你要创建的虚拟环境的名字，创建虚拟环境需要联网 进入虚拟环境： 12workon env_nameworkon + 两次tab键可以显示所有的虚拟环境 退出虚拟环境： 1deactivate 删除虚拟环境： 1rmvirtualenv env_name ubuntu安装virtualenvwrapper虚拟环境常见报错source ~/.bashrc报错： 123456/usr/bin/python: No module named virtualenvwrappervirtualenvwrapper.sh: There was a problem running the initialization hooks.If Python could not import the module virtualenvwrapper.hook_loader,check that virtualenvwrapper has been installed forVIRTUALENVWRAPPER_PYTHON=/usr/bin/python and that PATH isset properly. 解决方案： 1234# 在.bashrc文件中添加一行配置指定python路径export WORKON_HOME=$HOME/.virtualenvs export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3 # 本次新增source /usr/local/bin/virtualenvwrapper.sh 安装virtualenvwrapper报错： 1234567891011121314151617181920212223242526Couldn't find index page for 'pbr' (maybe misspelled?)Download error on https://pypi.python.org/simple/: [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed (_ssl.c:645) -- Some packages may not be found!No local packages or download links found for pbrTraceback (most recent call last):File "", line 1, inFile "/tmp/pip-build-6hblrn57/virtualenvwrapper/setup.py", line 7, inpbr=True,File "/usr/lib/python3.5/distutils/core.py", line 108, in setup_setup_distribution = dist = klass(attrs)File "/usr/lib/python3/dist-packages/setuptools/dist.py", line 269, in __init__self.fetch_build_eggs(attrs['setup_requires'])File "/usr/lib/python3/dist-packages/setuptools/dist.py", line 313, in fetch_build_eggsreplace_conflicting=True,File "/usr/lib/python3/dist-packages/pkg_resources/__init__.py", line 826, in resolvedist = best[req.key] = env.best_match(req, ws, installer)File "/usr/lib/python3/dist-packages/pkg_resources/__init__.py", line 1092, in best_matchreturn self.obtain(req, installer)File "/usr/lib/python3/dist-packages/pkg_resources/__init__.py", line 1104, in obtainreturn installer(requirement)File "/usr/lib/python3/dist-packages/setuptools/dist.py", line 380, in fetch_build_eggreturn cmd.easy_install(req)File "/usr/lib/python3/dist-packages/setuptools/command/easy_install.py", line 657, in easy_installraise DistutilsError(msg)distutils.errors.DistutilsError: Could not find suitable distribution for Requirement.parse('pbr')----------------------------------------Command "python setup.py egg_info" failed with error code 1 in /tmp/pip-build-6hblrn57/virtualenvwrapper/ 解决方案： 123sudo pip install-i https://pypi.douban.tsinghua.edu.cn/simple pbrsudo pip install-i https://pypi.douban.tsinghua.edu.cn/simple--no-deps stevedoresudo pip install-i https://pypi.douban.tsinghua.edu.cn/simple--no-deps virtualenvwrapper]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>环境配置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于多线程详解]]></title>
    <url>%2F2018%2F08%2F15%2Fduo-xian-cheng%2F</url>
    <content type="text"><![CDATA[多线程详解在Python中如何创建多线程？ 通过Thread创建多线程 通过Thread子类创建多线程 python的threading模块是对thread做了一些包装的，可以更加方便的被使用，线程的方法和进程的基本相似，这里就不多赘述，下面举几个栗子：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#例一线程的基本用法#coding=utf-8import threadingimport timedef xianyu(): print("咸鱼普拉思") time.sleep(1)if __name__ == "__main__": for i in range(5): t = threading.Thread(target=xianyu) t.start() #启动线程，即让线程开始执行输出：咸鱼普拉思咸鱼普拉思咸鱼普拉思咸鱼普拉思咸鱼普拉思[Finished in 1.1s]#例二使用Threading子类创建多线程#coding=utf-8import threadingimport timeclass MyThread(threading.Thread): def run(self): for i in range(3): time.sleep(1) msg = "I'm "+self.name+' @ '+str(i) print(msg)def test(): for i in range(5): t = MyThread() t.start()if __name__ == '__main__': test()输出：I'm Thread-1 @ 0I'm Thread-2 @ 0I'm Thread-3 @ 0I'm Thread-4 @ 0I'm Thread-5 @ 0I'm Thread-1 @ 1I'm Thread-2 @ 1I'm Thread-4 @ 1I'm Thread-3 @ 1I'm Thread-5 @ 1I'm Thread-1 @ 2I'm Thread-2 @ 2I'm Thread-5 @ 2I'm Thread-4 @ 2I'm Thread-3 @ 2[Finished in 3.2s] 多线程和多进程的执行有什么区别？ 多进程是多份程序同时执行 多线程是在一份程序下多个执行指针同时执行 多线程并不需要线程间通信，线程间共享全局变量，进程间不共享全局变量 进程是系统进行资源分配和调度的一个独立单位，线程是进程的一个实体，是CPU调度和分派的基本单位,它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源,只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。 线程的划分尺度小于进程(资源比进程少)，使得多线程程序的并发性高。 进程在执行过程中拥有独立的内存单元，而多个线程共享内存，从而极大地提高了程序的运行效率 线线程不能够独立执行，必须依存在进程中 线程执行开销小，但不利于资源的管理和保护，而进程正相反 线程的几种状态线程在执行过程中，如果中途执行sleep语句时，线程会进入到阻塞状态，当sleep结束之后，线程进入就绪状态，等待调度而线程调度将自行选择一个线程执行 。 具体线程状态变换参照下图： 线程之间共享全局变量举个栗子： 1234567891011121314151617181920212223from threading import Threadimport timenum = 100def work1(): global num for i in range(3): num += 1 print("----in work1, num is %d---"%num)def work2(): global num print("----in work2, num is %d---"%num)print("---线程创建之前g_num is %d---"%num)t1 = Thread(target=work1)t1.start()#延时一会，保证t1线程中的事情做完time.sleep(1)t2 = Thread(target=work2)t2.start()输出：---线程创建之前g_num is 100-------in work1, num is 103-------in work2, num is 103---[Finished in 1.1s] 总结： 在一个进程内的所有线程共享全局变量，能够在不适用其他方式的前提下完成多线程之间的数据共享（这点要比多进程要好） 缺点就是，线程是对全局变量随意遂改可能造成多线程之间对全局变量的混乱（即线程非安全） 什么是线程不安全？举个栗子： 123456789101112131415161718192021222324252627282930from threading import Threadimport timenum = 0def test1(): global num for i in range(1000000): num += 1 print("---test1---num=%d"%g_numnumdef test2(): global num for i in range(1000000): num += 1 print("---test2---num=%d"%num)p1 = Thread(target=test1)p1.start()# time.sleep(3)p2 = Thread(target=test2)p2.start()print("---num=%d---"%num)输出：当time.sleep(3)，没有取消屏蔽时---num=235159------test1---num=1172632---test2---num=1334237[Finished in 0.3s]当time.sleep(3)，取消屏蔽时---test1---num=1000000---num=1014670------test2---num=2000000[Finished in 3.3s] 上面举的栗子就是线程不安全的现象，具体可以解释为，线程1对数据num进行自增的时候，获取的值是num=0，此时系统把线程1调度为”sleeping”状态 ，而线程2在做同样操作时获取的num值还是为0，同时做自增1的操作，这时在线程2中num的值为1，此时系统把线程2调度为”sleeping”状态，线程1再做自增操作时，num还是刚刚获取到的0，长此往复下去，最终的结果就不是我们所预期的了。 没有控制多个线程对同一资源的访问，对数据造成破坏，使得线程运行的结果不可预期 ，这种现象就是线程不安全。 如何避免线程不安全的现象发生？当多个线程几乎同时修改某一个共享数据的时候，需要进行同步控制，线程同步能够保证多个线程安全访问竞争资源，最简单的同步机制是引入互斥锁。互斥锁为资源引入一个状态：锁定/非锁定。 某个线程要更改共享数据时，先将其锁定，此时资源的状态为“锁定”，其他线程不能更改；直到该线程释放资源，将资源的状态变成“非锁定”，其他的线程才能再次锁定该资源。互斥锁保证了每次只有一个线程进行写入操作，从而保证了多线程情况下数据的正确性。 举个栗子： 123456789101112131415161718192021222324252627282930313233from threading import Thread, Lockimport timenum = 0def test1(): global num for i in range(1000000): #True表示堵塞 即如果这个锁在上锁之前已经被上锁了，那么这个线程会在这里一直等待到解锁为止 #False表示非堵塞，即不管本次调用能够成功上锁，都不会卡在这,而是继续执行下面的代码 mutexFlag = mutex.acquire(True) if mutexFlag: num += 1 mutex.release() print("---test1---num=%d"%num)def test2(): global num for i in range(1000000): mutexFlag = mutex.acquire(True) #True表示堵塞 if mutexFlag: num += 1 mutex.release() print("---test2---num=%d"%num)#创建一个互斥锁#这个所默认是未上锁的状态mutex = Lock()p1 = Thread(target=test1)p1.start()p2 = Thread(target=test2)p2.start()print("---num=%d---"%num)输出：---num=61866------test1---num=1861180---test2---num=2000000 当一个线程调用锁的acquire()方法获得锁时，锁就进入“locked”状态。每次只有一个线程可以获得锁。如果此时另一个线程试图获得这个锁，该线程就会变为“blocked”状态，称为“阻塞”，直到拥有锁的线程调用锁的release()方法释放锁之后，锁进入“unlocked”状态。线程调度程序从处于同步阻塞状态的线程中选择一个来获得锁，并使得该线程进入运行状态。 加锁确保了某段关键代码只能由一个线程从头到尾完整地执行，但是阻止了多线程并发执行，包含锁的某段代码实际上只能以单线程模式执行，效率就大大地下降了，由于可以存在多个锁，不同的线程持有不同的锁，并试图获取对方持有的锁时，可能会造成死锁 什么是死锁?在线程间共享多个资源的时候，如果两个线程分别占有一部分资源并且同时等待对方的资源，就会造成死锁。 举个栗子： 123456789101112131415161718192021222324252627282930313233#coding=utf-8import threadingimport timeclass MyThread1(threading.Thread): def run(self): if mutexA.acquire(): print(self.name+'----do1---up----') time.sleep(1) if mutexB.acquire(): print(self.name+'----do1---down----') mutexB.release() mutexA.release()class MyThread2(threading.Thread): def run(self): if mutexB.acquire(): print(self.name+'----do2---up----') time.sleep(1) if mutexA.acquire(): print(self.name+'----do2---down----') mutexA.release() mutexB.release()mutexA = threading.Lock()mutexB = threading.Lock()if __name__ == '__main__': t1 = MyThread1() t2 = MyThread2() t1.start() t2.start() 生产者与消费者模型我们可以通过生产者和消费者模型来解决线程的同步，和线程安全。 Python的Queue模块中提供了同步的、线程安全的队列类，包括FIFO（先入先出)队列Queue，LIFO（后入先出）队列LifoQueue，和优先级队列PriorityQueue。这些队列都实现了锁原语（可以理解为原子操作，即要么不做，要么就做完），能够在多线程中直接使用，可以使用队列来实现线程间的同步。 举个栗子： 123456789101112131415161718192021222324252627282930313233343536373839#encoding=utf-8import threadingimport time#python2中# from Queue import Queue#python3中from queue import Queueclass Producer(threading.Thread): def run(self): global queue count = 0 while True: if queue.qsize() &lt; 1000: for i in range(100): count = count +1 msg = '生成产品'+str(count) queue.put(msg) print(msg) time.sleep(0.5)class Consumer(threading.Thread): def run(self): global queue while True: if queue.qsize() &gt; 100: for i in range(3): msg = self.name + '消费了 '+queue.get() print(msg) time.sleep(1)if __name__ == '__main__': queue = Queue() for i in range(500): queue.put('初始产品'+str(i)) for i in range(2): p = Producer() p.start() for i in range(5): c = Consumer() c.start() 什么是生产者与消费者模型？生产者消费者模式是通过一个容器来解决生产者和消费者的强耦合问题。生产者和消费者彼此之间不直接通讯，而通过阻塞队列来进行通讯，所以生产者生产完数据之后不用等待消费者处理，直接扔给阻塞队列，消费者不找生产者要数据，而是直接从阻塞队列里取，阻塞队列就相当于一个缓冲区，平衡了生产者和消费者的处理能力。这个阻塞队列就是用来给生产者和消费者解耦的。纵观大多数设计模式，都会找一个第三者出来进行解耦。 想使用全局变量有不想加锁怎么办？在多线程环境下，每个线程都有自己的数据。一个线程使用自己的局部变量比使用全局变量好，因为局部变量只有线程自己能看见，不会影响其他线程，而全局变量的修改必须加锁。 举个栗子： 1234567891011121314151617181920import threading# 创建全局ThreadLocal对象:local_school = threading.local()def process_student(): # 获取当前线程关联的student: std = local_school.student print('Hello, %s (in %s)' % (std, threading.current_thread().name))def process_thread(name): # 绑定ThreadLocal的student: local_school.student = name process_student()t1 = threading.Thread(target= process_thread, args=('咸鱼',), name='Thread-A')t2 = threading.Thread(target= process_thread, args=('普拉思',), name='Thread-B')t1.start()t2.start()t1.join()t2.join()输出：Hello, 咸鱼 (in Thread-A)Hello, 普拉思 (in Thread-B) 全局变量local_school就是一个ThreadLocal对象，每个Thread对它都可以读写student属性，但互不影响。你可以把local_school看成全局变量，但每个属性如local_school.student都是线程的局部变量，可以任意读写而互不干扰，也不用管理锁的问题，ThreadLocal内部会处理。 可以理解为全局变量local_school是一个dict，不但可以用local_school.student，还可以绑定其他变量，如local_school.teacher等等。 ThreadLocal最常用的地方就是为每个线程绑定一个数据库连接，HTTP请求，用户身份信息等，这样一个线程的所有调用到的处理函数都可以非常方便地访问这些资源。 一个ThreadLocal变量虽然是全局变量，但每个线程都只能读写自己线程的独立副本，互不干扰。ThreadLocal解决了参数在一个线程中各个函数之间互相传递的问题 同步调用和异步调用？ 同步调用就是你喊你朋友吃饭，你朋友在忙，你就一直在那等，等你朋友忙完了 ，你们一起去。 异步调用就是你喊你朋友吃饭，你朋友说知道了，待会忙完去找你 ，你就去做别的了。 举个栗子： 123456789101112131415161718192021222324from multiprocessing import Poolimport timeimport osdef test(): print("---进程池中的进程---pid=%d,ppid=%d--"%(os.getpid(),os.getppid())) for i in range(3): print("----%d---"%i) time.sleep(1) return "hahah"def test2(args): print("---callback func--pid=%d"%os.getpid()) print("---callback func--args=%s"%args)pool = Pool(3)pool.apply_async(func=test,callback=test2)time.sleep(5)print("----主进程-pid=%d----"%os.getpid())输出：---进程池中的进程---pid=9401,ppid=9400------0-------1-------2------callback func--pid=9400---callback func--args=hahah----主进程-pid=9400---- 注意：这里的callback是由主进程执行的，当子进程死亡，主进程回调函数。 什么是GIL锁？Python全局解释锁（GIL）简单来说就是一个互斥体（或者说锁），这样的机制只允许一个线程来控制Python解释器。这就意味着在任何一个时间点只有一个线程处于执行状态。 所以在python中多线程是假的，因为在执行过程中CPU中只有一个线程在执行。 当你使用多进程时，你的效率是高于多线程的。 Python GIL经常被认为是一个神秘而困难的话题，但是请记住作为一名Python支持者，只有当您正在编写C扩展或者您的程序中有计算密集型的多线程任务时才会被GIL影响。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[中文乱码、unicode和utf8]]></title>
    <url>%2F2018%2F08%2F15%2Fzhongwen-utf-bianma%2F</url>
    <content type="text"><![CDATA[中文乱码、unicode和utf8 http://openskill.cn/article/448 https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001386819196283586a37629844456ca7e5a7faa9b94ee8000 开始之前我们先了解下ASCII、GB2312、unicode、UTF-8都是啥？ 编码演化史ASCII编码在很久很久以前，美国人发明了计算机，计算机只能处理数字也就是把文字转换为8个bit也就是一个字节，8个bit最大能表示的数字为255，而[A-Z]、[a-z]、[0-9]再加上键盘上的一些符号正好255个，所以ASCII编码就成为了美国人的标准编码(用一个字节代表一个字母或者符号)，正好也满足了美国人的需求。 GB2312有了上面的情况，我大中国表示不服，我们中国汉字博大精深，255个明显是满足不了我们的需求的，于是我们就发明了GB2312编码(用两个字节表示汉字)，不仅包含了ASCII码还能表示我们的中国的汉字，于是有了我们中国的例子，世界各国纷纷研究除了支持自己语言的编码，在这种情况下多种语言混合显示就出现了乱码的情况了，对于这种情况就出现了unicode编码将所有语言统一到一起。 unicodeunicode编码的出现解决了多国语言展示乱码的问题，但是unicode的解决方案在全英文文档展示的情况下，unicode编码会比ASCII编码多一倍的存储空间(unicode的编码是16bit的，在表示ASCII编码时是直接在前面加上8个0)相应的在传输的时候就多了一倍的传输时间，在这种情况下就出现了UTF8编码。 UTF8UTF8编码相比于8bit的ASCII编码和16bit的unicode编码来说，UTF8编码是不定长的，它可以使用两个字节代表英文，用三个字节代表中文，UTF8这个时候优势就很大了，在实际运用中，我们可以将文件编码互相装换以获取最大化的利用内存，把文件保存在内存中我们采用内存占用更小的UTF8编码的格式，读写文件时我们采用更大更全的unicode编码，具体实例图如下： 代码演示Python2.7windows 在python2.7中当要将字符串encode为utf8，我们需要确保之前的字符串的编码方式为unicode，所以当字符串编码不为unicode时，我们需要使用decode方法，而在使用decode方法时我们需要指明原有字符串的编码格式(在windows系统中解释器默认编码为GB2312，Linux系统中为UTF-8编码)，所以就有了s.decode(&quot;gb2312&quot;).encode(&quot;utf-8&quot;)。 Linux按照上面解读，我们在Linux系统下操作如下： 这里就有一个疑问既然原有系统默认的字符串编码为utf-8，为什么不可以直接使用s.decode().encode(&#39;utf-8&#39;)？原因就是，如若不指定原有的系统编码格式(utf-8)，Linux系统下会调用python解释器的默认编码ASCII解析字符串，演示如下： 所以在根据上面的情况，在python2中出现编码互转的情况，可以参考下图： Python3.6Python2.7和Python3.6最大的区别就是在执行Python2.7项目时，当项目中包含汉字时，需要在文件头声明编码格式，否则项目中的中文显示就是乱码。 而在Python3中完全没有这样的顾虑，那是因为默认python3中全部的字符串就是unicode可以直接使用encode方法。 综上：为了避免给自己添麻烦，请认准unicode和UTF-8编码。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于深浅拷贝]]></title>
    <url>%2F2018%2F08%2F01%2Fdeepcopy-and-copy%2F</url>
    <content type="text"><![CDATA[关于 is 和 ==== 是 python 标准操作符中的比较操作符，用来比较判断两个对象的 value(值) 是否相等 。 12345678# 例1.1a = '2332424'b = '2332424'print(a == b)输出：True[Finished in 0.1s] is 也被叫做同一性运算符，这个运算符比较判断的是对象间的唯一身份标识，也就是id是否相同。 1234567891011121314# 例1.2a = b = ['2343456']c = ['2343456']print(a == b, id(a), id(b))print(a == c, id(a), id(c))print(a is b, id(a), id(b))print(a is c, id(a), id(c))输出：True 10715336 10715336True 10715336 10717064True 10715336 10715336False 10715336 10717064[Finished in 0.1s] 不同类型下， == 的结果都为True，但是 is 的结果则不一定 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384# 例1.3# a 和 b 都为数值时a = 1b = 1print(id(a))print(id(b))print(a == b)print(a is b)17150776161715077616TrueTrue[Finished in 0.1s]# a 和 b 都为字符串时a = 'abcdefg'b = 'abcdefg'print(id(a))print(id(b))print(a == b)print(a is b)1184769611847696TrueTrue[Finished in 0.1s]# a 和 b 都为列表时a = ['12345678']b = ['12345678']print(id(a))print(id(b))print(a == b)print(a is b)1694125616942984TrueFalse[Finished in 0.1s]# a 和 b 都是字典时a = &#123;'a':1,'b':2&#125; b = &#123;'a':1,'b':2&#125; print(id(a))print(id(b))print(a == b)print(a is b)1042119210421256TrueFalse[Finished in 0.1s]# a 和 b 都是元组时a = (1,2,3)b = (1,2,3)print(id(a))print(id(b))print(a == b)print(a is b)1196060811960680TrueFalse[Finished in 0.1s]# a 和 b 都为集合时a = set([1,2,3])b = set([1,2,3])print(id(a))print(id(b))print(a == b)print(a is b)1685050416919464TrueFalse[Finished in 0.1s] 只有数值型和字符串型的情况下，a is b才为True，当a和b是tuple，list，dict或set型时，a is b为False。 关于深浅拷贝首先我们来看几个栗子： 123456789101112131415# 例2.1：赋值a = [1,2,3]b = aa.append([4,5,6])print(a)print(b)print(a is b)print(a == b)输出：[1, 2, 3, [4, 5, 6]][1, 2, 3, [4, 5, 6]][Finished in 0.1s]TrueTrue 在例2.1中a的末尾新增了[4,5,6]之后，b的值也发生了变化，这是因为a把值赋值给b只是将创建的b对象指向了a对象指向的内存，这时的a和b指向的都是同一块内存空间(id)，所以修改a后b的值也一起改变。 1234567891011121314# 例2.2：赋值a = [1,2,3]b = aa = [4,5,6]print(a)print(b)print(id(a))print(id(b))输出：[4, 5, 6][1, 2, 3]1084813610846408[Finished in 0.1s] 在例2.2中先将a赋值给b，又将[4,5,6]赋值给a，这时已经给a创建了新的内存空间，所以打印a，b时a和b的值并不相同。 下面来看下深浅拷贝的不同之处： 首先 字符串 元组以及 数值的深浅拷贝是没有差别的，如下例子所示： 123456789101112131415161718192021222324252627282930# 例3.1# 字符串import copya = '1345'b = copy.copy(a)c = copy.deepcopy(a)print(id(a), id(b), id(c))输出：17025040 17025040 17025040[Finished in 0.1s]# 数值import copya = 1345b = copy.copy(a)c = copy.deepcopy(a)print(id(a), id(b), id(c))输出：9326480 9326480 9326480[Finished in 0.1s]# 元组import copya = (1,2,3)b = copy.copy(a)c = copy.deepcopy(a)print(id(a), id(b), id(c))输出：10715424 10715424 10715424[Finished in 0.2s] 而对于字典、列表 进行浅拷贝和深拷贝时，其id是有变化的 1234567891011121314151617181920212223242526# 列表import copya = [1,2,3]b = copy.copy(a)c = copy.deepcopy(a)print(id(a), id(b), id(c))输出：17540104 17040264 17419656[Finished in 0.1s]#字典import copya = &#123;"a": "1", "b": 2, "c": ["c", 3]&#125;b = copy.copy(a)c = copy.deepcopy(a)print(a, b, c)print(id(a), id(b), id(c))a['c'] = '666'print(a, b, c)print(id(a), id(b), id(c))输出：&#123;'b': 2, 'a': '1', 'c': ['c', 3]&#125; &#123;'b': 2, 'a': '1', 'c': ['c', 3]&#125; &#123;'b': 2, 'a': '1', 'c': ['c', 3]&#125;6882248 17105992 16886792&#123;'b': 2, 'a': '1', 'c': '666'&#125; &#123;'b': 2, 'a': '1', 'c': ['c', 3]&#125; &#123;'b': 2, 'a': '1', 'c': ['c', 3]&#125;6882248 17105992 16886792[Finished in 0.1s] 浅拷贝：不管多么复杂的数据结构，浅拷贝都只会copy一层，且当copy指向不可变类型时，copy不会执行copy操作。当copy指向可变类型时，copy会执行第一层拷贝，即拷贝浅层的指向。 深拷贝：其实深拷贝就是在内存中重新开辟一块空间，不管数据结构多么复杂，只要遇到可能发生改变的数据类型，就重新开辟一块内存空间把内容复制下来，直到最后一层，不再有复杂的数据类型，就保持其原引用。这样，不管数据结构多么的复杂，数据之间的修改都不会相互影响，这就是深拷贝，也同样可以将深拷贝理解为真正意义上的复制，是两个操作独立的个体。 http://www.cnblogs.com/wupeiqi/articles/5453708.html http://www.cnblogs.com/wupeiqi/articles/5453708.html]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Request模拟登陆]]></title>
    <url>%2F2018%2F07%2F29%2Fabout_csv_read_and_write%2F</url>
    <content type="text"><![CDATA[csv的读取1234567891011121314151617#demo1import csvwith open('xxx.csv','r') as f: reader = csv.reader(f) #reader是一个迭代器 #可以使用next(reader)跳过第一行的标题 for x in reader: print(x)#demo2import csvwith open('xxx.csv','r') as f: reader = csv.DictReader(f) for x in reader: #使用DictReader方法是不打印title的 #可以通过取键名获取值 print(x['turnoverVol']) csv的写入12345678910111213141516171819202122232425# demo1import csvheaders = ['name','age','classroom']values = [ ('zhiliao',18,'111'), ('wena',20,'222'), ('bbc',21,'111')]with open('xxx.csv','w',encoding='utf-8',newline='') as f: writer = csv.writer(fp) writer.writerow(headers) writer.writerows(values)# demo2import csvheaders = ['name','age','classroom']values = [ &#123;"name":'wenn',"age":20,"classroom":'222'&#125;, &#123;"name":'abc',"age":30,"classroom":'333'&#125;]with open('xxx.csv','w',encoding='utf-8',newline='') as f: writer = csv.DictWriter(fp,headers) writer = csv.writeheader() writer.writerow(&#123;'name':'zhiliao',"age":18,"classroom":'111'&#125;) writer.writerows(values)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
</search>
